<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SixDegree</title>
  
  <subtitle>host by chenjin</subtitle>
  <link href="//atom.xml" rel="self"/>
  
  <link href="http://sixdegree.github.io/"/>
  <updated>2016-10-22T07:38:02.000Z</updated>
  <id>http://sixdegree.github.io/</id>
  
  <author>
    <name>Chen Jin</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hadoop2.x Basic</title>
    <link href="http://sixdegree.github.io/2016/05/25/Hadoop2.x-Basic.html"/>
    <id>http://sixdegree.github.io/2016/05/25/Hadoop2.x-Basic.html</id>
    <published>2016-05-24T16:00:00.000Z</published>
    <updated>2016-10-22T07:38:02.000Z</updated>
    
    <content type="html"><![CDATA[<ol><li>Hadoop2.x HDFS HA &amp; Federation;</li><li>Hadoop2.x MapReduce on Yarn</li></ol><a id="more"></a><h2 id="header-1">概述</h2><p>Hadoop2中有两个重要的变更：</p><ol><li>DFS的NameNode可以以集群的方式布署，增强了NameNodes的水平扩展能力和高可用性，分别是:HDFS Federation与HA；</li><li>MapReduce将JobTracker中的资源管理及任务生命周期管理（包括定时触发及监控），拆分成两个独立的组件，并更名为YARN（Yet Another Resource Negotiator）</li></ol><h2 id="header-2">HDFS</h2><h3 id="header-3">1.x缺点</h3><p>Namenode problems:</p><ul><li>Only One</li><li>under great memory pressure</li></ul><p>SecondaryNamenode problems:</p><ul><li>It’s confusing name</li><li>No up-to-date FSIMAGE file</li><li>No automatic failover</li></ul><h3 id="header-4">2.x改进</h3><p>Muti-Namenode （水平扩展和高可用）</p><ul><li>Federation (different namespace)<ul><li>多个Namenode，一组Datanode</li><li>使用不同的HDFS目录（即不同的namespace，互不影响）</li><li>应用举例：不同Federation HDFS配置使用不同的Block大小以处理不同的需求</li></ul></li><li>HA (same namespace)<ul><li>多个Namenode，一组Datanode</li><li>使用相同的HDFS目录（即相同的namespace，只有一个Namenode负责读写）</li><li>只有一个Namenode为Active，对外提供读写服务，其他为StandBy</li><li>Active NN 一旦故障便自动切换到 standby NN（借助Zookeeper完成热切）</li><li>系统通过JournalNodes守护进程使Standby和Active的Namenode保持元数据同步<ul><li>Active NN 将修改持久化（写）到JournalNodes</li><li>StandBy NN 从JournalNodes读取修改信息，更新内部元数据</li><li>JournalNodes是轻量级的进程（通过editlog持久化存储），需为奇数个</li></ul></li><li>注意：Standby NN也执行namespace状态的checkpoints，所以不要再运行Secondary NN、CheckpointNode、BackupNode</li></ul></li></ul><p><img src="/2016/05/25/hdfs.png" alt="HDFS"><br>(图片来自 <a href="http://blog.csdn.net/jiewuyou" target="_blank" rel="noopener">http://blog.csdn.net/jiewuyou</a>)</p><h3 id="header-5">HDFS示例</h3><p>Nodes DNS:</p><ul><li>cluster1 namenode<ul><li>masterA.cls1</li><li>masterB.cls1</li></ul></li><li>cluster2 namenode<ul><li>masterA.cls2</li><li>masterB.cls2</li></ul></li><li>datanode<ul><li>slave1.cls</li><li>slave2.cls</li><li>slave3.cls</li></ul></li><li>zookeeper<ul><li>slave1.cls</li><li>slave2.cls</li><li>slave3.cls</li></ul></li><li>journalnode<ul><li>slave1.cls</li><li>slave2.cls</li><li>slave3.cls</li></ul></li></ul><h4 id="header-6">Configuration</h4><ol><li><p>hadoop-env.sh (on all nodes)</p><pre><code> export JAVA_HOME=/usr/local/jdk1.8</code></pre></li><li><p>Cluster1 (on masterA.cls1,masterB.cls1)</p><ul><li>core-site.sh<pre><code class="nullxml">  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://cluster1<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.tmp.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/usr/local/hadoop/tmp<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>ha.zookeeper.quorum<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>slave1.cls:2181,slave2.cls:2181,slave3.cls:2181<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></code></pre></li><li><p>hdfs-site.xml</p><pre><code class="nullxml">  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>3<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.nameservices<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>cluster1,cluster2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-comment">&lt;!-- journal nodes --&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>       <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>qjournal://slave1.cls:8485;slave2.cls:8485;slave3.cls:8485/cluster1<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/usr/local/hadoop/tmp/journal<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-comment">&lt;!-- Using ssh to switch namenode --&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>sshfence<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/root/.ssh/id_rsa<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-comment">&lt;!-- Cluster1 --&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.namenodes.cluster1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>masterA.cls1,masterB.cls1<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.rpc-address.cluster1.masterA.cls1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>masterA.cls1:9000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.http-address.cluster1.masterA.cls1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>masterA.cls1:50070<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.rpc-address.cluster1.masterB.cls1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>masterB.cls1:9000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.http-address.cluster1.masterB.cls1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>masterB.cls1:50070<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled.cluster1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.client.failover.proxy.provider.cluster1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>       <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-comment">&lt;!-- Cluster2 --&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.namenodes.cluster2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>masterA.cls2,masterB.cls2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.rpc-address.cluster2.masterA.cls2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>masterA.cls2:9000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.http-address.cluster2.masterA.cls2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>masterA.cls2:50070<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.rpc-address.cluster2.masterB.cls2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>masterB.cls2:9000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.http-address.cluster2.masterB.cls2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>masterB.cls2:50070<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled.cluster2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.client.failover.proxy.provider.cluster2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>      <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></code></pre></li></ul></li><li><p>Cluster2 (on masterA.cls2,masterB.cls2)</p><ul><li>core-site.sh (copy from cluster1) update:<pre><code class="nullxml"> <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>     <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>     <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://cluster2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span> <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></code></pre><ul><li>hdfs-site.xml (copy from cluster1) update:<pre><code class="nullxml"><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>   <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>qjournal://slave1.cls:8485;slave2.cls:8485;slave3.cls:8485/cluster2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></code></pre></li></ul></li></ul></li><li><p>slaves (on all nodes)</p><pre><code>  slave1.cls  slave2.cls  slave3.cls</code></pre></li></ol><h4 id="header-7">Run</h4><ol><li>start zookeeper<pre><code class="nullshell"><span class="hljs-meta">  #</span><span class="bash"> slave1.cls,slave2.cls,slave3.cls</span>  zkServer.sh start</code></pre></li><li><p>start namenode</p><pre><code class="nullshell"><span class="hljs-meta"> #</span><span class="bash"> slave1.cls,slave2.cls,slave3.cls</span> sbin/hadoop-daemon.sh start journalnode<span class="hljs-meta"> #</span><span class="bash"> masterA.cls1,masterB.cls1</span> bin/hdfs zkfc -formatZK<span class="hljs-meta"> #</span><span class="bash"> masterA.cls1</span> bin/hdfs namenode -format sbin/hadoop-daemon.sh start namenode<span class="hljs-meta"> #</span><span class="bash"> masterB.cls1</span> bin/hdfs namenode -bootstrapStandby sbin/hadoop-daemon.sh start namenode<span class="hljs-meta"> #</span><span class="bash"> masterA.cls1,masterB.cls2</span> sbin/hadoop-daemon.sh start zkfc</code></pre></li><li>start datanode<pre><code class="nullshell"><span class="hljs-meta"> #</span><span class="bash"> slave1.cls,slave2.cls,slave3.cls</span> sbin/hadoop-daemons.sh start datanode</code></pre></li></ol><h2 id="header-8">MapReduce</h2><h3 id="header-9">1.x缺点</h3><ul><li>JobTracker under greate pressure<ul><li>Job Coordination</li><li>Scheduling</li><li>Resource Management</li></ul></li><li>Cluster 资源利用率不高 (不同作业需要搭建不同的集群环境)</li></ul><h3 id="header-10">2.x改进</h3><p><img src="/2016/05/25/hadoop.png" alt="Hadoop"></p><p>引入Yarn</p><ul><li>JobTracker的功能分离成Yarn的两个单独的组件完成<ul><li>ResourceManager 全局管理所有应用程序计算资源的分配</li><li>ApplicationMaster 负责某一应用的任务调度和协调（每个应用一个，例如MapReduce,Storm,Spark等）</li></ul></li><li>Yarn具有通用性，因此整个集群也可作为其他计算框架的管理平台（例如Spark，Storm等）</li></ul><h3 id="header-11">Yarn</h3><p>Yarn：</p><ul><li>一套资源统一管理和调度的平台</li><li>可管理各种计算框架，包括 MapReduce 、 Spark 、 Strom 等</li></ul><p><img src="/2016/05/25/yarn.png" alt="Hadoop"></p><p>说明：</p><ol><li><p>ResourceManager</p><ul><li>YARN集群的Master，负责管理整个集群的资源分配和作业调度</li><li>接收提交的job，根据job的Context，NodeManager反馈的status，启动分配一个NodeManager的Container作为ApplicationManager</li><li>主要包含两个组件：<ul><li>Scheduler 负责将集群资源分配给应用程序</li><li>ApplicationManager 负责接收任务，调度启动每个Job所属的ApplicationMaster，<br>监控重启ApplicationMaster</li></ul></li></ul></li><li><p>NodeManager</p><ul><li>YARN集群的Slave，是集群中实际拥有实际资源的工作节点</li><li>负责Container状态的维护，并向RM保持心跳（类似RM在每台机器的上代理）</li><li>注：<ul><li>RM可将某个NM上的Container分配给某个Job的AppMstr</li><li>AppMstr将组成Job的多个Task调度到对应的NM上进行执行</li><li>一般DN和NM在同一个节点</li></ul></li></ul></li><li><p>ApplicationMaster</p><ul><li>负责申请资源，监控管理任务运行（一个Job生命周期内的所有工作）</li><li>比如:<ul><li>运行Task的资源，由AM向RM申请；</li><li>启动/停止NM上某Task的对应的Container，由AM向NM请求来完成</li></ul></li><li>是一个可变部分，用户可对不同编程模型写自己的AM实现，让更多类型的编程模型能够跑在此集群中</li></ul></li><li><p>Container</p><ul><li>资源的抽象，Yarn为了作资源隔离而提出的一个框架<ul><li>对NodeManager上的资源进行量化，组装成一个个Container，服务于已授权资源的任务</li><li>完成任务后，系统回收资源，供后续任务申请使用</li></ul></li><li>资源包括：内存，CPU，硬盘，网络等</li><li>对于资源的表示以内存为单位，比之前以剩余slot数目更合理</li></ul></li></ol><h3 id="header-12">Yarn配置示例</h3><h4 id="header-13">Configuration</h4><ul><li>mapred-site.xml<pre><code class="nullxml">  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.framework.name<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>yarn<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></code></pre></li><li>yarn-site.xml<pre><code class="nullxml">  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>       <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>       <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>masterA.cls1<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>       <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>       <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  ...</code></pre></li></ul><h4 id="header-14">Run</h4><pre><code class="nullshell"><span class="hljs-meta">#</span><span class="bash"> masterA.cls1</span>sbin/start-yarn.shsbin/stop-yarn.sh</code></pre>]]></content>
    
    <summary type="html">
    
      Hadoop2.x Basic Introduce (HDFS+Yarn+MapReduce)
    
    </summary>
    
    
      <category term="BigData" scheme="http://sixdegree.github.io/tags/BigData/"/>
    
  </entry>
  
  <entry>
    <title>Flume</title>
    <link href="http://sixdegree.github.io/2016/05/20/Flume.html"/>
    <id>http://sixdegree.github.io/2016/05/20/Flume.html</id>
    <published>2016-05-19T16:00:00.000Z</published>
    <updated>2016-06-04T08:35:45.000Z</updated>
    
    <content type="html"><![CDATA[<ol><li>Flume概述（Agent组件：Source，Channel，Sink）；</li><li>Flume安装；</li><li>Flume使用示例；</li></ol><a id="more"></a><h2 id="header-1">概述</h2><p>Flume 分布式的日志收集系统 <a href="https://flume.apache.org/FlumeUserGuide.html#setting-up-an-agent" target="_blank" rel="noopener">官网手册</a></p><p><img src="/2016/05/20/frame1.png" alt="Frame 1"></p><ul><li>agent：收集日志发送到目的地（运行在日志收集端的一个Java进程），包括三个核心组件：<ul><li>source 收集日志（数据临时存放在channel中）<ul><li>可处理各种类型各种格式的日志数据</li><li>例如日志类型：avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy、自定义</li></ul></li><li>channel 缓冲数据（数据只有在sink发送成功之后才会被删除）<ul><li>例如存放在：memory、jdbc、file、自定义</li></ul></li><li>sink 发送日志到目的地<ul><li>例如目的地：hdfs、logger、avro、thrift、ipc、file、null、hbase、solr、自定义</li></ul></li><li>特殊：interceptor 拦截器（在日志进入到source前，包装清洗过滤event）<ul><li>chain形式：可对一个source指定多个拦截器，按先后顺序依次处理</li><li>官方已有的拦截器：Timestamp/Host/Static/Regex Filtering/Regex Extractor/… </li></ul></li></ul></li><li>event：在整个数据传输过程中，流动的是event</li><li>注意：<ul><li>事务保证在event级别</li><li>flume支持多级agent</li><li>flume支持扇入(fan-in)，扇出(fan-out)<br>  <img src="/2016/05/20/frame2.png" alt="Frame 2"><br>  <img src="/2016/05/20/frame3.png" alt="Frame 3"></li></ul></li></ul><h2 id="header-2">使用示例</h2><ol><li>安装：直接下载解压即可</li><li><p>配置：在<code>$FLUME_HOME/conf</code>下添加一个配置文件（例如：<code>flume-conf-test1.properties</code>）</p><pre><code class="nullvim"> # agent1表示代理名称 agent1.sources=source1 agent1.sinks=sink1 agent1.channels=channel1 # 配置source1 # <span class="hljs-number">1</span>. Spooling Directory是监控指定文件夹中新文件的变化 # 一旦新文件出现，就解析该文件内容，然后写入到channle # 写入完成后，标记该文件已完成或者删除该文件 # <span class="hljs-number">2</span>. 添加Timestamp Interceptor # 在event的header中添加一个key叫timestamp,value为当前的时间戳 agent1.sources.source1.<span class="hljs-built_in">type</span>=spooldir agent1.sources.source1.spoolDir=/home/hadoop/<span class="hljs-built_in">input</span>/flume agent1.sources.source1.channels=channel1 agent1.sources.source1.fileHeader = false agent1.sources.source1.interceptors = i1 agent1.sources.source1.interceptors.i1.<span class="hljs-built_in">type</span> = timestamp # 配置sink1 agent1.sinks.sink1.<span class="hljs-built_in">type</span>=hdfs agent1.sinks.sink1.hdfs.path=hdf<span class="hljs-variable">s:</span>//cj.storm:<span class="hljs-number">9000</span>/output/flume agent1.sinks.sink1.hdfs.fileType=DataStream agent1.sinks.sink1.hdfs.writeFormat=TEXT # agent1.sinks.sink1.hdfs.rollInterval=<span class="hljs-number">0</span> # agent1.sinks.sink1.hdfs.rollSize=<span class="hljs-number">10485760</span> agent1.sinks.sink1.channel=channel1 agent1.sinks.sink1.hdfs.filePrefix=%Y-%<span class="hljs-keyword">m</span>-%d # 配置channel1 agent1.channels.channel1.<span class="hljs-built_in">type</span>=<span class="hljs-keyword">file</span> agent1.channels.channel1.checkpointDir=/home/hadoop/<span class="hljs-built_in">input</span>/flume_tmp/checkpoint agent1.channels.channel1.dataDirs=/home/hadoop/<span class="hljs-built_in">input</span>/flume_tmp/data</code></pre></li><li><p>测试</p><pre><code class="nullshell"><span class="hljs-meta"> #</span><span class="bash"> 1. 创建被监控目录</span><span class="hljs-meta"> &gt;</span><span class="bash"> mkdir -p /home/hadoop/input/flume</span><span class="hljs-meta"> #</span><span class="bash"> 2. 启动flume agent1</span><span class="hljs-meta"> #</span><span class="bash"> -n 指定agent名称</span><span class="hljs-meta"> #</span><span class="bash"> -c 指定配置文件目录</span><span class="hljs-meta"> #</span><span class="bash"> -f 指定配置文件</span><span class="hljs-meta"> &gt;</span><span class="bash"> bin/flume-ng agent -n agent1 -c conf -f conf/flume-conf-test1.properties \</span> -Dflume.root.logger=DEBUG,console<span class="hljs-meta"> #</span><span class="bash"> 3. 放入测试文件</span><span class="hljs-meta"> #</span><span class="bash"> 4. 查看运行结果</span><span class="hljs-meta"> &gt;</span><span class="bash"> hadoop fs -lsr /output/flume</span></code></pre></li></ol>]]></content>
    
    <summary type="html">
    
      Log Collection
    
    </summary>
    
    
      <category term="BigData" scheme="http://sixdegree.github.io/tags/BigData/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop</title>
    <link href="http://sixdegree.github.io/2016/05/15/Sqoop.html"/>
    <id>http://sixdegree.github.io/2016/05/15/Sqoop.html</id>
    <published>2016-05-14T16:00:00.000Z</published>
    <updated>2016-06-04T08:52:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>HDFS数据到关系型数据库的导入导出工具Sqoop简介和基本使用示例</p><a id="more"></a><h2 id="header-1">概述</h2><p>SQOOP – 使用MapReduce实现用于对数据进行导入导出</p><ul><li>Import: 把MySQL、Oracle等数据库中的数据导入到HDFS、Hive、HBase中</li><li>Export: 把HDFS、Hive、HBase中的数据导出到MySQL、Oracle等数据库中</li><li>注意：导入导出的事务是以Mapper任务为单位</li><li><a href="http://sqoop.apache.org/" target="_blank" rel="noopener">官网</a></li><li><a href="http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html#_incremental_imports" target="_blank" rel="noopener">用户手册</a></li></ul><p><img src="/2016/05/15/frame.png" alt="Export/Import"></p><h2 id="header-2">安装</h2><ol><li>下载解压</li><li>配置环境变量（<code>/etc/profile</code>）<ul><li>SQOOP_HOME</li><li>HADOOP_HOME</li><li>PATH</li></ul></li><li>将要使用的JDBC Connector Jar包放入<code>$SQOOP_HOME/lib</code>下</li></ol><h2 id="header-3">使用示例</h2><h3 id="header-4">查询</h3><ul><li><p>list databases</p><pre><code>  sqoop list-databases \  --connect jdbc:mysql://cj.storm:3306 --username root --password cj123</code></pre></li><li><p>list tables</p><pre><code>  sqoop list-tables \  --connect jdbc:mysql://cj.storm:3306/hive --username root --password cj123</code></pre></li><li>list jobs<pre><code>  sqoop job --list</code></pre></li></ul><h3 id="header-5">Import(MySQL=&gt;HDFS)</h3><p>一些参数说明：</p><ul><li><code>-m &lt;num&gt;</code>: 使用的mapper数（注意：对于无主键的表，需要增加参数<code>--split-by xxx</code> 或 <code>-m 1</code>）</li><li><code>--warehouse-dir &lt;path&gt;</code>: 指定存放的数据仓库（默认：<code>/user/{USER_NAME}</code>）</li><li><code>--target-dir &lt;path&gt;</code>: 指定存放的数据路径 （默认：<code>/user/{USER_NAME}/{tablename}</code>）</li><li><code>--null-string &lt;str&gt;</code>: 指定用什么代表空字段（默认：NULL）</li><li><code>--hive-import</code>: 表导入到hive中</li><li><code>--hive-table &lt;tablename&gt;</code>: hive table name</li><li><code>--hive-overwrite</code>: overwite hive table</li></ul><ol><li><p><code>mysql table =&gt; hdfs file</code>:</p><pre><code class="nullshell"><span class="hljs-meta"> #</span><span class="bash"> use --warehouse-dir</span><span class="hljs-meta"> #</span><span class="bash"> final file: /output/<span class="hljs-built_in">log</span>/part-m-00000</span> sqoop import \ --connect jdbc:mysql://cj.storm:3306/sqoop --username root --password cj123 \ --table log --fields-terminated-by ',' \ --warehouse-dir '/output' \ -m 1<span class="hljs-meta"> #</span><span class="bash"> use --target-dir &amp; query</span><span class="hljs-meta"> #</span><span class="bash"> final file: /output/log_2/part-m-00000</span> sqoop import \ --connect jdbc:mysql://cj.storm:3306/sqoop  --username root --P \ --query "select * from log where \$CONDITIONS" \ --target-dir /output/log_2 \ -m 1  <span class="hljs-meta"> #</span><span class="bash"> use --target-dir &amp; columns &amp; <span class="hljs-built_in">where</span></span> sqoop import \ --connect jdbc:mysql://cj.storm:3306/sqoop --username root --password cj123 \ --table log \ --columns "ip,status,method" \ --where "status='200' and method in ('GET','POST')"  \ -m 1  \ --target-dir /output/log_3 \ --fields-terminated-by ","</code></pre></li><li><p><code>mysql table =&gt; hive table</code>:</p><pre><code class="nullshell"><span class="hljs-meta"> #</span><span class="bash"> use hive-import</span> sqoop import  --connect jdbc:mysql://cj.storm:3306/hive  --username root --password cj123  --table TBLS  --fields-terminated-by '\t'   --null-string '**'   -m 1  --hive-import<span class="hljs-meta"> #</span><span class="bash"> use hive-import &amp; hive-table</span> sqoop import \ --connect jdbc:mysql://cj.storm:3306/hive  --username root --password cj123 \ --table TBLS \ --fields-terminated-by '\t' \ --null-string '**' \ -m 1 \ --hive-import \ --hive-table 'tbls_2'<span class="hljs-meta"> #</span><span class="bash"> check hive table</span> hive -e 'describe formatted tbls' hive -e 'describe formatted tbls_2'</code></pre></li><li><p><code>mysql table schema =&gt; hive table schema</code>:</p><pre><code class="nullshell"><span class="hljs-meta"> #</span><span class="bash"> just copy table schema (no data)</span> sqoop create-hive-table \ --connect jdbc:mysql://cj.storm:3306/sqoop --username root --password cj123 \ --table log \ --hive-table log_sqoop \ --fields-terminated-by ','</code></pre></li></ol><h3 id="header-6">Job</h3><ol><li>create job:<pre><code class="nullshell"> sqoop job \ --create myjob \ -- import --connect jdbc:mysql://cj.storm:3306/hive  --username root -P \ --table TBLS \ --fields-terminated-by &#39;\t&#39;  \ --null-string &#39;**&#39; \ -m 1 \ --hive-import \ --hive-table &#39;tbls_4&#39;</code></pre></li><li>exec job:<pre><code class="nullshell"> sqoop job --exec myjob</code></pre></li><li>查看 job:<pre><code class="nullshell"> sqoop job --list sqoop job --show myjob</code></pre></li><li>删除 job:<pre><code class="nullshell"> sqoop job --delete myjob</code></pre></li></ol>]]></content>
    
    <summary type="html">
    
      Sqoop introduction
    
    </summary>
    
    
      <category term="BigData" scheme="http://sixdegree.github.io/tags/BigData/"/>
    
  </entry>
  
  <entry>
    <title>Hive</title>
    <link href="http://sixdegree.github.io/2016/05/10/Hive.html"/>
    <id>http://sixdegree.github.io/2016/05/10/Hive.html</id>
    <published>2016-05-09T16:00:00.000Z</published>
    <updated>2016-10-22T07:37:44.000Z</updated>
    
    <content type="html"><![CDATA[<ol><li>Hive概述，安装；</li><li>HiveQL操作；</li><li>Hive的Java客户端操作；</li></ol><a id="more"></a><h2 id="header-1">概述</h2><ul><li>建立在Hadoop基础上的数据仓库，能够管理查询Hadoop中的数据</li><li>本质上，Hive是一个SQL解析引擎，将SQL语句转译成M/R Job，在Hadoop执行</li><li>Hive的表其实就是HDFS的目录，字段即为文件中的列，可以直接在M/R Job里使用这些数据</li><li>在HDFS中的默认存放位置：/user/hive/warehouse（hive-conf.xml的hive.metastore.warehouse.dir属性）</li></ul><h3 id="header-2">系统架构</h3><p><img src="/2016/05/10/frame.png" alt="Frame"></p><ul><li>用户接口<ul><li>CLI：Shell命令行</li><li>JDBC/ODBC：Java Connection （与使用传统数据库JDBC的方式类似）</li><li>WebGUI：通过浏览器访问 Hive</li></ul></li><li>MetaStore<ul><li>存储元数据（例如：表名，表属性，列属性，分区属性，数据所在路径等）</li><li>存储在数据库中，目前支持 mysql、derby(默认，内置)<ul><li>默认使用内嵌的derby数据库作为存储引擎</li><li>Derby引擎的一次只能打开一个会话</li><li>MySQL等外置存储引擎，可支持多用户同时访问</li></ul></li></ul></li><li>Driver<ul><li>包含解释器，编译器，优化器，执行器</li><li>完成HQL=&gt;Job，Trigger Exec</li></ul></li><li>Hadoop<ul><li>用 HDFS 进行存储</li><li>利用 MapReduce 进行计算</li></ul></li><li>注意：大部分的查询由 MapReduce 完成，但有些不是，例如<code>select * from table</code>（包含<code>*</code>的查询）</li></ul><p>现在Hive的整体框架，计算引擎不仅仅支持Map/Reduce，并且还支持Tez、Spark等。根<br>据不同的计算引擎又可以使用不同的资源调度和存储系统</p><p><img src="/2016/05/10/frame-new.png" alt="Frame New"></p><h3 id="header-3">数据存储</h3><p>存储结构主要包括：数据库、文件、表、视图</p><ul><li>使用 HDFS 进行存储，无专门的数据存储格式，也没有为数据建立索引</li><li>用户只需在建表时，指定Hive数据的列分隔符与行分隔符，Hive即可解析数据文件为Table</li><li>默认可以直接加载文本文件（TextFile），也支持SequenceFile </li><li>每一个 Table 在 Hive 中都有一个相应的目录存储数据</li></ul><h2 id="header-4">安装</h2><p>Hive可以安装在Hadoop集群中的任何一台机器上<br>metastore支持三种存储模式</p><ul><li>本地内嵌模式（默认）：元数据保持在Hive内嵌的derby中，只允许一个会话连接</li><li>本地独立模式：元数据保持在本地的一个DB中，允许多会话连接</li><li><p>远端独立模式：元数据保持在远程的一个DB中，允许多会话连接</p></li><li><p>安装Hive：</p><ol><li>下载解压</li><li>设置环境变量（<code>/etc/profile</code>文件）<ul><li><code>HIVE_HOME</code></li><li><code>HADOOP_HOME</code></li><li><code>PATH</code></li></ul></li><li>配置（<code>$HIVE_HOME/conf</code>目录下）<ul><li><code>hive-log4j.properties.template</code> =&gt; <code>hive-log4j.properties</code></li><li><code>hive-default.xml.template</code> =&gt; <code>hive-site.xml</code><pre><code class="nullxml"><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/user/hive/warehouse<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hive.exec.stagingdir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/tmp/hive/.hive-staging<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hive.exec.scratchdir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/tmp/hive<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hive.exec.local.scratchdir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/tmp/hive<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hive.downloaded.resources.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/tmp/hive/${hive.session.id}_resources<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hive.server2.logging.operation.log.location<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/tmp/hive/operation_logs<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></code></pre></li></ul></li><li><p>HDFS上创建和授权目录</p><pre><code>hadoop fs -mkidr /tmphadoop fs -chmod g+w /tmphadoop fs -mkidr /user/hive/warehousehadoop fs -chmod g+w /user/hive/warehouse</code></pre></li></ol></li><li><p>使用外部数据库（例如MySQL）作为Hive的metastore：</p><ol><li><p>安装MySQL</p><pre><code class="nullvim">&gt; rpm -<span class="hljs-keyword">qa</span> |<span class="hljs-keyword">grep</span> mysql             # 检查mysql&gt; rpm -<span class="hljs-keyword">e</span> --nodeps mysql           # 强力卸载mysql&gt; rpm -i mysql-server-********    # 安装mysql服务端&gt; rpm -i mysql-client-********    # 安装mysql客户端&gt; mysqld_safe &amp;                   # 启动mysql 服务端  &gt; mysql_secure_installation       # 设置root用户密码</code></pre></li><li>mysql 连接权限修改<pre><code class="nullvim">&gt; mysql -u root -pmysql&gt; use mysql;mysql&gt; select host,user from user;mysql&gt; grant all privileges on *.* to 'root'@'%' identified by 'mypassword' with grant option;mysql&gt; select host,user from user;mysql&gt; flush privileges;</code></pre></li><li>添加mysql的jdbc驱动包（置于到<code>$HIVE_HOME/lib</code>目录下）</li><li>添加配置Hive （<code>$HIVE_HOME/conf/hive-site.xml</code>）<pre><code class="nullxml"><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>jdbc:mysql://cj.storm:3306/hive?createDatabaseIfNotExist=true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>root<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>admin<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></code></pre></li></ol></li><li><p>运行Hive:</p><ul><li>保证Hadoop是启动状态</li><li>设置Hive运行模式<ul><li>分为本地与集群两种，可通过<code>mapred.job.tracker</code>参数设置</li><li>例如：<code>&gt; hive -e &quot;SET mapred.job.tracker=local&quot;</code></li></ul></li><li>启动Hive<ul><li>启动命令行 <code>&gt; hive --service cli</code>，同<code>&gt; hive</code></li><li>启动Web（port：9999） <code>&gt; hive --service hwi &amp;</code>，需要另外下载放入<code>hive-hwi</code>war包</li></ul></li><li>验证<ul><li>Hive命令<pre><code class="nullvim">&gt; hive -help&gt; hivehive&gt; show databases;OKdefaulthive&gt; exit;</code></pre></li></ul></li></ul></li></ul><h2 id="header-5">HiveQL</h2><h3 id="header-6">数据库</h3><p>类似传统数据库的DataBase，系统默认使用数据库<code>default</code>，也可指定</p><pre><code class="nullvim">&gt; hivehive&gt; create database &lt;数据库名&gt; ;hive&gt; use &lt;数据库名&gt; ;...hive&gt; drop database  &lt;数据库名&gt;;hive&gt; drop database  &lt;数据库名&gt; cascade;</code></pre><h3 id="header-7">表</h3><ul><li><p>查看表：</p><pre><code class="nullsql"># 查看所有的表show tables; # 支持模糊查询show tables &#39;*tmp*&#39;; # 查看表有哪些分区show partitions tmp_tb; #查看表详情describe tmp_tb; describe formatted tmp_tb;</code></pre></li><li>修改表结构（alter）<pre><code class="nullsql"><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> tmp_tb <span class="hljs-keyword">add</span> <span class="hljs-keyword">columns</span> (cols,<span class="hljs-keyword">string</span>);<span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> tmp_tb <span class="hljs-keyword">add</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">exists</span> <span class="hljs-keyword">partition</span>(<span class="hljs-keyword">day</span>=<span class="hljs-string">'2016-04-01'</span>,city=<span class="hljs-string">'wx'</span>);<span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> tmp_tb <span class="hljs-keyword">drop</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">exists</span> <span class="hljs-keyword">partition</span> (daytime=<span class="hljs-string">'2016-05-01'</span>,city=<span class="hljs-string">'sz'</span>);<span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> tmp_tb clustered <span class="hljs-keyword">by</span> (ip) <span class="hljs-keyword">into</span> <span class="hljs-number">3</span> buckets;</code></pre></li><li>删除表（drop）<pre><code class="nullsql"><span class="hljs-keyword">drop</span> <span class="hljs-keyword">table</span></code></pre></li><li>清空表 （truncate）<pre><code class="nullsql"># 无法清空外部表truncate table table_name;  # 不指定分区，将清空表中的所有分区truncate table table_name partition (dt=&#39;20080808&#39;);</code></pre></li><li><p>视图（view）</p><pre><code class="nullsql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">VIEW</span> v1 <span class="hljs-keyword">AS</span> <span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> t1;</code></pre></li><li><p>创建表</p><pre><code class="nullsql"># 创建表Create [EXTERNAL] TABLE [IF NOT EXISTS] table_name   [(col_name data_type, ...)]  [PARTITIONED BY (col_name data_type, ...)]   [    CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)]     INTO num_buckets BUCKETS  ]   [ROW FORMAT DELIMITED row_format]   [STORED AS file_format]   [LOCATION hdfs_path]# 复制表结构CREATE [EXTERNAL] TABLE target_table LIKE source_table [LOCATION hdfs_path];</code></pre><ul><li>EXTERNAL ：标识创建一个外部表</li><li>PARTITIONED By ：分区（每个分区列单独一个目录，分区列本身不会存储在数据文件中）</li><li>CLUSTERED By ：分桶，根据指定列的Hash值切分（一个桶一个数据文件，内容包含桶列）</li><li>ROW FORMAT DELIMITED ：指定数据分割符（默认只认单个字符）<ul><li>FIELDS TERMINATED BY</li><li>LINES TERMINATED BY</li><li>COLLECTION ITEMS TERMINATED BY</li><li>MAP KEYS TERMINATED BY</li><li>…</li></ul></li><li>STORED AS ：数据存储方式<ul><li>TEXTFILE 纯文本，不压缩</li><li>SEQUENCEFILE 序列化，压缩</li></ul></li><li>LOCATION ：创建表时就加载数据，指定数据文件所在位置（可选）</li></ul></li><li><p>加载数据(Load dataset)</p><ul><li>LOAD Cmd：将HDFS文件导入已创建的Hive表（加载时不做检查，查询时检查）<pre><code class="nullsql"><span class="hljs-keyword">LOAD</span> <span class="hljs-keyword">DATA</span> [<span class="hljs-keyword">LOCAL</span>] INPATH <span class="hljs-string">'filepath'</span> [OVERWRITE] <span class="hljs-keyword">INTO</span> <span class="hljs-keyword">TABLE</span> tablename[<span class="hljs-keyword">PARTITION</span> (partcol1=val1, partcol2=val2 ...)]</code></pre><pre><code class="nullsql">!hadoop fs -ls input/hive/stocks_db;<span class="hljs-keyword">load</span> <span class="hljs-keyword">data</span> inpath <span class="hljs-string">'input/hive/stocks_db'</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> stocks;</code></pre></li><li>CTAS：将Hive查询结果存放入一个新创表，原子级（select失败，table不会创建），目标表不能是分区表和外部表<pre><code class="nullsql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> [<span class="hljs-keyword">IF</span> <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">EXISTS</span>] table_name <span class="hljs-keyword">AS</span> <span class="hljs-keyword">SELECT</span> …</code></pre><pre><code class="nullsql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> stocks_ctas <span class="hljs-keyword">as</span> <span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> stocks;</code></pre></li><li><p>INSERT…SELECT：将Hive查询结果存入一个已创表</p><pre><code class="nullsql"><span class="hljs-keyword">INSERT</span> OVERWRITE|<span class="hljs-keyword">INTO</span> <span class="hljs-keyword">TABLE</span> tablename [<span class="hljs-keyword">PARTITION</span> (partcol1=val1, partcol2=val2 ...)] select_statement <span class="hljs-keyword">FROM</span> from_statement</code></pre><pre><code class="nullsql">insert into table stocks_ctas select s.* from stocks s;insert overwrite table stocks_ctas select s.* from stocks s;# 可以在同一个查询中使用多个insert子句from stocks_ctasinsert into t1 select id,nameinsert into t2 select id,telwhere age&gt;25</code></pre></li><li>LOCATION：指定Hive表数据文件位置（注意：不会再移动数据文件到Hive配置的数据仓库中）<pre><code class="nullsql">!hadoop fs -ls /user/hive/input/hive/stocks_db;<span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> stocks_loc(...) <span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">','</span>location <span class="hljs-string">'/user/hive/input/hive/stocks_db'</span>;</code></pre></li></ul></li></ul><p>说明：</p><ol><li><p>Hive中有两种性质的表（Table Type）：</p><ul><li>managed_table (Hive内部表) <ul><li>使用Load data命令插数据时，会将数据文件<code>移动</code>到数据仓库（由<code>hive-site.xml</code>配置的<code>hive.metastore.warehouse.dir</code>指定）</li><li>使用Drop table命令删除表时，元数据（metastore的db中）与对应的数据文件都会被删除</li></ul></li><li>external_table (Hive外部表)<ul><li>使用Load data命令插数据时，不会将数据文件<code>移动</code>到数据仓库</li><li>使用Drop table命令删除表时，只有元数据会被删除，实际数据文件不会有影响</li></ul></li></ul></li><li><p>Hive中表的数据类型：</p><ul><li>基本数据类型<ul><li>tinyint/smallint/int/bigint</li><li>float/double</li><li>boolean</li><li>string</li></ul></li><li>复杂数据类型<ul><li>Array/Map/Struct</li><li>没有date/datetime</li></ul></li></ul></li></ol><h3 id="header-8">分区表</h3><pre><code class="nullsql"># 使用普通表，会scan整张表，效率低select * from stocks where symbol=&#39;XYZ&#39; and ymd=&#39;2003-02-01&#39;;# 使用分区表，会先找到对应分区列目录，效率高select * from stocks_partition where symbol=&#39;XYZ&#39; and ymd=&#39;2003-02-01&#39;;</code></pre><p>分区表（partition table）<br>粗粒度的划分，分区列成了目录（为虚拟列），条件查询时可定位到目录提高效率</p><ul><li>创建分区表<pre><code class="nullsql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">exists</span> stocks_partition(  col1 <span class="hljs-keyword">string</span>,  col2 <span class="hljs-keyword">string</span>,  exch_name <span class="hljs-keyword">string</span>,  yr <span class="hljs-built_in">int</span>)partitioned <span class="hljs-keyword">by</span> (symbol <span class="hljs-keyword">string</span>)<span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">','</span> ;</code></pre></li><li><p>加载数据</p><ul><li><p>Using <code>insert</code></p><pre><code class="nullsql"># add partition(symbol=B7J) data:insert into table stocks_partition partition(symbol='B7J')select col1,col2,exch_name,yr from stocks where symbol ='B7J';# add partition(symbol=BB3) data:insert into table stocks_partition partition(symbol='BB3')select col1,col2,exch_name,yr from stocks where symbol ='BB3';=&gt; 也合并成一个insertfrom stocksinsert into table stocks_partition partition(symbol='B7J')select col1,col2,exch_name,yr from stocks where symbol='B7J'insert into table stocks_partition partition(symbol='BB3')select col1,col2,exch_name,yr from stocks where symbol ='BB3';# 注意：如下方式是错误的insert overwrite table stocks_partition partition(symbol='APPL')select col1,col2,exch_name,yr from stocks where symbol='ZUU';</code></pre></li><li><p>Using <code>location</code></p><pre><code class="nullsql"># add partition(symbol=ZUU) data:insert overwrite directory &#39;output/hive/stocks-zuu&#39;select col1,col2,exch_name,yr from stocks where symbol=&#39;ZUU&#39;;alter table stocks_partition add if not exists partition (symbol=&#39;ZUU&#39;) location &#39;/output/hive/stocks-zuu&#39;</code></pre></li></ul></li><li>添加删除分区<pre><code class="nullsql"><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> stocks_partition <span class="hljs-keyword">add</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">exists</span> <span class="hljs-keyword">partition</span>(symbol=<span class="hljs-string">'ZUU'</span>);<span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> stocks_partition <span class="hljs-keyword">drop</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">exists</span> <span class="hljs-keyword">partition</span>(symbol=<span class="hljs-string">'ZUU'</span>);</code></pre></li><li>查看表分区 <pre><code class="nullsql">hive&gt; show partitions stocks_partitionOKsymbol=B7Ksymbol=BB3symbol=ZUU</code></pre></li><li><p>数据查询</p><pre><code class="nullsql">selecet * from stocks_partitions where symbol=&#39;XYZ&#39; and ymd=&#39;2003-02-01&#39;;# 若设置了strict方式，则select的where中一定要包含partition column条件查询set hive.mapred.mode=strict;select * from stocks_partitions where ymd=&#39;2003-02-01&#39;;</code></pre></li><li><p>动态分区</p><ul><li>创建动态分区表<pre><code class="nullsql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">exists</span> stocks_dynamic_partition( col1 <span class="hljs-keyword">string</span>, col2 <span class="hljs-keyword">string</span>)partitioned <span class="hljs-keyword">by</span> (exch_name <span class="hljs-keyword">string</span>,yr <span class="hljs-built_in">int</span>,sym <span class="hljs-keyword">string</span>)<span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">','</span> ;</code></pre></li><li>启动动态分区<pre><code class="nullsql"><span class="hljs-keyword">set</span> hive.exec.dynamic.partition=<span class="hljs-literal">true</span>;</code></pre></li><li><p>加载数据（注意：默认动态分区要求至少有一个静态分区）</p><pre><code class="nullsql"># 如下方式，默认会报错# SemanticException:dynamic partition strict mode requires at least one static partition columninsert overwrite table stocks_dynamic_partition partition(exch_name,yr,symbol)select col1,col2,exch_name,year(ymd),symbol from stocks;=&gt; 解决方案1：set hive.exec.dynamic.partition.mode=nostrict;=&gt; 解决方案2：insert overwrite table stocks_dynamic_partition partition(exch_name='ABCSE',yr,symbol)select col1,col2,exch_name,year(ymd),symbol from stocks;</code></pre><ul><li>查看表<pre><code class="nullsql"><span class="hljs-keyword">show</span> <span class="hljs-keyword">partitions</span> stocks_dynamic_partition;<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> stocks_dynamic_partition <span class="hljs-keyword">where</span> exch_name=<span class="hljs-string">'ABCSE'</span> <span class="hljs-keyword">and</span> yr=<span class="hljs-number">2013</span> <span class="hljs-keyword">limit</span> <span class="hljs-number">10</span>;</code></pre></li><li>注意动态分区的分区数量是有限制的，可根据需要扩大设置（不推荐partition数量过多）：<pre><code class="nullsql"><span class="hljs-keyword">set</span> hive.exec.max.dynamic.partitions=<span class="hljs-number">1000</span>;<span class="hljs-keyword">set</span> hive.exec.max.dynamic.partitions.pernode=<span class="hljs-number">500</span>;</code></pre></li><li>数据表的目录结构<pre><code>stocks_dynamic_partition/exch_name=ABCSE/yr=2013/symbol=GEL/stocks_dynamic_partition/exch_name=ABCSE/yr=2013/symbol=ZUU/stocks_dynamic_partition/exch_name=ABCSE/yr=2014/symbol=GEL/...</code></pre></li></ul></li></ul></li></ul><h3 id="header-9">桶表</h3><p>桶表(Bucket Table)</p><ul><li>细粒度的划分，桶列仍在数据文件中</li><li>主要应用：<ul><li>提高数据抽样效率</li><li>提升某些查询操作效率，例如mapside join</li></ul></li></ul><ul><li>创建表<pre><code class="nullsql"> # 必须设置这个数据，hive才会按照你设置的桶的个数去生成数据 set hive.enforce.bucketing = true; create table t4(id int) clustered by(id) into 4 buckets;</code></pre></li><li>插入数据<pre><code class="nullsql"> insert into table t4 select id from t3;      # 追加 insert overrite table t4 select id from t3;  # 全部重写</code></pre></li><li><p>抽样查询</p><pre><code class="nullsql"># 查询带桶的表(在一部分桶上检索，效率高)select * from t4 tablesample(bucket 1 out of 4 on id);# 不带桶的表(会在整个数据集上检索，效率低)select * from t3 tablesample(bucket 1 out of 4 on id);select * from t3 tablesample(bucket 1 out of 4 on rand());</code></pre></li><li>数据表的目录结构<pre><code>t4/000000_0t4/000000_1t4/000000_2t4/000000_3</code></pre></li><li><p>分区+分桶：</p><pre><code class="nullsql"># 创建表create table if not exists stocks_bucket(  col1 string,  col2 string,  symbol string)partitioned by (exch_name string,yr string)clustered by (symbol) into 3 bucketsrow format delimited fields terminated by &#39;,&#39; ;</code></pre><pre><code class="nullsql"># 设置动态分区和使用桶set hive.exec.dynamic.partition=true;set hive.enforce.bucketing = true; # 插入数据insert into table stocks_bucket partition (exch_name=&#39;ABCE&#39;,yr)select col1,col2,year(ymd) from stocks</code></pre><pre><code class="nullsql"># 抽样查询对比select * from stocks tablesample(bucket 3 out of 5 on symbol) s;        # 低效select * from stocks_bucket tablesample(bucket 3 out of 5 on symbol) s; # 高效</code></pre><pre><code class="nullsql"># 数据表的目录结构stocks_bucket/exch_name=ABCE/yr=2013stocks_bucket/exch_name=ABCE/yr=2013/000000_0stocks_bucket/exch_name=ABCE/yr=2013/000000_1stocks_bucket/exch_name=ABCE/yr=2013/000000_2stocks_bucket/exch_name=ABCE/yr=2014stocks_bucket/exch_name=ABCE/yr=2014/000000_0stocks_bucket/exch_name=ABCE/yr=2014/000000_1stocks_bucket/exch_name=ABCE/yr=2014/000000_2...</code></pre></li></ul><h3 id="header-10">抽样</h3><p>tablesample 抽样</p><ul><li>tablesample(n precent/rows)<ul><li>n precent</li><li>n rows</li></ul></li><li>tablesample(nM)<ul><li>n兆</li></ul></li><li><p>tablesample(bucket x out of y [on columns])</p><ul><li>x: 从第几个桶开始抽样（从1开始）</li><li>y: 抽样的桶数（若是分桶表，则必须为总bucket数的倍数或者因子）</li><li>columns: 抽样的列</li><li>注意：<ul><li>基于已经分桶的表抽样，查询只会扫描相应桶中的数据</li><li>基于未分桶表的抽样，查询时候需要扫描整表数据</li></ul></li><li><p>示例：</p><pre><code class="nullsql"># 1. t1 为未分桶表# 1.1 scan全表，根据col1分为10个桶，从第3个桶中取数据；select * from t1 tablesample(bucket 3 out of 10 on col1);# 1.2 scan全表，根据随机数分为10个桶，从第3个桶中取数据；select * from t1 tablesample(bucket 3 out of 10 on rand());# 2. t2 为分桶表，有10个桶# 2.1 直接从第3个桶中取数据select * from t2 tablesample(bucket 3 out of 10 on col1);# 2.2 共抽取2(10/5)个桶的数据，从第3个和第8(3+5)个桶中抽取数据select * from t2 tablesample(bucket 3 out of 5 on col1);# 2.3 共抽取0.5(10/20)个桶的数据，从第3个桶中抽取一半数据select * from t2 tablesample(bucket 3 out of 20 on col1);</code></pre></li></ul></li><li>例如：<ul><li>tablesample(50 precent)</li><li>tablesample(50 rows)</li><li>tablesample(50M)</li><li>tablesample(bucket 3 out of 10)</li><li>tablesample(bucket 3 out of 10 on rand())</li></ul></li></ul><h3 id="header-11">单表查询</h3><pre><code class="nullsql"><span class="hljs-keyword">SELECT</span> [ALL | <span class="hljs-keyword">DISTINCT</span>] select_expr, select_expr, ...  <span class="hljs-keyword">FROM</span> table_reference   [<span class="hljs-keyword">WHERE</span> condition]   [<span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> col_list] [<span class="hljs-keyword">Having</span> condition]  [CLUSTER <span class="hljs-keyword">BY</span> col_list | [<span class="hljs-keyword">DISTRIBUTE</span> <span class="hljs-keyword">BY</span> col_list] [<span class="hljs-keyword">SORT</span> <span class="hljs-keyword">BY</span> col_list] | [<span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> col_list] ]  [<span class="hljs-keyword">LIMIT</span> <span class="hljs-built_in">number</span>]</code></pre><ul><li>where：过滤（mapper端）</li><li>group by：局部分组（reducer端），select中可使用一些聚合函数，例如sum，avg，count等</li><li>cluster by：等价于distribute by + sort by ,只是无法指定排序规则（默认asc）</li><li>distribute by：分区（partitioner），按指定字段划分数据到各个reduce/file</li><li>sort by：局部排序（reducer端）</li><li>order by：全局排序，只有一个reducer（数据量很大时慎用）</li><li>limit：减少数据量，传输到reduce端（单机）的数据记录数就减少到<code>n*(map个数)</code></li></ul><p>示例：</p><ul><li>加查询结果写入HDFS中<pre><code class="nullsql"><span class="hljs-keyword">insert</span> overwrite <span class="hljs-keyword">local</span> <span class="hljs-keyword">directory</span> <span class="hljs-string">'/home/hive/output/hive/stocks'</span><span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">','</span><span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> stocks <span class="hljs-keyword">distributed</span> <span class="hljs-keyword">by</span> symbol <span class="hljs-keyword">sort</span> <span class="hljs-keyword">by</span> symbol <span class="hljs-keyword">asc</span>,price_close <span class="hljs-keyword">desc</span>;</code></pre></li><li>全局排序 order by<pre><code class="nullsql"># 无论设置了多少个reducer，这里只会使用一个reducer（数据量很大时效率低）set mapreduce.job.reduces=3;select * from stocks order by price_close desc;</code></pre></li><li>局部排序 sort by<pre><code class="nullsql"># 每个reducer中排序set mapreduce.job.reduces=3;select * from stocks sort by price_close desc;</code></pre></li><li>分区 distribute/cluster by<pre><code class="nullsql"># distribute by 控制某个特定行应该到哪个reducer# sort by 为每个reducer产生一个排好序的文件# distribute by + sort by = cluster byset mapreduce.job.reduces=3;select * from stocks distributed by symbol sort by symbol asc;select * from stocks cluster by symbol;</code></pre></li><li>聚合操作 group by<pre><code class="nullsql"><span class="hljs-keyword">select</span> symbol,<span class="hljs-keyword">count</span>(*) <span class="hljs-keyword">from</span> stocks <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> symbol;</code></pre></li><li>Top N查询<pre><code class="nullsql"><span class="hljs-keyword">SET</span> mapred.reduce.tasks = <span class="hljs-number">1</span><span class="hljs-keyword">SELECT</span> * <span class="hljs-keyword">FROM</span> sales <span class="hljs-keyword">SORT</span> <span class="hljs-keyword">BY</span> amount <span class="hljs-keyword">DESC</span> <span class="hljs-keyword">LIMIT</span> <span class="hljs-number">5</span></code></pre></li></ul><h3 id="header-12">连接查询</h3><ul><li><p>使用Join</p><ul><li><code>{inner} join</code>,<code>{left|right|full} [outer] join</code>,<code>cross join</code><pre><code class="nullsql"><span class="hljs-keyword">SELECT</span> a.* <span class="hljs-keyword">FROM</span> a <span class="hljs-keyword">JOIN</span> b <span class="hljs-keyword">ON</span> (a.id = b.id <span class="hljs-keyword">AND</span> a.department = b.department);<span class="hljs-keyword">SELECT</span> a.* <span class="hljs-keyword">FROM</span> a <span class="hljs-keyword">LEFT</span> <span class="hljs-keyword">JOIN</span> b <span class="hljs-keyword">ON</span> (a.id = b.id <span class="hljs-keyword">AND</span> a.department = b.department);<span class="hljs-keyword">SELECT</span> a.val, b.val, c.val <span class="hljs-keyword">FROM</span> a <span class="hljs-keyword">JOIN</span> b <span class="hljs-keyword">ON</span> (a.key = b.key1) <span class="hljs-keyword">JOIN</span> c <span class="hljs-keyword">ON</span> (c.key = b.key2);</code></pre></li><li>特例：<code>LEFT SEMI JOIN</code><pre><code class="nullsql"><span class="hljs-keyword">SELECT</span> a.key, a.value <span class="hljs-keyword">FROM</span> a <span class="hljs-keyword">WHERE</span> a.key <span class="hljs-keyword">in</span> (<span class="hljs-keyword">SELECT</span> b.key <span class="hljs-keyword">FROM</span> B);<span class="hljs-keyword">SELECT</span> a.key, a.val <span class="hljs-keyword">FROM</span> a <span class="hljs-keyword">LEFT</span> SEMI <span class="hljs-keyword">JOIN</span> b <span class="hljs-keyword">ON</span> (a.key = b.key);</code></pre></li></ul></li><li><p>Hive中的Join可分为：</p><ul><li>Common Join（Reduce阶段完成join）<ul><li>Map阶段：读取源表的数据，以Join on条件中的列为key，以join后所关心的(select或者where中需要用到的)列为value（value中包含Tag，用于标明此value对应哪个表）</li><li>Shuffle阶段：根据key的值进行hash,并将key/value按照hash值推送至不同的reduce中，这样确保两个表中相同的key位于同一个reduce中</li><li>Reduce阶段：根据key的值完成join操作，通过Tag识别不同表中的数据</li></ul></li><li>Map Join（Map阶段完成join）：通常用于一个很小的表和一个大表进行join的场景<ul><li>Local Task（Client端本地执行）：扫描小表，装载到DistributeCache中</li><li>Map Task：读取DistributeCache数据至内存，遍历大表记录，两者进行join，输出结果</li><li>无Reducer Task</li></ul></li></ul></li><li><p>Hive中的join操作</p><ul><li><p>join操作是在where操作之前执行，即where条件不能起到减少join数据的作用，应尽量在<code>on</code>中加入约束条件</p><pre><code class="nullsql"><span class="hljs-keyword">SELECT</span> a.val, b.val <span class="hljs-keyword">FROM</span> a <span class="hljs-keyword">JOIN</span> b <span class="hljs-keyword">ON</span> (a.key=b.key)<span class="hljs-keyword">WHERE</span> a.ds=<span class="hljs-string">'2009-07-07'</span> <span class="hljs-keyword">AND</span> b.ds=<span class="hljs-string">'2009-07-07'</span>=&gt; 优化为：<span class="hljs-keyword">SELECT</span> a.val, b.val <span class="hljs-keyword">FROM</span> a <span class="hljs-keyword">JOIN</span> b <span class="hljs-keyword">ON</span> (a.key=b.key <span class="hljs-keyword">AND</span> b.ds=<span class="hljs-string">'2009-07-07'</span> <span class="hljs-keyword">AND</span> a.ds=<span class="hljs-string">'2009-07-07'</span>)</code></pre></li><li><p>多表连接，会转换成多个MR Job，但关联条件相同的多表join会自动优化成一个mapreduce job</p><pre><code class="nullsql"># 在两个mapred程序中执行joinSELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2)# 在一个mapre程序中执行joinSELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)</code></pre></li><li>多表连接，前一个join生成的数据会缓存到内存，通过stream取后一张表数据，应尽量将记录多的表放在后面join，也可使用<code>/*+ STREAMTABLE(table) */</code>指定将哪个大表stream化<pre><code class="nullsql"><span class="hljs-keyword">SELECT</span> <span class="hljs-comment">/*+ STREAMTABLE(a) */</span> a.val, b.val, c.val <span class="hljs-keyword">FROM</span> a <span class="hljs-keyword">JOIN</span> b <span class="hljs-keyword">ON</span> (a.key = b.key1) <span class="hljs-keyword">JOIN</span> c <span class="hljs-keyword">ON</span> (c.key = b.key1)</code></pre></li><li><p>Map Side Join</p><ul><li>可在查询中使用<code>/*+ mapjoin(table) */</code> 指定将哪个小表装载到DistributeCache中<pre><code class="nullsql"># 注意： 这里无法使用a FULL/RIGHT JOIN bSELECT /*+ MAPJOIN(b) */ a.key, a.value FROM a join b on a.key = b.key;</code></pre></li><li>Auto Map Side Join：系统自动判断使用mapjoin（由参数hive.auto.convert.join决定，默认为true）<pre><code class="nullsql"># Local Task 中找出符合mapjoin条件的表，装载到DistributeCache中，后续使用map join；若未找到符合条件的表，则使用common joinset hive.auto.convert.join=true;# 根据参数hive.mapjoin.smalltable.filesize的设置判断mapjoin的表SELECT a.key, a.value FROM a join b on a.key = b.key;</code></pre></li><li>与map join相关的hive参数<pre><code># hive.join.emit.interval # hive.auto.convert.join # hive.mapjoin.smalltable.filesize# hive.mapjoin.size.key  # hive.mapjoin.cache.numrows</code></pre></li><li><p>Sort Map Bucket Map Join：根据join key将各个关联表进行Bucket，提高join效率</p><pre><code class="nullsql"># 创建分桶表create table a_smb(...) clustered by (key) sort by (key) into 10 buckets;create table b_smb(...) clustered by (key) sort by (key) into 5 buckets;# 为分桶表加载数据set hive.enforce.bucketing=true;insert into table a_smb select * from a;insert into table b_smb select * from b;# 打开SMB Map Joinset hive.auto.convert.sortmerge.join=true;set hive.optimize.bucketmapjoin = true; set hive.optimize.bucketmapjoin.sortedmerge = true; set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat; select * from a_smb a join b_smb on a.key=b.key;</code></pre></li></ul></li></ul></li></ul><h2 id="header-13">示例</h2><h3 id="header-14">示例1：导入Apache log</h3><ul><li>log格式:<pre><code>127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] &quot;GET /apache_pb.gif HTTP/1.0&quot; 200 2326</code></pre></li><li><p>创建表:</p><pre><code class="nullsql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> apachelog (host <span class="hljs-keyword">STRING</span>,<span class="hljs-keyword">identity</span> <span class="hljs-keyword">STRING</span>,username <span class="hljs-keyword">STRING</span>,<span class="hljs-keyword">time</span> <span class="hljs-keyword">STRING</span>,request <span class="hljs-keyword">STRING</span>,<span class="hljs-keyword">status</span> <span class="hljs-keyword">STRING</span>,<span class="hljs-keyword">size</span> <span class="hljs-keyword">STRING</span>,referer <span class="hljs-keyword">STRING</span>,<span class="hljs-keyword">agent</span> <span class="hljs-keyword">STRING</span>)<span class="hljs-keyword">ROW</span> <span class="hljs-keyword">FORMAT</span> SERDE <span class="hljs-string">'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'</span><span class="hljs-keyword">WITH</span> SERDEPROPERTIES (  <span class="hljs-string">"input.regex"</span> = <span class="hljs-string">"([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) ([^ \"]*|\"[^\"]*\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\".*\") ([^ \"]*|\".*\"))?"</span>,  <span class="hljs-string">"output.format.string"</span> = <span class="hljs-string">"%1$s %2$s %3$s %4$s %5$s %6$s %7$s %8$s %9$s"</span>)<span class="hljs-keyword">STORED</span> <span class="hljs-keyword">AS</span> TEXTFILE;</code></pre></li><li><p>添加jar包到hive的执行环境:</p><pre><code>add jar $HIVE_HOME/lib/hive-contrib-1.2.1.jar</code></pre></li><li>加载数据:<pre><code># inpath will be deleted!load data inpath &#39;/input/access_2013_05_31.log&#39; into table apachelog;</code></pre></li><li>查询:<pre><code>show tables;describe formatted apachelog;select * from apachelog limit 10;select count(*) from apachelog;select status,count(*) from apachelog group by status;</code></pre></li><li>HDFS目录:<pre><code>/user/hive/warehouse/apachelog/user/hive/warehouse/apachelog/access_2013_05_31.log</code></pre></li></ul><h3 id="header-15">示例2：创建内部表</h3><ul><li>创建内部表log:<pre><code class="nullsql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> <span class="hljs-keyword">log</span>(  ip <span class="hljs-keyword">string</span>,  datetime <span class="hljs-keyword">string</span>,  method <span class="hljs-keyword">string</span>,  <span class="hljs-keyword">url</span> <span class="hljs-keyword">string</span>,  <span class="hljs-keyword">status</span> <span class="hljs-keyword">string</span>,  <span class="hljs-keyword">size</span> <span class="hljs-keyword">string</span>)<span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">','</span> ;</code></pre></li><li><p>插入数据:</p><pre><code class="nullsql">insert into table log select host,time,substring(split(request,' ')[0],2) as method,split(request,' ')[1] as url,status,size from apachelog; # 重新过滤掉某些数据（hive中没有delete操作，只能overwrite）insert overwrite table log select * from log where length(method)&lt;7 and method in ('GET','POST','PUT','DELETE','OPTION','HEAD');</code></pre></li><li><p>查询:</p><pre><code class="nullsql"><span class="hljs-keyword">describe</span> formatted <span class="hljs-keyword">log</span>;<span class="hljs-keyword">set</span> mapred.reduce.tasks=<span class="hljs-number">3</span>;<span class="hljs-keyword">select</span> method <span class="hljs-keyword">from</span> <span class="hljs-keyword">log</span> <span class="hljs-keyword">where</span> <span class="hljs-keyword">length</span>(method)&lt;<span class="hljs-number">7</span> <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> method;<span class="hljs-keyword">select</span> <span class="hljs-keyword">status</span>,method,<span class="hljs-keyword">count</span>(*) <span class="hljs-keyword">from</span> <span class="hljs-keyword">log</span> <span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> <span class="hljs-keyword">status</span>,method;</code></pre></li><li>HDFS目录:<pre><code>/user/hive/warehouse/log/user/hive/warehouse/log/000000_0</code></pre></li></ul><h3 id="header-16">示例3：内部静态分区表</h3><ul><li>创建内部分区表log_partition:（注意分区字段不能包含在建表字段中）<pre><code class="nullsql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> log_partition(  ip <span class="hljs-keyword">string</span>,  datetime <span class="hljs-keyword">string</span>,  <span class="hljs-keyword">url</span> <span class="hljs-keyword">string</span>,  <span class="hljs-keyword">size</span> <span class="hljs-keyword">string</span>)partitioned <span class="hljs-keyword">by</span> (<span class="hljs-keyword">status</span> <span class="hljs-keyword">string</span>,method <span class="hljs-keyword">string</span>)<span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">','</span> ;</code></pre></li><li><p>插入partition数据:（使用insert…select）</p><pre><code class="nullsql"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> log_partition <span class="hljs-keyword">partition</span>(<span class="hljs-keyword">status</span>=<span class="hljs-string">'200'</span>,method=<span class="hljs-string">'GET'</span>)<span class="hljs-keyword">select</span> ip,datetime,<span class="hljs-keyword">url</span>,<span class="hljs-keyword">size</span> <span class="hljs-keyword">from</span> <span class="hljs-keyword">log</span> <span class="hljs-keyword">where</span> <span class="hljs-keyword">status</span>=<span class="hljs-string">'200'</span> <span class="hljs-keyword">and</span> method=<span class="hljs-string">'GET'</span>;<span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> log_partition <span class="hljs-keyword">partition</span>(<span class="hljs-keyword">status</span>=<span class="hljs-string">'200'</span>,method=<span class="hljs-string">'POST'</span>)<span class="hljs-keyword">select</span> ip,datetime,<span class="hljs-keyword">url</span>,<span class="hljs-keyword">size</span> <span class="hljs-keyword">from</span> <span class="hljs-keyword">log</span> <span class="hljs-keyword">where</span> <span class="hljs-keyword">status</span>=<span class="hljs-string">'200'</span> <span class="hljs-keyword">and</span> method=<span class="hljs-string">'POST'</span><span class="hljs-keyword">from</span> <span class="hljs-keyword">log</span><span class="hljs-keyword">insert</span> overwrite <span class="hljs-keyword">table</span> log_partition <span class="hljs-keyword">partition</span>(<span class="hljs-keyword">status</span>=<span class="hljs-string">'301'</span>,method=<span class="hljs-string">'GET'</span>)<span class="hljs-keyword">select</span> ip,datetime,<span class="hljs-keyword">url</span>,<span class="hljs-keyword">size</span> <span class="hljs-keyword">where</span> <span class="hljs-keyword">status</span>=<span class="hljs-string">'301'</span> <span class="hljs-keyword">and</span> method=<span class="hljs-string">'GET'</span><span class="hljs-keyword">insert</span> overwrite <span class="hljs-keyword">table</span> log_partition <span class="hljs-keyword">partition</span>(<span class="hljs-keyword">status</span>=<span class="hljs-string">'301'</span>,method=<span class="hljs-string">'POST'</span>)<span class="hljs-keyword">select</span> ip,datetime,<span class="hljs-keyword">url</span>,<span class="hljs-keyword">size</span> <span class="hljs-keyword">where</span> <span class="hljs-keyword">status</span>=<span class="hljs-string">'301'</span> <span class="hljs-keyword">and</span> method=<span class="hljs-string">'POST'</span><span class="hljs-keyword">insert</span> overwrite <span class="hljs-keyword">table</span> log_partition <span class="hljs-keyword">partition</span>(<span class="hljs-keyword">status</span>=<span class="hljs-string">'400'</span>,method=<span class="hljs-string">'GET'</span>)<span class="hljs-keyword">select</span> ip,datetime,<span class="hljs-keyword">url</span>,<span class="hljs-keyword">size</span> <span class="hljs-keyword">where</span> <span class="hljs-keyword">status</span>=<span class="hljs-string">'400'</span><span class="hljs-keyword">and</span> method=<span class="hljs-string">'GET'</span>;</code></pre></li><li><p>插入partition数据:（使用alter…location）</p><pre><code class="nullsql"><span class="hljs-keyword">insert</span> overwrite <span class="hljs-keyword">directory</span> <span class="hljs-string">'/user/hive/warehouse/log_partition/status=400/method=POST'</span><span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">','</span><span class="hljs-keyword">select</span> ip,datetime,<span class="hljs-keyword">url</span>,<span class="hljs-keyword">size</span> <span class="hljs-keyword">from</span> <span class="hljs-keyword">log</span> <span class="hljs-keyword">where</span> <span class="hljs-keyword">status</span>=<span class="hljs-string">'400'</span> <span class="hljs-keyword">and</span> method=<span class="hljs-string">'POST'</span>;<span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> log_partition <span class="hljs-keyword">add</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">exists</span> <span class="hljs-keyword">partition</span> (<span class="hljs-keyword">status</span>=<span class="hljs-string">'400'</span>,method=<span class="hljs-string">'POST'</span>) location <span class="hljs-string">'/user/hive/warehouse/log_partition/status=400/method=POST'</span>;</code></pre></li><li>删除partition数据:（注意会删除partition对应的目录和文件）<pre><code class="nullsql"><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> log_partition <span class="hljs-keyword">drop</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">exists</span> <span class="hljs-keyword">partition</span> (<span class="hljs-keyword">status</span>=<span class="hljs-string">'301'</span>,method=<span class="hljs-string">'GET'</span>);</code></pre></li><li>查看:<pre><code class="nullsql"><span class="hljs-keyword">describe</span> formatted log_partition;<span class="hljs-keyword">show</span> <span class="hljs-keyword">partitions</span> log_partition;<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> log_partition <span class="hljs-keyword">where</span> <span class="hljs-keyword">status</span>=<span class="hljs-string">'400'</span> <span class="hljs-keyword">and</span> method=<span class="hljs-string">'POST'</span>;</code></pre></li><li>HDFS目录:<pre><code>/user/hive/warehouse/log_partition/user/hive/warehouse/log_partition/status=200/user/hive/warehouse/log_partition/status=200/method=GET/user/hive/warehouse/log_partition/status=200/method=GET/000000_0/user/hive/warehouse/log_partition/status=301/user/hive/warehouse/log_partition/status=301/method=POST/user/hive/warehouse/log_partition/status=301/method=POST/000000_0/user/hive/warehouse/log_partition/status=400/user/hive/warehouse/log_partition/status=400/method=GET/user/hive/warehouse/log_partition/status=400/method=GET/000000_0/user/hive/warehouse/log_partition/status=400/method=POST/user/hive/warehouse/log_partition/status=400/method=POST/000000_0</code></pre></li></ul><h3 id="header-17">示例4：内部动态分区表</h3><ul><li>创建内部分区表log_dynamic_partition:<pre><code class="nullsql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> log_dynamic_partition <span class="hljs-keyword">like</span> log_partition;</code></pre></li><li>打开动态分区:<pre><code class="nullsql"><span class="hljs-keyword">set</span> hive.exec.dynamic.partition=<span class="hljs-literal">true</span>;</code></pre></li><li>插入数据:（注意：1. 字段和顺序；2. 第一个为静态partition）<pre><code class="nullsql"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> log_dynamic_partition <span class="hljs-keyword">partition</span>(<span class="hljs-keyword">status</span>=<span class="hljs-string">'200'</span>,method)<span class="hljs-keyword">select</span> ip,datetime,<span class="hljs-keyword">url</span>,<span class="hljs-keyword">size</span>,method <span class="hljs-keyword">from</span> <span class="hljs-keyword">log</span> <span class="hljs-keyword">where</span> <span class="hljs-keyword">status</span>=<span class="hljs-string">'200'</span>;</code></pre></li><li>插入数据:（注意：1. 字段和顺序；2. 所有都为动态partition）<pre><code class="nullsql"># 设置为nostrict模式set hive.exec.dynamic.partition.mode=nostrict;# 插入insert overwrite table log_dynamic_partition partition(status,method)select ip,datetime,url,size,status,method from log;</code></pre></li><li>查看:<pre><code class="nullsql"><span class="hljs-keyword">describe</span> formatted log_dynamic_partition;<span class="hljs-keyword">show</span> <span class="hljs-keyword">partitions</span> log_dynamic_partition;<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> log_dynamic_partition <span class="hljs-keyword">where</span> <span class="hljs-keyword">status</span>=<span class="hljs-string">'400'</span> <span class="hljs-keyword">and</span> method=<span class="hljs-string">'POST'</span>;</code></pre></li><li>HDFS目录:<pre><code>/user/hive/warehouse/log_dynamic_partition/user/hive/warehouse/log_dynamic_partition/status=200/user/hive/warehouse/log_dynamic_partition/status=200/method=GET/user/hive/warehouse/log_dynamic_partition/status=200/method=GET/000000_0/user/hive/warehouse/log_dynamic_partition/status=301/user/hive/warehouse/log_dynamic_partition/status=301/method=GET/user/hive/warehouse/log_dynamic_partition/status=301/method=GET/000000_0/user/hive/warehouse/log_dynamic_partition/status=301/method=POST/user/hive/warehouse/log_dynamic_partition/status=301/method=POST/000000_0/user/hive/warehouse/log_dynamic_partition/status=400/user/hive/warehouse/log_dynamic_partition/status=400/method=GET/user/hive/warehouse/log_dynamic_partition/status=400/method=GET/000000_0/user/hive/warehouse/log_dynamic_partition/status=400/method=POST/user/hive/warehouse/log_dynamic_partition/status=400/method=POST/000000_0...</code></pre></li></ul><h3 id="header-18">示例5：内部表分桶</h3><ul><li>创建分桶表log_bucket: （注意：分桶字段为建表中的字段）<pre><code class="nullsql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> log_bucket(  ip <span class="hljs-keyword">string</span>,  datetime <span class="hljs-keyword">string</span>,  method <span class="hljs-keyword">string</span>,  <span class="hljs-keyword">url</span> <span class="hljs-keyword">string</span>,  <span class="hljs-keyword">status</span> <span class="hljs-keyword">string</span>,  <span class="hljs-keyword">size</span> <span class="hljs-keyword">string</span>)clustered <span class="hljs-keyword">by</span> (<span class="hljs-keyword">status</span>,method) <span class="hljs-keyword">into</span> <span class="hljs-number">5</span> buckets<span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">','</span> ;</code></pre></li><li>打开分桶:<pre><code class="nullsql"><span class="hljs-keyword">set</span> hive.enforce.bucketing = <span class="hljs-literal">true</span>;</code></pre></li><li>插入数据:<pre><code class="nullsql"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> log_bucket<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> <span class="hljs-keyword">log</span>;</code></pre></li><li>查看:<pre><code class="nullsql"><span class="hljs-keyword">describe</span> formatted log_bucket;<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> log_bucket tablesample(bucket <span class="hljs-number">1</span> <span class="hljs-keyword">out</span> <span class="hljs-keyword">of</span> <span class="hljs-number">5</span> <span class="hljs-keyword">on</span> <span class="hljs-keyword">status</span>);<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> <span class="hljs-keyword">log</span> tablesample(bucket <span class="hljs-number">1</span> <span class="hljs-keyword">out</span> <span class="hljs-keyword">of</span> <span class="hljs-number">5</span> <span class="hljs-keyword">on</span> <span class="hljs-keyword">status</span>);</code></pre></li><li>HDFS目录:<pre><code>/user/hive/warehouse/log_bucket/user/hive/warehouse/log_bucket/000000_0/user/hive/warehouse/log_bucket/000001_0/user/hive/warehouse/log_bucket/000002_0/user/hive/warehouse/log_bucket/000003_0/user/hive/warehouse/log_bucket/000004_0</code></pre></li></ul><h3 id="header-19">示例6：内部分区分桶表</h3><ul><li>创建分区分桶表log_partition_bucket:<pre><code class="nullsql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> log_partition_bucket(  ip <span class="hljs-keyword">string</span>,  datetime <span class="hljs-keyword">string</span>,  <span class="hljs-keyword">url</span> <span class="hljs-keyword">string</span>,  <span class="hljs-keyword">size</span> <span class="hljs-keyword">string</span>)partitioned <span class="hljs-keyword">by</span> (<span class="hljs-keyword">status</span> <span class="hljs-keyword">string</span>,method <span class="hljs-keyword">string</span>) clustered <span class="hljs-keyword">by</span> (ip) <span class="hljs-keyword">into</span> <span class="hljs-number">5</span> buckets<span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">','</span> ;</code></pre></li><li>打开动态分区和分桶:<pre><code class="nullsql"><span class="hljs-keyword">set</span> hive.exec.dynamic.partition=<span class="hljs-literal">true</span>;<span class="hljs-keyword">set</span> hive.exec.dynamic.partition.mode=nostrict;<span class="hljs-keyword">set</span> hive.enforce.bucketing = <span class="hljs-literal">true</span>;</code></pre></li><li>插入数据:<pre><code class="nullsql"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> log_partition_bucket <span class="hljs-keyword">partition</span>(<span class="hljs-keyword">status</span>,method)<span class="hljs-keyword">select</span> ip,datetime,<span class="hljs-keyword">url</span>,<span class="hljs-keyword">size</span>,<span class="hljs-keyword">status</span>,method <span class="hljs-keyword">from</span> <span class="hljs-keyword">log</span>;</code></pre></li><li>查看:<pre><code class="nullsql"><span class="hljs-keyword">describe</span> formatted log_partition_bucket;<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> log_partition_bucket tablesample(bucket <span class="hljs-number">1</span> <span class="hljs-keyword">out</span> <span class="hljs-keyword">of</span> <span class="hljs-number">5</span> <span class="hljs-keyword">on</span> <span class="hljs-keyword">status</span>) <span class="hljs-keyword">limit</span> <span class="hljs-number">5</span>;</code></pre></li><li>HDFS目录:<pre><code>/user/hive/warehouse/log_partition_bucket/user/hive/warehouse/log_partition_bucket/status=200/user/hive/warehouse/log_partition_bucket/status=200/method=GET/user/hive/warehouse/log_partition_bucket/status=200/method=GET/000000_0/user/hive/warehouse/log_partition_bucket/status=200/method=GET/000001_0/user/hive/warehouse/log_partition_bucket/status=200/method=GET/000002_0/user/hive/warehouse/log_partition_bucket/status=200/method=GET/000003_0/user/hive/warehouse/log_partition_bucket/status=200/method=GET/000004_0/user/hive/warehouse/log_partition_bucket/status=301/user/hive/warehouse/log_partition_bucket/status=301/method=GET/user/hive/warehouse/log_partition_bucket/status=301/method=GET/000000_0/user/hive/warehouse/log_partition_bucket/status=301/method=GET/000001_0/user/hive/warehouse/log_partition_bucket/status=301/method=GET/000002_0/user/hive/warehouse/log_partition_bucket/status=301/method=GET/000003_0/user/hive/warehouse/log_partition_bucket/status=301/method=GET/000004_0/user/hive/warehouse/log_partition_bucket/status=301/method=POST/user/hive/warehouse/log_partition_bucket/status=301/method=POST/000000_0/user/hive/warehouse/log_partition_bucket/status=301/method=POST/000001_0/user/hive/warehouse/log_partition_bucket/status=301/method=POST/000002_0/user/hive/warehouse/log_partition_bucket/status=301/method=POST/000003_0/user/hive/warehouse/log_partition_bucket/status=301/method=POST/000004_0...</code></pre></li></ul><h3 id="header-20">示例6：外部表</h3><ul><li>创建外部表log_external:<pre><code class="nullsql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">external</span> <span class="hljs-keyword">table</span> log_external(  ip <span class="hljs-keyword">string</span>,  datetime <span class="hljs-keyword">string</span>,  method <span class="hljs-keyword">string</span>,  <span class="hljs-keyword">url</span> <span class="hljs-keyword">string</span>,  <span class="hljs-keyword">status</span> <span class="hljs-keyword">string</span>,  <span class="hljs-keyword">size</span> <span class="hljs-keyword">string</span>)<span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">','</span> location <span class="hljs-string">'/input/external'</span>;</code></pre></li><li><p>插入数据:</p><pre><code class="nullsql"># generate file: /input/external/000000_0insert into table log_externalselect * from log where status=&#39;200&#39;;# generate file: /input/external/000000_0_copy_1insert into table log_externalselect * from log where status=&#39;400&#39;;</code></pre></li><li><p>插入数据:</p><pre><code class="nullsql"># generate file: /input/log_301/000000_0insert overwrite directory &#39;/input/log_301&#39;row format delimited fields terminated by &#39;,&#39;select * from log where status=&#39;301&#39;; # generate file: /input/external/000000_0_copy_2# delete file: /input/log_301/000000_0load data inpath &#39;/input/log_301&#39; into table log_external;</code></pre></li><li>查看:<pre><code class="nullsql"><span class="hljs-keyword">describe</span> formatted log_external;<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> log_external <span class="hljs-keyword">limit</span> <span class="hljs-number">5</span>;<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> log_external <span class="hljs-keyword">where</span> <span class="hljs-keyword">status</span>=<span class="hljs-string">'400'</span> <span class="hljs-keyword">limit</span> <span class="hljs-number">5</span>;<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> log_external <span class="hljs-keyword">where</span> <span class="hljs-keyword">status</span>=<span class="hljs-string">'301'</span> <span class="hljs-keyword">limit</span> <span class="hljs-number">5</span>;</code></pre></li><li>HDFS目录:<pre><code>/input/external/000000_0/input/external/000000_0_copy_1/input/external/000000_0_copy_2</code></pre></li><li>删除表:（注意：1. 无法使用truncate清空外部表；2. 数据文件不会被删除）<pre><code class="nullsql"><span class="hljs-keyword">drop</span> <span class="hljs-keyword">table</span> log_external;</code></pre></li><li>重新创建外部表:（不用重新插入数据就有数据可查出了）<pre><code class="nullsql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">external</span> <span class="hljs-keyword">table</span> log_external(  ip <span class="hljs-keyword">string</span>,  datetime <span class="hljs-keyword">string</span>,  method <span class="hljs-keyword">string</span>,  <span class="hljs-keyword">url</span> <span class="hljs-keyword">string</span>,  <span class="hljs-keyword">status</span> <span class="hljs-keyword">string</span>,  <span class="hljs-keyword">size</span> <span class="hljs-keyword">string</span>)<span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">','</span> location <span class="hljs-string">'/input/external'</span>;</code></pre></li><li>查看表:<pre><code class="nullsql"><span class="hljs-keyword">describe</span> formatted log_external;<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> log_external <span class="hljs-keyword">limit</span> <span class="hljs-number">5</span>;<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> log_external <span class="hljs-keyword">where</span> <span class="hljs-keyword">status</span>=<span class="hljs-string">'400'</span> <span class="hljs-keyword">limit</span> <span class="hljs-number">5</span>;<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> log_external <span class="hljs-keyword">where</span> <span class="hljs-keyword">status</span>=<span class="hljs-string">'301'</span> <span class="hljs-keyword">limit</span> <span class="hljs-number">5</span>;</code></pre></li></ul><h3 id="header-21">示例7：外部分区分桶表</h3><ul><li>创建外部分区分桶表log_external_partition:<pre><code class="nullsql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">external</span> <span class="hljs-keyword">table</span> log_external_partition(  ip <span class="hljs-keyword">string</span>,  datetime <span class="hljs-keyword">string</span>,  <span class="hljs-keyword">url</span> <span class="hljs-keyword">string</span>,  <span class="hljs-keyword">size</span> <span class="hljs-keyword">string</span>)partitioned <span class="hljs-keyword">by</span> (<span class="hljs-keyword">status</span> <span class="hljs-keyword">string</span>,method <span class="hljs-keyword">string</span>)clustered <span class="hljs-keyword">by</span> (ip) <span class="hljs-keyword">into</span> <span class="hljs-number">5</span> buckets <span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">','</span>location <span class="hljs-string">'/input/external_partition'</span>;</code></pre></li><li><p>插入partition数据: （alter location）</p><pre><code class="nullsql">insert overwrite directory &#39;/input/log_301_GET&#39; row format delimited fields terminated by &#39;,&#39;select ip,datetime,url,size from log where status=&#39;301&#39; and method=&#39;GET&#39;; # generate file: /input/log_301_GET/000000_0~000004_0alter table log_external_partition add partition (status=&#39;301&#39;,method=&#39;GET&#39;) location &#39;/input/log_301_GET&#39;;</code></pre></li><li><p>插入partition数据:（动态分区）</p><pre><code class="nullsql">set hive.exec.dynamic.partition=true;set hive.exec.dynamic.partition.mode=nostrict;set hive.enforce.bucketing = true;# generate file: # /input/log_301_GET/000000_0_copy_1# /input/external_partition/status=xxx/method=yyy/000000_0~000004_0insert into table log_external_partition partition(status,method)select ip,datetime,url,size,status,method from log;</code></pre></li><li><p>HDFS目录：</p><pre><code>/input/log_301_GET/input/log_301_GET/000000_0/input/log_301_GET/000000_0_copy_1/input/log_301_GET/000001_0/input/log_301_GET/000002_0/input/log_301_GET/000003_0/input/log_301_GET/000004_0/input/external_partition/input/external_partition/status=200/input/external_partition/status=200/method=GET/input/external_partition/status=200/method=GET/000000_0/input/external_partition/status=200/method=GET/000001_0/input/external_partition/status=200/method=GET/000002_0/input/external_partition/status=200/method=GET/000003_0/input/external_partition/status=200/method=GET/000004_0/input/external_partition/status=301/input/external_partition/status=301/method=POST/input/external_partition/status=301/method=POST/000000_0/input/external_partition/status=301/method=POST/000001_0/input/external_partition/status=301/method=POST/000002_0/input/external_partition/status=301/method=POST/000003_0/input/external_partition/status=301/method=POST/000004_0...</code></pre></li></ul><h3 id="header-22">示例8：全排序</h3><ul><li><p>使用order by （不管设置几个reducer，最终只使用一个reducer）</p><pre><code class="nullsql">set mapred.reduce.tasks=5;# 全排序insert overwrite directory &#39;/input/log_order&#39; select ip,size from log order by ip;# 局部排序insert overwrite directory &#39;/input/log_distribute&#39; select ip,size from log distribute by ip sort by size;# 全排序（数据量很大时，效率会高些）insert overwrite directory &#39;/input/log_order_opt&#39;select * from (select ip,size from log distribute by ip sort by size) s order by ip;</code></pre></li><li>使用TotalOrderSort<ul><li>生成抽样文件</li><li>使用TotalOrderSort</li></ul></li></ul><h3 id="header-23">示例9：Join</h3><ul><li><p>创建两张表并插入数据:（create table as select）</p><pre><code class="nullsql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> a_log <span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">','</span> <span class="hljs-keyword">as</span> <span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> <span class="hljs-keyword">log</span> <span class="hljs-keyword">where</span> <span class="hljs-keyword">status</span>=<span class="hljs-string">'200'</span>;<span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> b_log<span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">','</span> <span class="hljs-keyword">as</span> <span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> <span class="hljs-keyword">log</span> <span class="hljs-keyword">where</span> <span class="hljs-keyword">status</span>=<span class="hljs-string">'301'</span>;</code></pre></li><li>查询: （会自动判断使用mapjoin）<pre><code class="nullsql"><span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> a_log a <span class="hljs-keyword">join</span> b_log b <span class="hljs-keyword">on</span> a.ip=b.ip <span class="hljs-keyword">where</span> a.method=<span class="hljs-string">'POST'</span> <span class="hljs-keyword">limit</span> <span class="hljs-number">5</span>;<span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> a_log a <span class="hljs-keyword">join</span> b_log b <span class="hljs-keyword">on</span> a.ip=b.ip <span class="hljs-keyword">and</span> a.method=<span class="hljs-string">'POST'</span> <span class="hljs-keyword">limit</span> <span class="hljs-number">5</span>;</code></pre></li><li>HDFS目录:<pre><code>/user/hive/warehouse/a_log/user/hive/warehouse/a_log/000000_0/user/hive/warehouse/b_log/user/hive/warehouse/b_log/000000_0</code></pre></li><li><p>创建两张分区表并插入数据:</p><pre><code class="nullsql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> a_log_smb <span class="hljs-keyword">like</span> a_log;<span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> b_log_smb <span class="hljs-keyword">like</span> b_log;<span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> a_log_smb clustered <span class="hljs-keyword">by</span> (ip) <span class="hljs-keyword">into</span> <span class="hljs-number">3</span> buckets;<span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> b_log_smb clustered <span class="hljs-keyword">by</span> (ip) <span class="hljs-keyword">into</span> <span class="hljs-number">5</span> buckets;<span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> a_log_smb <span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> a_log;<span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> b_log_smb <span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> b_log;</code></pre></li><li>查询:(SMP)<pre><code>select * from a_log_smb a join b_log_smb b on a.ip=b.ip limit 5;</code></pre></li><li>HDFS目录：<pre><code>/user/hive/warehouse/a_log_smb/user/hive/warehouse/a_log_smb/000000_0/user/hive/warehouse/a_log_smb/000001_0/user/hive/warehouse/a_log_smb/000002_0/user/hive/warehouse/b_log_smb/user/hive/warehouse/b_log_smb/000000_0/user/hive/warehouse/b_log_smb/000001_0/user/hive/warehouse/b_log_smb/000002_0/user/hive/warehouse/b_log_smb/000003_0/user/hive/warehouse/b_log_smb/000004_0</code></pre></li></ul><h2 id="header-24">优化策略</h2><p>数据倾斜：由于数据的不均衡原因，导致数据分布不均匀，造成数据大量的集中到一点，造成数据热点</p><p>Hadoop 计算框架特性：</p><ul><li>不怕数据大，怕数据倾斜</li><li>job过多，耗时长（job初始化时间长）</li></ul><p>常用优化手段：</p><ul><li>减少Job数</li><li>并行Job，例如设置：<pre><code class="nullsql"># 对于同一个SQL产生的JOB,如果不存在依赖的情况下，将会并行启动JOBset hive.exec.parallel=true;set hive.exec.parallel.thread.number=16;</code></pre></li><li><p>合理设置Mapper和Reducer数</p><ul><li>减少mapper数：合并小文件<pre><code class="nullsql"># 100~128M的按照100M分割，&lt;100M合并set mapred.max.split.size=100000000;set mapred.min.split.size.per.node=100000000;set mapred.min.split.size.per.rack=100000000;set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</code></pre></li><li>增加mapper数：拆分文件（分区，分桶）</li><li><p>reducer数</p><pre><code class="nullsql"># 每个reduce任务处理的数据量，默认为1000^3=1Ghive.exec.reducers.bytes.per.reducer# 每个任务最大的reduce数，默认为999hive.exec.reducers.max# 设置reducer数量mapred.reduce.tasks</code></pre></li></ul></li><li><p>合理压缩，减少网络传输和I/O压力，例如设置：</p><pre><code class="nullsql"><span class="hljs-keyword">set</span> mapred.output.compress = <span class="hljs-literal">true</span>;  <span class="hljs-keyword">set</span> mapred.output.compression.codec = org.apache.hadoop.io.compress.GzipCodec;  <span class="hljs-keyword">set</span> mapred.output.compression.type = <span class="hljs-keyword">BLOCK</span>;  <span class="hljs-keyword">set</span> mapred.compress.map.output = <span class="hljs-literal">true</span>;  <span class="hljs-keyword">set</span> mapred.map.output.compression.codec = org.apache.hadoop.io.compress.LzoCodec;  <span class="hljs-keyword">set</span> hive.exec.compress.output = <span class="hljs-literal">true</span>;  <span class="hljs-keyword">set</span> hive.exec.compress.intermediate = <span class="hljs-literal">true</span>;  <span class="hljs-keyword">set</span> hive.intermediate.compression.codec = org.apache.hadoop.io.compress.LzoCodec;</code></pre></li><li>以SequenceFile保存，节约序列化和反序列化时间</li><li>少用count distinct，例如：<pre><code class="nullsql">select status,count(distinct ip) from log group by status;=&gt;select status,count(ip) from (select status,ip from log group by status,ip) agroup by status;</code></pre></li><li>join优化<ul><li>尽量将condition放入join on中</li><li>尽量大表滞后或使用STREAMTABLE(table)标识大表</li><li>使用SMB Map Join (SMB: Sort Merge Bucket) </li></ul></li><li>合理分区，分桶</li><li>小数据量，尽量使用本地MapReduce，例如设置：<pre><code class="nullsql"><span class="hljs-keyword">set</span> hive.exec.mode.local.auto=<span class="hljs-literal">true</span>;  <span class="hljs-keyword">set</span> hive.exec.mode.local.auto.inputbytes.max=<span class="hljs-number">50000000</span>;<span class="hljs-keyword">set</span> hive.exec.mode.local.auto.tasks.max=<span class="hljs-number">10</span>;</code></pre></li></ul><h2 id="header-25">自定义函数</h2><p>查看函数</p><pre><code class="nullsql"><span class="hljs-keyword">SHOW</span> FUNCTIONS; <span class="hljs-keyword">DESCRIBE</span> <span class="hljs-keyword">FUNCTION</span> &lt;function_name&gt;</code></pre><p>自定义函数包括三种UDF、UDAF、UDTF，可直接应用于Select语句</p><ul><li>UDF：User-Defined-Function<ul><li>用户自定义函数（只能实现一进一出的操作）</li><li><code>extends UDF</code></li></ul></li><li>UDAF：User-Defined Aggregation Funcation<ul><li>用户自定义聚合函数（可实现多进一出的操作）</li><li><code>extends UDAF</code>+ 内部Evaluator<code>implements UDAFEvaluator</code><ul><li>init 初始化</li><li>iterate 遍历</li><li>terminatePartial 类似Hadoop的Combiner</li><li>merge 合并</li><li>terminate 返回最终的聚集函数结果</li></ul></li></ul></li><li>UDTF：User-Defined Table-Generating Function<ul><li>用户自定义表函数（可实现一进多出的操作）</li><li><code>extends GenericUDTF</code></li></ul></li></ul><p>UDF示例：</p><ol><li>自定义函数<pre><code class="nulljava"><span class="hljs-keyword">import</span> org.apache.Hadoop.hive.ql.exec.UDF   <span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Helloword</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">UDF</span></span>{   <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">evaluate</span><span class="hljs-params">()</span></span>{       <span class="hljs-keyword">return</span> <span class="hljs-string">"hello world!"</span>;   }   <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">evaluate</span><span class="hljs-params">(String str)</span></span>{       <span class="hljs-keyword">return</span> <span class="hljs-string">"hello world: "</span> + str;   }   }</code></pre></li><li>上传jar包到目标机器</li><li>添加到Hive中<pre><code class="nullvim"># 进入hive客户端，添加jar包hive&gt; <span class="hljs-built_in">add</span> jar udf_helloword.jar</code></pre></li><li>创建临时函数<pre><code class="nullvim">hive&gt; create temporary <span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">helloword</span> <span class="hljs-title">as</span> '<span class="hljs-title">com</span>.<span class="hljs-title">cj</span>.<span class="hljs-title">hive</span>.<span class="hljs-title">udf</span>.<span class="hljs-title">Helloword</span>'</span></code></pre></li><li>测试<pre><code class="nullsql"><span class="hljs-keyword">select</span> helloword(<span class="hljs-keyword">name</span>) <span class="hljs-keyword">from</span> <span class="hljs-keyword">users</span>;</code></pre></li><li>删除临时函数<pre><code class="nullvim">hive&gt; <span class="hljs-keyword">drop</span> temporaty <span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">helloword</span>;</span></code></pre></li></ol><p>注：helloworld为临时的函数，所以每次进入hive都需要add jar以及create temporary操作</p><h2 id="header-26">Java API</h2><ul><li>启动Hive远程服务<pre><code class="nullvim">&gt; hive --service hiveserver2 &gt;/dev/null  2&gt;/dev/null &amp;</code></pre></li><li>Java客户端加入依赖包<pre><code class="nullxml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.hive<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>hive-jdbc<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>${hive.version}<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></code></pre></li><li><p>JAVA客户端连接操作代码</p><pre><code class="nulljava"><span class="hljs-meta">@Test</span><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">testConnection</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> ClassNotFoundException, SQLException </span>{  Class.forName(<span class="hljs-string">"org.apache.hive.jdbc.HiveDriver"</span>);  Connection conn = DriverManager.getConnection(<span class="hljs-string">"jdbc:hive2://cj.storm:10000/default"</span>, <span class="hljs-string">""</span>, <span class="hljs-string">""</span>);  Statement stmt = conn.createStatement();  String querySQL = <span class="hljs-string">"select * from log_partition where status='200' and method='GET' limit 10"</span>;  ResultSet res = stmt.executeQuery(querySQL);  <span class="hljs-keyword">while</span> (res.next()) {    System.out.println(res.getString(<span class="hljs-number">1</span>) + <span class="hljs-string">"\t"</span> + res.getString(<span class="hljs-number">2</span>));  }  res.close();  conn.close();}</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      Hive introduction
    
    </summary>
    
    
      <category term="BigData" scheme="http://sixdegree.github.io/tags/BigData/"/>
    
  </entry>
  
  <entry>
    <title>HBase</title>
    <link href="http://sixdegree.github.io/2016/05/05/HBase.html"/>
    <id>http://sixdegree.github.io/2016/05/05/HBase.html</id>
    <published>2016-05-04T16:00:00.000Z</published>
    <updated>2016-05-24T01:45:30.000Z</updated>
    
    <content type="html"><![CDATA[<ol><li>HBase概述，模型；</li><li>HBase安装，操作；</li></ol><a id="more"></a><h2 id="header-1">概述</h2><p>HBase：Hadoop Database</p><ul><li>一种在Hadoop之上的NoSQL 的Key/vale数据库,适合实时查询</li><li>利用Hadoop HDFS作为其文件存储系统</li><li>利用Hadoop MapReduce来处理其海量数据</li><li>利用Zookeeper作为协调工具</li><li>是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统</li><li>适合海量数据(如20PB)的秒级简单查询的数据库<ul><li>适合key-value查询</li><li>适合按时间排序top n的场景</li><li>适合大量读写</li></ul></li></ul><h3 id="header-2">数据模型</h3><ul><li><p>数据模型<br>  <img src="/2016/05/05/datamodal.png" alt="Data Modal"><br>  <img src="/2016/05/05/keyvalue.png" alt="Key Value"></p><ul><li>Table:存储管理数据</li><li><code>RowKey</code>:行键（类似于关系型数据库中的主键）</li><li><code>ColumnFamily</code>：列族（定义表时指定），可包含任意多个列（插入记录时动态增加）</li><li><code>Cell</code>：单元格，由<code>{rowKey,columnFamily:columnName}</code>确定的存储单元<ul><li>可存储一份数据的多个版本，由<code>Timestamp（时间戳）</code>属性区分，即数据具有版本特性</li><li>由<code>{rowKey, columnFamily:columnName, version}</code>可确定某一版的Data</li><li>若不指定时间戳或者版本，默认取最新的数据</li></ul></li></ul></li><li><p>逻辑数据模型<br>  <img src="/2016/05/05/logicalmodal.png" alt="Logical Modal"></p></li><li><p>物理数据模型<br>  <img src="/2016/05/05/physicalmodal.png" alt="Physical Modal"></p></li><li><p>说明：</p><ul><li>存储划分：<ul><li><code>Table</code>按<code>RowKey</code>范围<code>[startKey,endKey)</code>划分成N个<code>Region</code><ul><li>各个<code>Region</code>分散存储在不同的<code>RegionServer</code>（单独的物理机器）中</li><li>这样对表的操作转化为对多台<code>RegionServer</code>的并行操作</li></ul></li><li><code>Region</code>按<code>ColumnFamily</code>一对一划分成<code>Store</code>,每个store包括:<ul><li><code>MemStore</code> 内存存储 （先，达到阀值后，写入StoreFile）<ul><li><code>StoreFile</code> 文件存储（对应一个<code>HFile</code>，存放在Hadoop HDFS）</li></ul></li></ul></li></ul></li><li>存储结构：<ul><li><code>RowKey</code>,<code>ColumnName</code>按字典顺序物理存储</li><li><code>Timestamp</code>是一个64位整数</li><li>所有数据以<code>byte[]</code>存储</li></ul></li></ul></li></ul><p><img src="/2016/05/05/region.png" alt="Region"></p><h3 id="header-3">架构体系</h3><p>主从式结构</p><ul><li>Master（主）：可以启动多个，由Zookeeper的Master Election机制保证总有一个Master运行<ul><li>管理RegionServer</li><li>分配Region</li></ul></li><li>RegionServer（从）：一个物理节点一个<ul><li>存储Region</li><li>响应用户I/O请求，向HDFS文件系统中读写数据</li><li>合并切分Region（StoreFile）</li></ul></li><li>Zookeeper（协调）<ul><li>保证集群中只有一个Running Master<ul><li>监控RegionServer的状态，实时通知给Master</li><li>存储HBase的Schema（包括有哪些Table，每个Table有哪些ColumnFamily）</li><li>存储Region寻址入口（即<code>-ROOT-</code>表的location）</li></ul></li></ul></li></ul><p><img src="/2016/05/05/frame1.png" alt="Frame"></p><p><img src="/2016/05/05/frame2.png" alt="Frame"></p><p>HRegion 进程：</p><ul><li>HLog ：预写式日志（所有更新操作先记录进日志，再操作），用于做灾难恢复Failover</li><li>Store ：每个Store存放一个列族<ul><li>MemStore 内存存储 （先，达到阀值后，写入StoreFile）</li><li>StoreFile 文件存储（对应一个HFile，存放在Hadoop HDFS）<ul><li>每次写入就形成一份单独的<ul><li>数量增长到一定阀值：合并StoreFile（合并时会进行版本合并和删除工作）– HDFS适合存储大文件</li><li>大小超过一定阀值：分割当前Region（再由HMaster分配到其他RegionServer）– 实现负载均衡</li></ul></li></ul></li></ul></li></ul><p>注意：</p><ul><li>HBase中有两张特殊的Table（<code>-ROOT-</code>和<code>.META.</code>）<ul><li><code>.META.</code>：记录了<code>用户表</code>的Region信息，<code>.META.</code>表本身可划分成N个regoin</li><li><code>-ROOT-</code>：记录了<code>.META.</code>表划分成的N个Region的信息，<code>-ROOT-</code>本身只有一个，不划分<br><img src="/2016/05/05/meta.png" alt="META"></li></ul></li><li>Client访问HBase上数据数据（并不需要master参与）:<code>zookeeper</code>=&gt;<code>-ROOT-</code>表=&gt;<code>.META.</code>表=&gt;Region位置=&gt;访问</li><li>Client包含访问HBase的接口,可通过维护着一些cache来加快对HBase的访问（比如缓存region的位置信息）</li></ul><h2 id="header-4">安装</h2><h3 id="header-5">伪分布式</h3><ol><li>下载解压安装包</li><li>设置环境变量（<code>/etc/profile</code>文件）<ul><li><code>HBASE_HOME</code></li><li><code>PATH</code></li></ul></li><li>配置（<code>$HBASE_HOME/conf</code>目录下）<ul><li><code>hbase-env.sh</code><pre><code class="nullvim">  export JAVA_HOME=/usr/local/jdk  # 使用HBase内置的Zookeeper  export HBASE_MANAGES_ZK=true</code></pre></li><li><code>hbase-site.xml</code><pre><code class="nullxml">  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.rootdir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://hadoop0:9000/hbase<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.cluster.distributed<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hadoop0<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>1<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></code></pre></li></ul></li><li>启动<ul><li>运行Hadoop</li><li>运行HBase: <code>start-hbase.sh</code></li></ul></li><li>验证<ul><li><code>jps</code> 三个进程：HMaster、HRegionServer、HQuorumPeer</li><li><code>http://hadoop0:60010</code></li></ul></li></ol><h3 id="header-6">集群式</h3><p>（在原来的hadoop0上的hbase伪分布基础上进行搭建）</p><p>例如：</p><ul><li>Master：hadoop0</li><li>RegionServer：hadoop1,hadoop2</li></ul><ol><li>配置（<code>$HBASE_HOME/conf</code>目录下）<ul><li><code>hbase-env.sh</code><pre><code class="nullvim">  export JAVA_HOME=/usr/local/jdk  # 不使用HBase内置的Zookeeper  export HBASE_MANAGES_ZK=false</code></pre></li><li><code>hbase-site.xml</code><pre><code class="nullxml">  ...  <span class="hljs-comment">&lt;!-- 配置Zookeeper监控管理的节点（hostname）--&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hadoop0,hadoop1,hadoop2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span></code></pre></li><li><code>regionservers</code> （配置RegionServer的hostname）<pre><code class="nullvim">  hadoop1  hadoop2</code></pre></li></ul></li><li>复制配置好的hbase到其他节点<pre><code class="nullvim"> scp -rp ./hbase `hadoop`@hadoop1:~ scp -rp ./hbase `hadoop`@hadoop2:~</code></pre></li><li>启动<ul><li>运行Hadoop</li><li>运行zookeeper</li><li>运行HBase: <code>start-hbase.sh</code></li></ul></li></ol><h2 id="header-7">操作</h2><p>注意：HBase不能支持where条件、Order by 查询，只支持按照Row key来查询，但是可以通过HBase提供的API进行条件过滤</p><h3 id="header-8">Shell Cmd</h3><ul><li>启用HBase Shell命令行<pre><code class="nullshell"><span class="hljs-meta">  &gt;</span><span class="bash"> hbase shell</span>  ...<span class="hljs-meta">  &gt;</span><span class="bash"> quite</span></code></pre></li><li>常用命令<table class="table"><thead><tr><th>名称</th><th>命令表达式</th></tr></thead><tbody><tr><td>创建表</td><td>create ‘表名称’, ‘列族名称1’,’列族名称2’,’列族名称N’</td></tr><tr><td>添加记录</td><td>put ‘表名称’, ‘行键’, ‘列族名称:列名称’, ‘值’</td></tr><tr><td>查看记录</td><td>get ‘表名称’, ‘行键’</td></tr><tr><td>查看表中的记录总数</td><td>count ‘表名称’</td></tr><tr><td>删除记录</td><td>delete ‘表名’ ,’行键’ , ‘列名称’</td></tr><tr><td>删除一张表</td><td>先要屏蔽该表，才能对该表进行删除，第一步 disable ‘表名称’ 第二步 drop ‘表名称’</td></tr><tr><td>查看所有记录</td><td>scan “表名称”</td></tr><tr><td>查看某个表某个列中所有数据</td><td>scan “表名称” , {COLUMNS=&gt;’列族名称:列名称’}</td></tr><tr><td>更新记录</td><td>就是重写一遍进行覆盖</td></tr></tbody></table></li><li>注意：<ul><li>HBase其实没有delete操作，只有insert，以Timestamp区分新旧记录</li><li>为充分利用分布式，可使用reverse key，hash，复合行键等技巧改造行键</li><li>行键打乱后，可更均匀随机的分配到各节点，而不是集中在一个节点</li></ul></li></ul><p>操作示例：</p><ul><li><p>表操作：</p><ul><li>创建表<pre><code class="nullshell"><span class="hljs-meta">  &gt;</span><span class="bash"> create <span class="hljs-string">'users'</span>,<span class="hljs-string">'user_id'</span>,<span class="hljs-string">'address'</span>,<span class="hljs-string">'info'</span></span></code></pre></li><li>查看表<pre><code class="nullshell"><span class="hljs-meta">  &gt;</span><span class="bash"> list</span><span class="hljs-meta">  &gt;</span><span class="bash"> describe <span class="hljs-string">'users'</span></span></code></pre></li><li>验证表<pre><code class="nullshell"><span class="hljs-meta">  &gt;</span><span class="bash"> exists <span class="hljs-string">'users'</span></span><span class="hljs-meta">  &gt;</span><span class="bash"> is_enabled <span class="hljs-string">'users'</span></span><span class="hljs-meta">  &gt;</span><span class="bash"> is_disabled <span class="hljs-string">'users'</span></span></code></pre></li><li>删除表<pre><code class="nullshell"><span class="hljs-meta">  &gt;</span><span class="bash"> <span class="hljs-built_in">disable</span> <span class="hljs-string">'users'</span></span><span class="hljs-meta">  &gt;</span><span class="bash"> delete <span class="hljs-string">'users'</span></span></code></pre></li></ul></li><li><p>记录操作：</p><ul><li>插入<pre><code class="nullshell"><span class="hljs-meta">  &gt;</span><span class="bash"> put <span class="hljs-string">'users'</span>,<span class="hljs-string">'xiaoming'</span>,<span class="hljs-string">'info:age'</span>,<span class="hljs-string">'24'</span>;</span><span class="hljs-meta">  &gt;</span><span class="bash"> put <span class="hljs-string">'users'</span>,<span class="hljs-string">'xiaoming'</span>,<span class="hljs-string">'info:birthday'</span>,<span class="hljs-string">'1987-06-17'</span>;</span><span class="hljs-meta">  &gt;</span><span class="bash"> put <span class="hljs-string">'users'</span>,<span class="hljs-string">'xiaoming'</span>,<span class="hljs-string">'info:company'</span>,<span class="hljs-string">'alibaba'</span>;</span><span class="hljs-meta">  &gt;</span><span class="bash"> put <span class="hljs-string">'users'</span>,<span class="hljs-string">'xiaoming'</span>,<span class="hljs-string">'address:contry'</span>,<span class="hljs-string">'china'</span>;</span><span class="hljs-meta">  &gt;</span><span class="bash"> put <span class="hljs-string">'users'</span>,<span class="hljs-string">'xiaoming'</span>,<span class="hljs-string">'address:province'</span>,<span class="hljs-string">'zhejiang'</span>;</span><span class="hljs-meta">  &gt;</span><span class="bash"> put <span class="hljs-string">'users'</span>,<span class="hljs-string">'xiaoming'</span>,<span class="hljs-string">'address:city'</span>,<span class="hljs-string">'hangzhou'</span>;</span><span class="hljs-meta">  &gt;</span><span class="bash"> put <span class="hljs-string">'users'</span>,<span class="hljs-string">'zhangyifei'</span>,<span class="hljs-string">'info:birthday'</span>,<span class="hljs-string">'1987-4-17'</span>;</span><span class="hljs-meta">  &gt;</span><span class="bash"> put <span class="hljs-string">'users'</span>,<span class="hljs-string">'zhangyifei'</span>,<span class="hljs-string">'info:favorite'</span>,<span class="hljs-string">'movie'</span>;</span><span class="hljs-meta">  &gt;</span><span class="bash"> put <span class="hljs-string">'users'</span>,<span class="hljs-string">'zhangyifei'</span>,<span class="hljs-string">'info:company'</span>,<span class="hljs-string">'alibaba'</span>;</span><span class="hljs-meta">  &gt;</span><span class="bash"> put <span class="hljs-string">'users'</span>,<span class="hljs-string">'zhangyifei'</span>,<span class="hljs-string">'address:contry'</span>,<span class="hljs-string">'china'</span>;</span><span class="hljs-meta">  &gt;</span><span class="bash"> put <span class="hljs-string">'users'</span>,<span class="hljs-string">'zhangyifei'</span>,<span class="hljs-string">'address:province'</span>,<span class="hljs-string">'guangdong'</span>;</span><span class="hljs-meta">  &gt;</span><span class="bash"> put <span class="hljs-string">'users'</span>,<span class="hljs-string">'zhangyifei'</span>,<span class="hljs-string">'address:city'</span>,<span class="hljs-string">'jieyang'</span>;</span><span class="hljs-meta">  &gt;</span><span class="bash"> put <span class="hljs-string">'users'</span>,<span class="hljs-string">'zhangyifei'</span>,<span class="hljs-string">'address:town'</span>,<span class="hljs-string">'xianqiao'</span>;</span></code></pre></li><li><p>查询</p><pre><code class="nullshell"><span class="hljs-meta">  &gt;</span><span class="bash"> get <span class="hljs-string">'users'</span>,<span class="hljs-string">'xiaoming'</span></span><span class="hljs-meta">  &gt;</span><span class="bash"> get <span class="hljs-string">'users'</span>,<span class="hljs-string">'xiaoming'</span>,<span class="hljs-string">'info'</span></span><span class="hljs-meta">  &gt;</span><span class="bash"> get <span class="hljs-string">'users'</span>,<span class="hljs-string">'xiaoming'</span>,<span class="hljs-string">'info:age'</span></span><span class="hljs-meta">  #</span><span class="bash"> 获取单元格数据的版本数据</span><span class="hljs-meta">  &gt;</span><span class="bash"> get <span class="hljs-string">'users'</span>,<span class="hljs-string">'xiaoming'</span>,{COLUMN=&gt;<span class="hljs-string">'info:age'</span>,VERSIONS=&gt;1}</span><span class="hljs-meta">  &gt;</span><span class="bash"> get <span class="hljs-string">'users'</span>,<span class="hljs-string">'xiaoming'</span>,{COLUMN=&gt;<span class="hljs-string">'info:age'</span>,VERSIONS=&gt;2}</span><span class="hljs-meta">  &gt;</span><span class="bash"> get <span class="hljs-string">'users'</span>,<span class="hljs-string">'xiaoming'</span>,{COLUMN=&gt;<span class="hljs-string">'info:age'</span>,VERSIONS=&gt;3}</span><span class="hljs-meta">  #</span><span class="bash"> 获取单元格数据的某个版本数据</span><span class="hljs-meta">  &gt;</span><span class="bash"> get <span class="hljs-string">'users'</span>,<span class="hljs-string">'xiaoming'</span>,{COLUMN=&gt;<span class="hljs-string">'info:age'</span>,TIMESTAMP=&gt;1364874937056}</span><span class="hljs-meta">  #</span><span class="bash"> 全表扫描</span><span class="hljs-meta">  &gt;</span><span class="bash"> scan <span class="hljs-string">'users'</span></span><span class="hljs-meta">  #</span><span class="bash"> 统计表的行数</span><span class="hljs-meta">  &gt;</span><span class="bash"> count <span class="hljs-string">'users'</span></span></code></pre></li><li>更新<pre><code class="nullshell"><span class="hljs-meta">  &gt;</span><span class="bash"> put <span class="hljs-string">'users'</span>,<span class="hljs-string">'xiaoming'</span>,<span class="hljs-string">'info:age'</span> ,<span class="hljs-string">'29'</span></span><span class="hljs-meta">  &gt;</span><span class="bash"> get <span class="hljs-string">'users'</span>,<span class="hljs-string">'xiaoming'</span>,<span class="hljs-string">'info:age'</span></span></code></pre></li><li><p>删除</p><pre><code class="nullshell"><span class="hljs-meta">  #</span><span class="bash"> 删除某列值</span><span class="hljs-meta">  &gt;</span><span class="bash"> delete <span class="hljs-string">'users'</span>,<span class="hljs-string">'xiaoming'</span>,<span class="hljs-string">'info:age'</span></span><span class="hljs-meta">  #</span><span class="bash"> 删除整行</span><span class="hljs-meta">  &gt;</span><span class="bash">deleteall <span class="hljs-string">'users'</span>,<span class="hljs-string">'xiaoming'</span></span><span class="hljs-meta">  #</span><span class="bash"> 清空</span><span class="hljs-meta">  &gt;</span><span class="bash"> truncate <span class="hljs-string">'users'</span></span></code></pre></li></ul></li></ul><h3 id="header-9">Java API</h3><p>依赖包：</p><pre><code class="nullxml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.hbase<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>hbase-client<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>${hbase.version}<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></code></pre><p>使用示例：</p><pre><code class="nulljava">Configuration conf = HBaseConfiguration.create();conf.set(<span class="hljs-string">"hbase.rootdir"</span>, <span class="hljs-string">"hdfs://hadoop0:9000/hbase"</span>);<span class="hljs-comment">//使用eclipse时必须添加这个，否则无法定位</span>conf.set(<span class="hljs-string">"hbase.zookeeper.quorum"</span>, <span class="hljs-string">"hadoop0"</span>);HBaseAdmin admin = <span class="hljs-keyword">new</span> HBaseAdmin(conf);String tableName=<span class="hljs-string">"users"</span>;String columnFamily=<span class="hljs-string">"info"</span>;<span class="hljs-comment">// 1. 创建一张表</span><span class="hljs-keyword">if</span> (admin.tableExists(tableName)) {    System.out.println(<span class="hljs-string">"table exists!"</span>);}<span class="hljs-keyword">else</span>{    HTableDescriptor tableDesc = <span class="hljs-keyword">new</span> HTableDescriptor(tableName);    tableDesc.addFamily(<span class="hljs-keyword">new</span> HColumnDescriptor(columnFamily));    admin.createTable(tableDesc);    System.out.println(<span class="hljs-string">"create table success!"</span>);}<span class="hljs-comment">// 2. 添加一条记录</span>String rowKey=<span class="hljs-string">"xiaoming"</span>;String column=<span class="hljs-string">"age"</span>;String data=<span class="hljs-string">"24"</span>;HTable table = <span class="hljs-keyword">new</span> HTable(conf, tableName);Put p1 = <span class="hljs-keyword">new</span> Put(Bytes.toBytes(rowKey));p1.add(Bytes.toBytes(columnFamily), Bytes.toBytes(column),     Bytes.toBytes(data));table.put(p1);System.out.println(<span class="hljs-string">"put'"</span>+rowKey+<span class="hljs-string">"',"</span>+columnFamily+<span class="hljs-string">":"</span>+column+<span class="hljs-string">"','"</span>+data+<span class="hljs-string">"'"</span>);<span class="hljs-comment">// 3. 读取一条记录</span>Get get = <span class="hljs-keyword">new</span> Get(Bytes.toBytes(rowKey));Result result = table.get(get);System.out.println(<span class="hljs-string">"Get: "</span>+result);<span class="hljs-comment">// 4. 显示所有数据</span>Scan scan = <span class="hljs-keyword">new</span> Scan();ResultScanner scanner = table.getScanner(scan);<span class="hljs-comment">/*scan.setStartRow(Bytes.toBytes("134/"));scan.setStopRow( Bytes.toBytes("134:"));scan.setMaxVersions(1);*/</span><span class="hljs-keyword">for</span> (Result res : scanner) {    System.out.println(<span class="hljs-string">"Scan: "</span>+res    );}table.close();<span class="hljs-comment">// 5. 删除表</span><span class="hljs-keyword">if</span>(admin.tableExists(tableName)){    <span class="hljs-keyword">try</span> {      admin.disableTable(tableName);      admin.deleteTable(tableName);    } <span class="hljs-keyword">catch</span> (IOException e) {      e.printStackTrace();      System.out.println(<span class="hljs-string">"Delete "</span>+tableName+<span class="hljs-string">" 失败"</span>);    }}admin.close();System.out.println(<span class="hljs-string">"Delete "</span>+tableName+<span class="hljs-string">" 成功"</span>);</code></pre><h3 id="header-10">结合MapReduce操作</h3><p>Hbase对Mapreduce API进行了扩展，方便Mapreduce任务读写HTable数据</p><table class="table"><thead><tr><th>HBase MapReduce</th><th>Hadoop MapReduce</th></tr></thead><tbody><tr><td>org.apache.hadoop.hbase.mapreduce.TableMapper</td><td>org.apache.hadoop.mapreduce.Mapper</td></tr><tr><td>org.apache.hadoop.hbase.mapreduce.TableReducer</td><td>org.apache.hadoop.mapreduce.Reducer</td></tr><tr><td>org.apache.hadoop.hbase.mapreduce.TableInputFormat</td><td>org.apache.hadoop.mapreduce.InputFormat</td></tr><tr><td>org.apache.hadoop.hbase.mapreduce.TableOutputFormat</td><td>org.apache.hadoop.mapreduce.OutputFormat</td></tr></tbody></table><p>依赖包：</p><pre><code class="nullxml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.hbase<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>hbase-server<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>${hbase.version}<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></code></pre><p>使用示例1：统计</p><pre><code class="nulljava"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>{       Configuration config = HBaseConfiguration.create();     configuration.set(<span class="hljs-string">"hbase.zookeeper.quorum"</span>, <span class="hljs-string">"hadoop0"</span>);    Job job = <span class="hljs-keyword">new</span> Job(config,<span class="hljs-string">"HBaseTotal"</span>);      job.setJarByClass(TotalOnHBase.class);     Scan scan = <span class="hljs-keyword">new</span> Scan();      scan.setCaching(<span class="hljs-number">500</span>);    scan.setCacheBlocks(<span class="hljs-keyword">false</span>);    TableMapReduceUtil.initTableMapperJob(<span class="hljs-string">"access-log"</span>,scan,MyTableMapper.class,Text.class,IntWritable.class,job);      TableMapReduceUtil.initTableReducerJob(<span class="hljs-string">"total-access"</span>,MyTableReducer.class,job);      job.waitForCompletion(<span class="hljs-keyword">true</span>);}</code></pre><pre><code class="nulljava"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyTableMapper</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">TableMapper</span>&lt;<span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>&gt;  </span>{      <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> IntWritable ONE = <span class="hljs-keyword">new</span> IntWritable(<span class="hljs-number">1</span>);      <span class="hljs-keyword">private</span> Text text = <span class="hljs-keyword">new</span> Text();      <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">map</span><span class="hljs-params">(ImmutableBytesWritable row, Result value, Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException </span>{          String ip = Bytes.toString(row.get()).split(<span class="hljs-string">"-"</span>)[<span class="hljs-number">0</span>];          String url = <span class="hljs-keyword">new</span> String(value.getValue(Bytes.toBytes(<span class="hljs-string">"info"</span>), Bytes.toBytes(<span class="hljs-string">"url"</span>)));          text.set(ip+<span class="hljs-string">"&amp;"</span>+url);          context.write(text, ONE);      }  }</code></pre><pre><code class="nulljava"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyTableReducer</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">TableReducer</span>&lt;<span class="hljs-title">Text</span>, <span class="hljs-title">IntWritable</span>, <span class="hljs-title">ImmutableBytesWritable</span>&gt;  </span>{      <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">reduce</span><span class="hljs-params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException </span>{          <span class="hljs-keyword">int</span> sum = <span class="hljs-number">0</span>;          <span class="hljs-keyword">for</span> (IntWritable val : values) {              sum += val.get();          }          Put put = <span class="hljs-keyword">new</span> Put(key.getBytes());          put.add(Bytes.toBytes(<span class="hljs-string">"info"</span>), Bytes.toBytes(<span class="hljs-string">"count"</span>), Bytes.toBytes(String.valueOf(sum)));          context.write(<span class="hljs-keyword">null</span>, put);      }  }</code></pre><p>使用示例2：批量导入（<code>TableOutputFormat</code>,<code>TableReducer</code>）</p><pre><code class="nulljava"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>{    <span class="hljs-keyword">final</span> Configuration configuration = <span class="hljs-keyword">new</span> Configuration();    <span class="hljs-comment">//设置zookeeper</span>    configuration.set(<span class="hljs-string">"hbase.zookeeper.quorum"</span>, <span class="hljs-string">"hadoop0"</span>);    <span class="hljs-comment">//设置hbase表名称</span>    configuration.set(TableOutputFormat.OUTPUT_TABLE, <span class="hljs-string">"wlan_log"</span>);    <span class="hljs-comment">//将该值改大，防止hbase超时退出</span>    configuration.set(<span class="hljs-string">"dfs.socket.timeout"</span>, <span class="hljs-string">"180000"</span>);    <span class="hljs-keyword">final</span> Job job = <span class="hljs-keyword">new</span> Job(configuration, <span class="hljs-string">"HBaseBatchImport"</span>);    FileInputFormat.setInputPaths(job, <span class="hljs-string">"hdfs://hadoop0:9000/input"</span>);    job.setInputFormatClass(TextInputFormat.class);    job.setMapperClass(BatchImportMapper.class);    job.setMapOutputKeyClass(LongWritable.class);    job.setMapOutputValueClass(Text.class);    <span class="hljs-comment">//BatchImportReducer extends TableReducer</span>    job.setReducerClass(BatchImportReducer.class);    <span class="hljs-comment">//不再设置输出路径，而是设置输出格式类型</span>    job.setOutputFormatClass(TableOutputFormat.class);    job.waitForCompletion(<span class="hljs-keyword">true</span>);}</code></pre><pre><code class="nulljava"><span class="hljs-comment">// BatchImportMapper 数据清理，省略...</span><span class="hljs-comment">// BatchImportReducer 将数据写入HBase</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BatchImportReducer</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">TableReducer</span>&lt;<span class="hljs-title">LongWritable</span>, <span class="hljs-title">Text</span>, <span class="hljs-title">NullWritable</span>&gt;</span>{    <span class="hljs-function"><span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title">reduce</span><span class="hljs-params">(LongWritable key, Iterable&lt;Text&gt; values,Context context)</span> <span class="hljs-keyword">throws</span> IOException ,InterruptedException </span>{        <span class="hljs-keyword">for</span> (Text text : values) {            <span class="hljs-keyword">final</span> String[] splited = text.toString().split(<span class="hljs-string">"\t"</span>);            <span class="hljs-keyword">final</span> Put put = <span class="hljs-keyword">new</span> Put(Bytes.toBytes(splited[<span class="hljs-number">0</span>]));            put.add(Bytes.toBytes(<span class="hljs-string">"info"</span>), Bytes.toBytes(<span class="hljs-string">"age"</span>), Bytes.toBytes(splited[<span class="hljs-number">1</span>]));            <span class="hljs-comment">//省略其他字段，调用put.add(....)即可</span>            context.write(NullWritable.get(), put);        }    };}</code></pre>]]></content>
    
    <summary type="html">
    
      HBase introduction
    
    </summary>
    
    
      <category term="BigData" scheme="http://sixdegree.github.io/tags/BigData/"/>
    
      <category term="NoSql" scheme="http://sixdegree.github.io/tags/NoSql/"/>
    
  </entry>
  
</feed>
