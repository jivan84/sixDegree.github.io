<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SixDegree</title>
  
  <subtitle>host by chenjin</subtitle>
  <link href="//atom.xml" rel="self"/>
  
  <link href="http://sixdegree.github.io/"/>
  <updated>2019-07-08T12:47:36.000Z</updated>
  <id>http://sixdegree.github.io/</id>
  
  <author>
    <name>Chen Jin</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python Selenium</title>
    <link href="http://sixdegree.github.io/2019/03/25/Python-Selenium.html"/>
    <id>http://sixdegree.github.io/2019/03/25/Python-Selenium.html</id>
    <published>2019-03-24T16:00:00.000Z</published>
    <updated>2019-07-08T12:47:36.000Z</updated>
    
    <content type="html"><![CDATA[<ol><li>Install: selenium,browser drive</li><li>browser</li><li>查找元素：find_elements/find_elements_by_xxx,find_element/find_element_by_xxx</li><li>交互操作：action(eg: click,key_down,…),action_chains (ActionChains,drag_and_drop)</li><li>执行javascript: execute_script(…)</li><li>切换：switch_to.xxx，back/forward()</li><li>异常处理：selenium.common.exceptions（eg: TimeoutException, NoSuchElementException）</li><li>Cookie: add/get/delete_cookie(…),get_cookies(),delete_all_cookies()</li><li>等待元素: 强制等待 time.sleep(seconds), 隐式等待 browser.implicitly_wait(seconds),显示等待 WebDriverWait,expected_conditions</li></ol><a id="more"></a><h2 id="header-1">Install</h2><ul><li><p>安装<code>selenium</code></p><pre><code class="lang-bash">  $ pip install selenium  # check:  $ python3  &gt;&gt;&gt; from selenium import webdriver  &gt;&gt;&gt; help(webdriver)</code></pre></li><li><p>安装browser驱动程序,eg: chrome的<code>chromedrive</code></p><ul><li><a href="http://chromedriver.chromium.org/" target="_blank" rel="noopener">download</a></li><li>copy to path,eg: mac <code>/usr/local/bin</code></li><li>check: <code>chromedriver -v</code></li></ul></li></ul><h2 id="header-2">browser</h2><pre><code class="lang-python">def get_browser(slience=False):    if not slience:        return webdriver.Chrome()                           # 会弹出一个 chrome 浏览器    else:        chrome_options=Options()        chrome_options.add_argument(&#39;--headless&#39;)         chrome_options.add_argument(&#39;--disable-gpu&#39;)        browser=webdriver.Chrome(options=chrome_options)    # 创建的chrome浏览器是不可见的        return browserdef test_browser(slience=False):    browser=get_browser(slience)    browser.get(&#39;http://www.baidu.com&#39;)    print(browser.page_source)    browser.close()if __name__==&#39;__main__&#39;:    test_browser()    # test_browser(slience=True)</code></pre><h2 id="header-3">查找元素</h2><ul><li><code>find_element_by_xxx(...)</code>,<code>find_element(By.xxx,xxx)</code>: 返回匹配的第一个元素（<code>WebElement</code>类型对象），找不到则抛出异常</li><li><code>find_elements_by_xxx(...)</code>,<code>find_elements(By.xxx,xxx)</code>: 返回所有匹配的元素列表，找不到则返回空列表</li><li>eg: <code>find_elements(By.CSS_SELECTOR,&#39;.service-bd li&#39;)</code> = <code>find_elements_by_css_selector(&quot;.service-bd li&quot;)</code></li><li><code>WebElement</code>类型对象：<ul><li><code>.text</code> 获取文本值（它与它的所有子孙节点的文字的组合，无则返回空字符串）</li><li><code>.id</code></li><li><code>.tag_name</code></li><li><code>.location</code></li><li><code>.size</code></li><li><code>.get_attribute(attrName)</code> 获取属性值（无则返回None）</li><li><code>find_element_by_xxx / find_elements_by_xxx(...)</code></li><li><code>find_element / find_elements(By.xxx,xxx)</code></li></ul></li><li>使用:<ul><li>XPath<ul><li><code>find_element_by_xpath / find_elements_by_xpath(xpath)</code></li><li>eg: <code>find_element_by_xpath(&quot;//div[@class=&#39;detail&#39;]/a&quot;)</code></li></ul></li><li>CSS Selector<ul><li><code>find_element_by_css_selector / find_elements_by_css_selector(css)</code></li><li>eg: <code>find_element_by_css_selector(&quot;div[class=&#39;detail&#39;] &gt; div span&quot;)</code></li></ul></li><li>Tag<ul><li><code>find_element_by_id(id)</code>: 一个或异常</li><li><code>find_element_by_tag_name / find_elements_by_tag_name(tagName)</code></li><li><code>find_element_by_class_name / find_elements_by_class_name(classValue)</code>: 使用元素的class值查找元素</li><li><code>find_element_by_name / find_elements_by_name(name)</code>: 通过<code>name</code>属性查找</li><li><code>find_element_by_link_text / find_elements_by_link_text(linkText)</code>: 文本值为linkText的超级链接元素<code>&lt;a&gt;</code></li><li><code>find_element_by_partial_link_text / find_elements_by_partial_link_text(linkText)</code>: 文本值包含linkText的超级链接元素<code>&lt;a&gt;</code></li><li>eg: <code>find_element_by_class_name(&quot;p1&quot;)</code> = <code>find_elements_by_xpath(&quot;//*[@class=&#39;p1&#39;]&quot;)</code> = <code>find_elements_by_css_selector(&quot;*[class=&#39;p1&#39;]&quot;)</code></li></ul></li></ul></li></ul><pre><code class="lang-python">def test_element():    browser=get_browser(slience=True)    browser.get(&#39;http://www.baidu.com&#39;)    print(&#39;--- input ---&#39;)    input= browser.find_element_by_id(&#39;kw&#39;)    print_element(input)    print(&#39;--- searchBtn ---&#39;)    #searchBtn = browser.find_element_by_id(&#39;su&#39;)    searchBtn=browser.find_element(By.ID,&#39;su&#39;)    print_element(searchBtn)def print_element(ele):    print(&quot;id:&quot;,ele.id)    print(&quot;tag_name:&quot;,ele.tag_name)    print(&quot;location:&quot;,ele.location)    print(&quot;size:&quot;,ele.size)    print(&quot;text:&quot;,ele.text)    print(&quot;class:&quot;,ele.get_attribute(&quot;class&quot;))    print(&quot;name:&quot;,ele.get_attribute(&quot;name&quot;))    print(&quot;type:&quot;,ele.get_attribute(&quot;type&quot;))    print(&quot;value:&quot;,ele.get_attribute(&quot;value&quot;))    print(&quot;id:&quot;,ele.get_attribute(&quot;id&quot;))</code></pre><h2 id="header-4">交互操作</h2><p><a href="https://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains" target="_blank" rel="noopener">Refer to Action Chains</a></p><ul><li>click,click_and_hold,double_click,context_click</li><li>drag_and_drop,drag_and_drop_by_offset</li><li>key_down,key_up</li><li>move_by_offset,move_to_element,move_to_element_with_offset</li><li>pause,perform,release,reset_actions</li><li>send_keys,send_keys_to_element</li></ul><p><strong> Sample1: action </strong></p><pre><code class="lang-python">browser=webdriver.Chrome()input= browser.find_element_by_id(&#39;kw&#39;)input.send_keys(&quot;MakBook&quot;)searchBtn = browser.find_element_by_id(&#39;su&#39;)searchBtn.click()time.sleep(2)input.clear()input.send_keys(&quot;ipad&quot;)</code></pre><p><strong> Sample2: action_chains </strong></p><pre><code class="lang-python">from selenium.webdriver import ActionChainsbrowser=webdriver.Chrome()browser.get(&quot;http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable&quot;)time.sleep(1)browser.switch_to.frame(&#39;iframeResult&#39;)source = browser.find_element_by_css_selector(&#39;#draggable&#39;)target = browser.find_element_by_css_selector(&#39;#droppable&#39;)actions = ActionChains(browser)actions.drag_and_drop(source, target)actions.perform()time.sleep(1)</code></pre><h2 id="header-5">执行Javascript</h2><p><code>execute_script(script)</code></p><pre><code class="lang-python">browser=webdriver.Chrome()browser.execute_script(&#39;window.scrollTo(0, document.body.scrollHeight)&#39;)browser.execute_script(&#39;alert(&quot;To Bottom&quot;)&#39;)</code></pre><h2 id="header-6">切换</h2><ul><li><code>switch_to.xxx</code><ul><li><code>window(windowName)</code></li><li><code>frame(frameName)</code></li><li><code>parent_frame()</code></li><li><code>active_element()</code></li><li><code>default_content()</code></li><li><code>alert()</code></li></ul></li><li><code>back()</code>,<code>forward()</code></li></ul><p><strong> Sample1: window tab切换 </strong></p><pre><code class="lang-python"># 1. window tab切换：# 执行js命令`window.open()`打开选项卡# 不同的选项卡是存在`browser.window_handles`列表中# eg: 通过`browser.window_handles[0]`可以操作第一个选项卡def test_window():    browser=get_browser()    browser.get(&#39;https://www.baidu.com&#39;)    browser.execute_script(&#39;window.open()&#39;)    print(browser.window_handles)    browser.switch_to.window(browser.window_handles[1])    browser.get(&#39;https://www.douban.com/&#39;)    time.sleep(1)    browser.switch_to.window(browser.window_handles[0])    browser.get(&#39;https://python.org&#39;)    time.sleep(1)    # 浏览器的前进和后退: back(),forward()    browser.back()    time.sleep(1)    browser.forward()    time.sleep(1)    browser.close()</code></pre><p><strong> Sample2: frame切换 </strong></p><pre><code class="lang-python">def test_frame():    browser=get_browser(slience=True)    browser.get(&#39;http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable&#39;)    browser.switch_to.frame(&#39;iframeResult&#39;)    source = browser.find_element_by_css_selector(&#39;div#draggable&#39;)    print(source)    print(source.text)    try:        logo = browser.find_element_by_class_name(&#39;logo&#39;)    except NoSuchElementException:        print(&#39;NO LOGO&#39;)    browser.switch_to.parent_frame()    logo = browser.find_element_by_class_name(&#39;logo&#39;)    print(logo)    print(logo.text)</code></pre><h2 id="header-7">异常处理</h2><p><code>selenium.common.exceptions</code></p><pre><code class="lang-python">from selenium import webdriverfrom selenium.common.exceptions import TimeoutException, NoSuchElementExceptionbrowser=webdriver.Chrome()try:    browser.get(&#39;http://www.baidu.com&#39;)    input= browser.find_element_by_id(&#39;kw&#39;)    input.send_keys(&quot;MakBook&quot;)    searchBtn = browser.find_element_by_id(&#39;su&#39;)    searchBtn.click()    print(&quot;clicked!&quot;)    span=browser.find_element_by_xpath(&quot;//div[@id=&#39;container&#39;]//div[@class=&#39;nums&#39;]/span[@class=&#39;nums_text&#39;]&quot;)    print(&quot;result:&quot;,span.text)except (TimeoutException,NoSuchElementException) as e:    print(&quot;Occur Exception:&quot;,e)except Exception as e:    print(&quot;Unknow Exception:&quot;,type(e),e)finally:    print(&quot;close!&quot;)    browser.close()</code></pre><h2 id="header-8">Cookie</h2><ul><li><code>get_cookie(name)</code></li><li><code>add_cookie(dict)</code>: required keys “name” and “value”</li><li><code>delete_cookie(name)</code></li><li><code>get_cookies()</code></li><li><code>delete_all_cookes()</code></li></ul><pre><code class="lang-python">def test_cookie():    browser=get_browser(slience=True)    browser.get(&#39;http://www.baidu.com&#39;)    cookies=browser.get_cookies()    print(cookies)    browser.add_cookie({&#39;name&#39;:&#39;user&#39;,&#39;value&#39;:&#39;Tom&#39;})    print(browser.get_cookie(&#39;user&#39;))</code></pre><h2 id="header-9">等待元素</h2><ul><li>强制等待 <code>time.sleep(seconds)</code></li><li><p>隐式等待 <code>browser.implicitly_wait(seconds)</code></p><pre><code class="lang-python">  browser=get_browser()  browser.get(&#39;http://www.baidu.com&#39;)  input= browser.find_element_by_id(&#39;kw&#39;)  input.send_keys(&quot;MakBook&quot;)  searchBtn = browser.find_element_by_id(&#39;su&#39;)  searchBtn.click()  browser.implicitly_wait(3)  span=browser.find_element_by_xpath(&quot;//div[@id=&#39;container&#39;]//div[@class=&#39;nums&#39;]/span[@class=&#39;nums_text&#39;]&quot;)  print(span.text)  browser.close()</code></pre></li><li><p>显示等待</p><pre><code class="lang-python">  from selenium.webdriver.support.wait import WebDriverWait  from selenium.webdriver.support import expected_conditions as EC  browser=get_browser()  browser.get(&#39;http://www.baidu.com&#39;)  input= browser.find_element_by_id(&#39;kw&#39;)  input.send_keys(&quot;MakBook&quot;)  searchBtn = browser.find_element_by_id(&#39;su&#39;)  searchBtn.click()  wait=WebDriverWait(browser,10, 0.5)  optionLocator = (By.XPATH, &quot;//select/option&quot;)  option=wait.until(EC.presence_of_element_located(optionLocator))  print(option)  btnLocator=(By.CSS_SELECTOR, &#39;.btn-search&#39;)  btn=wait.until(EC.element_to_be_clickable(btnLocator))  print(btn)  browser.close()</code></pre><ul><li>EC 常用的判断条件：<ul><li><code>title_is</code> : 标题是某内容</li><li><code>title_contains</code> : 标题包含某内容</li><li><code>visibility_of</code> : 可见，传入元素对象</li><li><code>staleness_of</code> : 判断一个元素是否仍在DOM，可判断页面是否已经刷新</li><li><code>alert_is_present</code> : 是否出现Alert</li><li><code>frame_to_be_available_and_switch_to_it</code> : frame加载并切换</li><li><code>element_selection_state_to_be</code> : 传入元素对象以及状态，相等返回True，否则返回False</li><li><code>element_located_selection_state_to_be</code> : 传入定位元组以及状态，相等返回True，否则返回False</li><li><code>presence_of_element_located(locator)</code> : 指定元素出现，传入定位元组，如(By.ID, ‘p’)</li><li><code>presence_of_all_elements_located(locator)</code></li><li><code>invisibility/visibility_of_element_located(locator)</code>: 指定元素不可见／可见</li><li><code>element_to_be_clickable(locator)</code> : 指定元素可点击</li><li><code>element_located_to_be_selected(locator)</code> : 指定元素可选择</li><li><code>element_to_be_selected(element)</code></li><li><code>text_to_be_present_in_element(locator,text)</code> : 指定元素的文本包含指定文本</li><li><code>text_to_be_present_in_element_value(locator,text)</code> : 指定元素值包含某文字</li></ul></li></ul></li></ul><h2 id="header-10">Reference</h2><ul><li><a href="https://github.com/sixDegree/python-basic-demo" target="_blank" rel="noopener">My Demo</a></li><li><a href="https://selenium-python.readthedocs.io/" target="_blank" rel="noopener">Selenium Doc</a></li><li><a href="https://www.cnblogs.com/zhaof/p/6953241.html" target="_blank" rel="noopener">python爬虫从入门到放弃（八）之 Selenium库的使用</a></li><li><a href="https://www.cnblogs.com/LOVEYU/p/8392269.html" target="_blank" rel="noopener">selenium+python自动化测试系列(一)：登录</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ol&gt;
&lt;li&gt;Install: selenium,browser drive&lt;/li&gt;
&lt;li&gt;browser&lt;/li&gt;
&lt;li&gt;查找元素：find_elements/find_elements_by_xxx,find_element/find_element_by_xxx&lt;/li&gt;
&lt;li&gt;交互操作：action(eg: click,key_down,…),action_chains (ActionChains,drag_and_drop)&lt;/li&gt;
&lt;li&gt;执行javascript: execute_script(…)&lt;/li&gt;
&lt;li&gt;切换：switch_to.xxx，back/forward()&lt;/li&gt;
&lt;li&gt;异常处理：selenium.common.exceptions（eg: TimeoutException, NoSuchElementException）&lt;/li&gt;
&lt;li&gt;Cookie: add/get/delete_cookie(…),get_cookies(),delete_all_cookies()&lt;/li&gt;
&lt;li&gt;等待元素: 强制等待 time.sleep(seconds), 隐式等待 browser.implicitly_wait(seconds),显示等待 WebDriverWait,expected_conditions&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://sixdegree.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Python 爬虫框架Scrapy</title>
    <link href="http://sixdegree.github.io/2019/03/20/Python-Scrapy.html"/>
    <id>http://sixdegree.github.io/2019/03/20/Python-Scrapy.html</id>
    <published>2019-03-19T16:00:00.000Z</published>
    <updated>2019-07-08T13:19:11.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Scrapy架构，常用命令，文档解析</li><li>Spider: Spider,CrawlSpider,XMLFeedSpider,CSVFeedSpider,SitemapSpider</li><li>Item,ItemLoader</li><li>Middleware: ItemPipeline,Spider/DownloaderMiddleware,Item Exporters</li><li>应用示例：基于Excel爬取，Login,常见问题</li><li>Scrapy-Redis 分布式架构（共享抓取队列）</li><li>Scrapyd（分布式发布）</li></ul><a id="more"></a><h2 id="header-1">Starter</h2><ul><li><p>一套基于<code>Twisted</code>事件驱动的异步爬虫框架</p></li><li><p>是为持续运行设计的专业爬虫框架,提供了操作的Scrapy命令行</p></li><li><p>VS. Requests</p><table class="table"><thead><tr><th style="text-align:left">Requests</th><th style="text-align:left">Scrapy</th></tr></thead><tbody><tr><td style="text-align:left">功能库,重点在于页面下载（页面级爬虫）</td><td style="text-align:left">框架,重点在于爬虫结构（网站级爬虫）</td></tr><tr><td style="text-align:left">阻塞IO</td><td style="text-align:left">基于<code>Twisted</code>事件驱动，异步</td></tr><tr><td style="text-align:left">并发性考虑不足</td><td style="text-align:left">并发性好</td></tr><tr><td style="text-align:left">定制灵活</td><td style="text-align:left">一般定制灵活,深度定制困难</td></tr><tr><td style="text-align:left">场景：小需求</td><td style="text-align:left">场景：大需求</td></tr></tbody></table></li><li><p>install: <code>pip install scrapy</code></p></li><li><p>check:<code>scrapy -h</code></p><pre><code class="lang-bash">  $ scrapy -h  Scrapy 1.6.0 - no active project  Usage:    scrapy &lt;command&gt; [options] [args]  Available commands:    bench         Run quick benchmark test    fetch         Fetch a URL using the Scrapy downloader    genspider     Generate new spider using pre-defined templates    runspider     Run a self-contained spider (without creating a project)    settings      Get settings values    shell         Interactive scraping console    startproject  Create new project    version       Print Scrapy version    view          Open URL in browser, as seen by Scrapy    [ more ]      More commands available when run from project directory  Use &quot;scrapy &lt;command&gt; -h&quot; to see more info about a command</code></pre></li></ul><h3 id="header-2">使用</h3><ol><li><p>创建工程: </p><ul><li><code>scrapy startproject &lt;name&gt; [dir]</code></li></ul></li><li><p>创建<code>Spider</code>: </p><ul><li>在工程中产生一个Scrapy爬虫: <code>scrapy genspider [options] &lt;spiderName&gt; &lt;domain&gt;</code></li><li>编写<code>vi &lt;spiderName&gt;.py</code><ul><li>start_urls: 初始URL地址</li><li>parse(response): 获取页面后的解析处理</li></ul></li></ul></li><li><p>编写<code>Item Pipeline</code>: </p><ul><li><code>pipelines.py</code>: 定义对爬取项<code>Scraped Item</code>的处理类 </li><li><code>setting.py</code>: 添加到<code>ITEM_PIPELINES</code>配置项</li></ul></li><li><p>配置优化</p><ul><li>配置<code>settings.py</code>文件</li><li>eg: 配置并发连接选项<ul><li><code>CONCURRENT_REQUESTS</code>: <code>Downloader</code>最大并发请求下载数量,默认32</li><li><code>CONCURRENT_ITEMS</code>: <code>Item Pipeline</code>最大并发ITEM处理数量，默认100</li><li><code>CONCURRENT_REQUESTS_PER_DOMAIN</code>: 每个目标域名最大的并发请求数量,默认8 </li><li><code>CONCURRENT_REQUESTS_PER_IP</code>: 每个目标IP最大的并发请求数量,默认0,非0有效</li></ul></li></ul></li><li><p>执行：<code>scrapy crawl &lt;spiderName&gt;</code></p></li></ol><h3 id="header-3">Demo</h3><ol><li><p>创建项目</p><pre><code class="lang-bash"> $ scrapy startproject douban_demo New Scrapy project &#39;douban_demo&#39;, using template directory &#39;/usr/local/lib/python3.7/site-packages/scrapy/templates/project&#39;, created in:     /Users/cj/space/python/douban_demo You can start your first spider with:     cd douban_demo     scrapy genspider example example.com</code></pre><ul><li><p>查看项目目录</p><pre><code class="lang-bash">  $ tree  .  ├── douban_demo  │   ├── __init__.py  │   ├── __pycache__  │   │   ├── __init__.cpython-37.pyc  │   │   └── settings.cpython-37.pyc  │   ├── items.py  │   ├── middlewares.py  │   ├── pipelines.py  │   ├── settings.py  │   └── spiders  │       ├── __init__.py  │       ├── __pycache__  │          └── __init__.cpython-37.pyc  └── scrapy.cfg  4 directories, 10 files</code></pre></li><li>查看自动创建的<code>pipelines.py</code><pre><code class="lang-python">  # -*- coding: utf-8 -*-  # Define your item pipelines here  #  # Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting  # See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html  class DoubanDemoPipeline(object):      def process_item(self, item, spider):          return item</code></pre></li><li>查看自动创建的<code>items.py</code><pre><code class="lang-python">  # -*- coding: utf-8 -*-  # Define here the models for your scraped items  #  # See documentation in:  # https://doc.scrapy.org/en/latest/topics/items.html  import scrapy  class DoubanDemoItem(scrapy.Item):      # define the fields for your item here like:      # name = scrapy.Field()      pass</code></pre></li><li><p>查看自动生成的<code>settings.py</code></p><pre><code class="lang-python">  # -*- coding: utf-8 -*-  # Scrapy settings for douban_demo project  #  # For simplicity, this file contains only settings considered important or  # commonly used. You can find more settings consulting the documentation:  #  #     https://doc.scrapy.org/en/latest/topics/settings.html  #     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html  #     https://doc.scrapy.org/en/latest/topics/spider-middleware.html  BOT_NAME = &#39;douban_demo&#39;  SPIDER_MODULES = [&#39;douban_demo.spiders&#39;]  NEWSPIDER_MODULE = &#39;douban_demo.spiders&#39;  # Crawl responsibly by identifying yourself (and your website) on the user-agent  #USER_AGENT = &#39;douban_demo (+http://www.yourdomain.com)&#39;  # Obey robots.txt rules  ROBOTSTXT_OBEY = True  # Configure maximum concurrent requests performed by Scrapy (default: 16)  #CONCURRENT_REQUESTS = 32  # ...</code></pre></li></ul></li><li><p>创建Spider</p><pre><code class="lang-bash"> # 进入项目目录，创建一个Spider $ cd douban_demo/ $ scrapy genspider movie movie.douban.com Created spider &#39;movie&#39; using template &#39;basic&#39; in module:   douban_demo.spiders.movie</code></pre><ul><li><p>查看新建的spider：<code>spider/movie.py</code></p><pre><code class="lang-python">  # -*- coding: utf-8 -*-  import scrapy  class MovieSpider(scrapy.Spider):      name = &#39;movie&#39;      allowed_domains = [&#39;movie.douban.com&#39;]      start_urls = [&#39;http://movie.douban.com/&#39;]      def parse(self, response):          pass</code></pre></li><li><p>编写spider：<code>spider/movie.py</code></p><pre><code class="lang-python">  # -*- coding: utf-8 -*-  import scrapy  import re  import json  from douban_demo.items import MovieItem  class MovieSpider(scrapy.Spider):      name = &#39;movie&#39;      allowed_domains = [&#39;movie.douban.com&#39;]      start_urls = [&#39;https://movie.douban.com/j/search_subjects?type=movie&amp;tag=热门&amp;sort=recommend&amp;page_limit=20&amp;page_start=0&#39;]      def parse(self, response):          print(response.url)          result=json.loads(response.body)          subjects=result.get(&#39;subjects&#39;)          if len(subjects)&gt;0:              for subject in subjects:                  # print(subject)                  yield MovieItem(subject)</code></pre></li><li>items.py<pre><code class="lang-python">  # -*- coding: utf-8 -*-  import scrapy  class MovieItem(scrapy.Item):      # {&#39;rate&#39;: &#39;7.0&#39;, &#39;cover_x&#39;: 7142, &#39;title&#39;: &#39;飞驰人生&#39;, &#39;url&#39;: &#39;https://movie.douban.com/subject/30163509/&#39;, &#39;playable&#39;: True, &#39;cover&#39;: &#39;https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2542973862.jpg&#39;, &#39;id&#39;: &#39;30163509&#39;, &#39;cover_y&#39;: 10000, &#39;is_new&#39;: False}      rate=scrapy.Field()      title=scrapy.Field()      url=scrapy.Field()      id=scrapy.Field()      is_new=scrapy.Field()</code></pre></li></ul></li><li><p>编写pipeline.py（optional）</p><pre><code class="lang-python"> from scrapy.conf import settings class MoviePipeline(object):     def process_item(self, item, spider):         print(item)         return item</code></pre></li><li><p>配置<code>setting.py</code></p><pre><code class="lang-python"> USER_AGENT=&#39;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:60.0) Gecko/20100101 Firefox/60.0&#39; LOG_LEVEL = &#39;INFO&#39; FEED_EXPORT_ENCODING=&#39;utf-8&#39; # Obey robots.txt rules ROBOTSTXT_OBEY = False ITEM_PIPELINES = {     &#39;douban_demo.pipelines.MoviePipeline&#39;:100 }</code></pre></li><li><p>运行Spider</p><pre><code class="lang-bash"> $ scrapy crawl movie $ scrapy crawl movie -o movies.json -s FEED_EXPORT_ENCODING=utf-8 -L INFO</code></pre></li></ol><h2 id="header-4">架构</h2><p><img src="/2019/03/20/scrapy.png" alt="Scrapy"></p><h3 id="header-5">“5+2”结构</h3><ol><li><p>引擎 <code>Engine</code>: </p><ul><li>控制所有模块之间的数据流,根据条件触发事件</li></ul></li><li><p>下载器 <code>Downloader</code>: </p><ul><li>根据请求下载网页</li></ul></li><li><p>调度器 <code>Scheduler</code>: </p><ul><li>对所有爬取请求进行调度管理</li></ul></li><li><p>爬虫 <code>Spider</code> (需要用户编写配置代码):</p><ul><li>解析<code>Downloader</code>返回的响应<code>Response</code></li><li>产生爬取项<code>Scraped item</code></li><li>产生额外的爬取请求<code>Request</code></li></ul></li><li><p>管道 <code>Item Pipelines</code> (需要用户编写配置代码):</p><ul><li>以流水线方式处理<code>Spider</code>产生的爬取项<code>Scraped item</code></li><li>由一组操作顺序组成,类似流水线,每个操作是一个<code>Item Pipeline</code>类型</li><li>可能操作包括: 清理、检验和查重爬取项中的HTML数据、将数据存储到数据库</li></ul></li><li><p>中间件 (用户可以编写配置代码)：</p><ul><li>下载中间件 <code>Downloader Middleware</code>: 修改、丢弃、新增请求<code>Request</code>或响应<code>Response</code></li><li>爬虫中间件<code>Spider Middleware</code>: 修改、丢弃、新增<code>Request</code>或爬取项<code>Scraped item</code></li></ul></li></ol><h3 id="header-6">出入口</h3><ol><li><p>框架入口: </p><ul><li><code>Spider</code>的初始爬取请求 </li></ul></li><li><p>框架出口: </p><ul><li><code>Item Pipeline</code></li></ul></li><li><p>数据流：</p><ul><li><code>Engine</code>控制各模块数据流,不间断从<code>Scheduler</code>处获得爬取<code>Request</code>,直至<code>Request</code>为空 </li></ul></li><li><p>用户编写： </p><ul><li><code>Spider</code>：处理链接爬取和页面解析</li><li><code>Item Pipelines</code>：处理信息存储</li><li><code>Middleware</code>：<ul><li><code>Spider Middleware</code>: 过滤<code>new requests</code> &amp; <code>Scraped Item</code></li><li><code>Downloader Middleware</code>: 过滤<code>request</code> &amp; <code>response</code></li></ul></li><li><code>Setting</code>：配置</li></ul></li></ol><h3 id="header-7">流程（数据流的三个路径）</h3><ul><li>UR2IM 流程<pre><code>  URL -&gt; Request -&gt; Response -&gt; Item -&gt; More URL          ^                      |-&gt; store   |          |__________________________________|</code></pre></li><li>路径1: <ul><li>Spider =&gt; <code>request</code> =&gt; Engine =&gt; <code>request</code> =&gt; Schedule<br>  (<code>Engine</code>从<code>Spider</code>处获得爬取<code>Request</code>，然后将<code>Request</code>转发给<code>Scheduler</code>,用于调度)</li></ul></li><li>路径2:<ul><li>Schedule =&gt; <code>request</code> =&gt; Engine =&gt; <code>request</code> =&gt; Downloader Middleware =&gt; Downloader<br>  （<code>Engine</code>从<code>Scheduler</code>处获得下一个要爬取的<code>Request</code>，通过<code>Downloader Middleware</code>后发给<code>Downloader</code> ）</li><li>Downloader =&gt; <code>response</code> =&gt; Downloader Middleware =&gt; Engine =&gt; Spider Middleware =&gt; Spider<br>  （爬取网页后<code>Downloader</code>形成响应<code>Response</code>, 通过<code>Downloader Middleware</code>后发回<code>Engine</code>，<code>Engine</code>将收到的响应通过<code>Spider Middleware</code>发送给<code>Spider</code>处理）</li></ul></li><li>路径3:<ul><li>Spider =&gt; <code>Scraped Item</code> &amp; <code>New Requests</code> =&gt; Engine<br>  ( <code>Spider</code>处理响应后产生爬取项<code>Scraped Item</code> 和新的爬取请求<code>Requests</code>给<code>Engine</code>)</li><li>Engine =&gt; <code>Scraped Item</code> =&gt; Item Pipeline<br>  (<code>Engine</code>将爬取项<code>Scraped Item</code>发送给框架出口<code>Item Pipeline</code>)</li><li>Engine =&gt; <code>New Requests</code> =&gt; Scheduler<br>  (<code>Engine</code>将爬取<code>Request</code>发送给<code>Scheduler</code>)</li></ul></li></ul><p><img src="/2019/03/20/scrapy-flow.png" alt="Scrapy Flow"></p><h3 id="header-8">常用命令</h3><p>Scrapy采用命令行创建和运行爬虫</p><p>命令行格式：<code>scrapy &lt;command&gt; [options] [args]</code></p><ol><li><p>Scrapy 命令：</p><pre><code class="lang-bash"> $ scrapy -h Scrapy 1.6.0 - project: douban Usage:   scrapy &lt;command&gt; [options] [args] Available commands:   bench         Run quick benchmark test   check         Check spider contracts   crawl         Run a spider   edit          Edit spider   fetch         Fetch a URL using the Scrapy downloader   genspider     Generate new spider using pre-defined templates   list          List available spiders   parse         Parse URL (using its spider) and print the results   runspider     Run a self-contained spider (without creating a project)   settings      Get settings values   shell         Interactive scraping console   startproject  Create new project   version       Print Scrapy version   view          Open URL in browser, as seen by Scrapy Use &quot;scrapy &lt;command&gt; -h&quot; to see more info about a command</code></pre><ul><li><p>常用命令说明：</p><table class="table"><thead><tr><th style="text-align:left">命令</th><th style="text-align:left">说明</th></tr></thead><tbody><tr><td style="text-align:left"><code>scrapy startproject &lt;name&gt; [dir]</code></td><td style="text-align:left">创建一个新工程</td></tr><tr><td style="text-align:left"><code>scrapy genspider [options] &lt;name&gt; &lt;domain&gt;</code></td><td style="text-align:left">创建一个爬虫</td></tr><tr><td style="text-align:left"><code>scrapy list</code></td><td style="text-align:left">列出工程中所有爬虫</td></tr><tr><td style="text-align:left"><code>scrapy crawl &lt;spider&gt;</code></td><td style="text-align:left">运行一个爬虫</td></tr><tr><td style="text-align:left"><code>scrapy settings [options]</code></td><td style="text-align:left">获得爬虫配置信息</td></tr><tr><td style="text-align:left"><code>scrapy shell [url]</code></td><td style="text-align:left">启动URL调试命令行</td></tr></tbody></table></li><li><p>Global Options:</p><pre><code class="lang-bash">  Global Options  --------------  ---logfile=FILE          log file. if omitted stderr will be used  --loglevel=LEVEL, -L LEVEL                          log level (default: DEBUG)  --nolog                 disable logging completely  --profile=FILE          write python cProfile stats to FILE  --pidfile=FILE          write process ID to FILE  --set=NAME=VALUE, -s NAME=VALUE                          set/override setting (may be repeated)  --pdb                   enable pdb on failure</code></pre></li></ul></li><li><p>Generate a spider options:</p><pre><code class="lang-bash"> $ scrapy genspider -h Usage =====   scrapy genspider [options] &lt;name&gt; &lt;domain&gt; Generate new spider using pre-defined templates Options ======= --help, -h              show this help message and exit --list, -l              List available templates --edit, -e              Edit spider after creating it --dump=TEMPLATE, -d TEMPLATE                         Dump template to standard output --template=TEMPLATE, -t TEMPLATE                         Uses a custom template. --force                 If the spider already exists, overwrite it with the                         template</code></pre></li><li><p>Run a spider options:</p><pre><code class="lang-bash"> Usage =====   scrapy crawl [options] &lt;spider&gt; Run a spider Options ======= --help, -h              show this help message and exit -a NAME=VALUE           set spider argument (may be repeated) --output=FILE, -o FILE  dump scraped items into FILE (use - for stdout) --output-format=FORMAT, -t FORMAT                         format to use for dumping items with -o</code></pre></li><li><p>Set spider settings:</p><pre><code class="lang-bash"> Usage =====   scrapy settings [options] Get settings values Options ======= --help, -h              show this help message and exit --get=SETTING           print raw setting value --getbool=SETTING       print setting value, interpreted as a boolean --getint=SETTING        print setting value, interpreted as an integer --getfloat=SETTING      print setting value, interpreted as a float --getlist=SETTING       print setting value, interpreted as a list</code></pre></li></ol><p><strong> 示例：使用 <code>scrapy shell</code> 交互式调试 </strong></p><pre><code class="lang-bash">$ scrapy shell...[s] Available Scrapy objects:[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x105cd57f0&gt;[s]   item       {}[s]   settings   &lt;scrapy.settings.Settings object at 0x105cd58d0&gt;[s] Useful shortcuts:[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)[s]   fetch(req)                  Fetch a scrapy.Request and update local objects[s]   shelp()           Shell help (print this help)[s]   view(response)    View response in a browser&gt;&gt;&gt; &gt;&gt;&gt; exit()</code></pre><pre><code class="lang-bash">$ scrapy shell https://movie.douban.com/top250...[s] Available Scrapy objects:[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x1037be908&gt;[s]   item       {}[s]   request    &lt;GET https://movie.douban.com/top250&gt;[s]   response   &lt;200 https://movie.douban.com/top250&gt;[s]   settings   &lt;scrapy.settings.Settings object at 0x1037be9e8&gt;[s]   spider     &lt;Top250Spider &#39;top250&#39; at 0x103bb2048&gt;[s] Useful shortcuts:[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)[s]   fetch(req)                  Fetch a scrapy.Request and update local objects[s]   shelp()           Shell help (print this help)[s]   view(response)    View response in a browser&gt;&gt;&gt; records=response.xpath(&#39;//ol[@class=&quot;grid_view&quot;]//div[@class=&quot;item&quot;]/div[@class=&quot;info&quot;]&#39;)&gt;&gt;&gt; len(records)25&gt;&gt;&gt; records.xpath(&#39;./div[@class=&quot;bd&quot;]/div[@class=&quot;star&quot;]/span[@class=&quot;rating_num&quot;]/text()&#39;).extract()[&#39;9.6&#39;, &#39;9.6&#39;, &#39;9.4&#39;, &#39;9.4&#39;, &#39;9.5&#39;, &#39;9.4&#39;, &#39;9.3&#39;, &#39;9.5&#39;, &#39;9.3&#39;, &#39;9.3&#39;, &#39;9.3&#39;, &#39;9.2&#39;, &#39;9.2&#39;, &#39;9.3&#39;, &#39;9.2&#39;, &#39;9.2&#39;, &#39;9.2&#39;, &#39;9.2&#39;, &#39;9.3&#39;, &#39;9.3&#39;, &#39;9.2&#39;, &#39;9.2&#39;, &#39;9.0&#39;, &#39;9.0&#39;, &#39;9.2&#39;]&gt;&gt;&gt; records[0].xpath(&#39;./div[@class=&quot;hd&quot;]/a/span[@class=&quot;title&quot;]/text()&#39;)[&lt;Selector xpath=&#39;./div[@class=&quot;hd&quot;]/a/span[@class=&quot;title&quot;]/text()&#39; data=&#39;肖申克&#39;&gt;, &lt;Selector xpath=&#39;./div[@class=&quot;hd&quot;]/a/span[@class=&quot;title&quot;]/text()&#39; data=&#39;\xa0/\xa0The Shawshank Redemption&#39;&gt;]&gt;&gt;&gt; records[0].xpath(&#39;./div[@class=&quot;hd&quot;]/a/span[@class=&quot;title&quot;]/text()&#39;).extract()[&#39;肖申克的救赎&#39;, &#39;\xa0/\xa0The Shawshank Redemption&#39;]&gt;&gt;&gt; records[0].xpath(&#39;./div[@class=&quot;hd&quot;]/a/span[@class=&quot;title&quot;]/text()&#39;).get()&#39;肖申克的救赎&#39;&gt;&gt;&gt; records[0].xpath(&#39;./div[@class=&quot;hd&quot;]/a/span[@class=&quot;title&quot;]/text()&#39;).extract_first()&#39;肖申克的救赎&#39;&gt;&gt;&gt;records[0].xpath(&#39;./div[@class=&quot;hd&quot;]/a/span[@class=&quot;title&quot;]/text()&#39;).re(&#39;[A-Za-z ]+&#39;)[&#39;The Shawshank Redemption&#39;]&gt;&gt;&gt; records=response.css(&#39;.grid_view .item .info&#39;)&gt;&gt;&gt; len(records)25# &gt;&gt;&gt; records.css(&#39;.bd .star .rating_num&#39;).xpath(&#39;text()&#39;).extract()# &gt;&gt;&gt; records.css(&#39;.bd .star .rating_num::text&#39;).extract()# &gt;&gt;&gt; records.css(&#39;.bd .star .rating_num&#39;).re(&#39;[\d.]+&#39;)&gt;&gt;&gt; records.xpath(&#39;./div[@class=&quot;bd&quot;]/div[@class=&quot;star&quot;]/span[@class=&quot;rating_num&quot;]/text()&#39;).extract()[&#39;9.6&#39;, &#39;9.6&#39;, &#39;9.4&#39;, &#39;9.4&#39;, &#39;9.5&#39;, &#39;9.4&#39;, &#39;9.3&#39;, &#39;9.5&#39;, &#39;9.3&#39;, &#39;9.3&#39;, &#39;9.3&#39;, &#39;9.2&#39;, &#39;9.2&#39;, &#39;9.3&#39;, &#39;9.2&#39;, &#39;9.2&#39;, &#39;9.2&#39;, &#39;9.2&#39;, &#39;9.3&#39;, &#39;9.3&#39;, &#39;9.2&#39;, &#39;9.2&#39;, &#39;9.0&#39;, &#39;9.0&#39;, &#39;9.2&#39;]&gt;&gt;&gt; len(response.css(&#39;div&#39;))183&gt;&gt;&gt; len(records.css(&#39;div&#39;))100&gt;&gt;&gt; exit()</code></pre><h3 id="header-9">解析文档</h3><ul><li>可使用BeautifulSoup,lxml解析，而Scrapy内部使用的是lxml（效率更高），可使用Xpath,CssSelector进行文档定位解析</li><li><code>response.xpath()</code>和<code>response.css()</code> 返回的<code>Selector</code>对象列表<code>SelectorList</code>是可以被串联起来的</li><li>获取<code>Selector</code>对象／<code>SelectorList</code>中的<code>data</code>，可以使用<code>.extract/getall()</code>,<code>.extract_first/get()</code>,<code>.re(pattern)</code></li><li>获取<code>Selector</code>对象/<code>SelectorList</code>中标签的某个属性值，可以使用<code>.attrib[&#39;attrName&#39;]</code></li><li>xpath可使用<code>/@attrName</code>获取属性值，eg: <code>response.xpath(//div[@class=&quot;item&quot;]//img/@src)</code></li><li>css selector 可使用伪代码<code>::</code>，eg: <ul><li>select text nodes, use <code>::text</code></li><li>select attribute values, use <code>::attr(attrName)</code></li><li>eg: <code>response.css(&#39;title::text&#39;).get(default=&#39;&#39;)</code>,<code>response.css(&#39;a::attr(href)&#39;).getall()</code></li></ul></li><li><a href="http://doc.scrapy.org/en/latest/topics/selectors.html" target="_blank" rel="noopener">Selector 文档</a></li></ul><h3 id="header-10">Settings 优先级</h3><pre><code class="lang-python">SETTINGS_PRIORITIES = {    &#39;default&#39;: 0,    &#39;command&#39;: 10,    &#39;project&#39;: 20,    &#39;spider&#39;: 30,    &#39;cmdline&#39;: 40,}</code></pre><ol><li>default: <code>scrapy/settings/default_settings.py</code></li><li>project: <code>[project]/settings.py</code></li><li>spider: Spider中配置的<code>custom_settings</code>属性</li><li>cmdline: 命令行运行时传入的 <code>-s xxxx=xxx</code> 参数</li></ol><pre><code class="lang-bash">$ scrapy settings --get CONCURRENT_REQUESTS16$ scrapy settings -s CONCURRENT_REQUESTS=19 --get CONCURRENT_REQUESTS19$ scrapy crawl movie -s CONCURRENT_REQUESTS=19$ scrapy shell -s CONCURRENT_REQUESTS=19[s] Available Scrapy objects:[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x105cd57f0&gt;[s]   item       {}[s]   settings   &lt;scrapy.settings.Settings object at 0x105cd58d0&gt;[s] Useful shortcuts:[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)[s]   fetch(req)                  Fetch a scrapy.Request and update local objects[s]   shelp()           Shell help (print this help)[s]   view(response)    View response in a browser&gt;&gt;&gt; settings.get(&#39;CONCURRENT_REQUESTS&#39;)&#39;19&#39;&gt;&gt;&gt; settings.getint(&#39;CONCURRENT_REQUESTS&#39;)19&gt;&gt;&gt; exit()</code></pre><h3 id="header-11">Default Settings</h3><ul><li><p>项目</p><pre><code class="lang-python">  BOT_NAME = &#39;scrapybot&#39;   # eg: &#39;douban&#39;  SPIDER_MODULES = []      # eg: [&#39;douban.spiders&#39;]  NEWSPIDER_MODULE = &#39;&#39;    # eg: &#39;douban.spiders&#39;  TEMPLATES_DIR = abspath(join(dirname(__file__), &#39;..&#39;, &#39;templates&#39;))  DEFAULT_ITEM_CLASS = &#39;scrapy.item.Item&#39;  EDITOR = &#39;vi&#39;  if sys.platform == &#39;win32&#39;:      EDITOR = &#39;%s -m idlelib.idle&#39;  # mail  MAIL_HOST = &#39;localhost&#39;  MAIL_PORT = 25  MAIL_FROM = &#39;scrapy@localhost&#39;  MAIL_PASS = None  MAIL_USER = None</code></pre></li><li><p>分析</p><pre><code class="lang-python">  # 日志 Log ：  LOG_ENABLED = True  LOG_ENCODING = &#39;utf-8&#39;  LOG_FORMATTER = &#39;scrapy.logformatter.LogFormatter&#39;  LOG_FORMAT = &#39;%(asctime)s [%(name)s] %(levelname)s: %(message)s&#39;  LOG_DATEFORMAT = &#39;%Y-%m-%d %H:%M:%S&#39;  LOG_STDOUT = False  LOG_LEVEL = &#39;DEBUG&#39;  LOG_FILE = None  LOG_SHORT_NAMES = False  LOGSTATS_INTERVAL = 60.0  # 统计 Stats ：  STATS_CLASS = &#39;scrapy.statscollectors.MemoryStatsCollector&#39;  STATS_DUMP = True  STATSMAILER_RCPTS = []  DEPTH_STATS_VERBOSE = False  DOWNLOADER_STATS = True  # Telnet：  TELNETCONSOLE_ENABLED = 1  TELNETCONSOLE_PORT = [6023, 6073]  TELNETCONSOLE_HOST = &#39;127.0.0.1&#39;  TELNETCONSOLE_USERNAME = &#39;scrapy&#39;  TELNETCONSOLE_PASSWORD = None</code></pre></li><li><p>爬取策略</p><pre><code class="lang-python">  # Cookie  COOKIES_ENABLED = True  COOKIES_DEBUG = False  # Request  DEFAULT_REQUEST_HEADERS = {      &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,      &#39;Accept-Language&#39;: &#39;en&#39;,  }  # User-Agent  USER_AGENT = &#39;Scrapy/%s (+https://scrapy.org)&#39; % import_module(&#39;scrapy&#39;).__version__  # robots.txt  ROBOTSTXT_OBEY = False  # 代理  HTTPPROXY_ENABLED = True  HTTPPROXY_AUTH_ENCODING = &#39;latin-1&#39;  # referer  REFERER_ENABLED = True  REFERRER_POLICY = &#39;scrapy.spidermiddlewares.referer.DefaultReferrerPolicy&#39;  # rediret  REDIRECT_ENABLED = True  REDIRECT_MAX_TIMES = 20  # uses Firefox default setting  REDIRECT_PRIORITY_ADJUST = +2  # retry  RETRY_ENABLED = True  RETRY_TIMES = 2  # initial response + 2 retries = 3 requests  RETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524, 408]  RETRY_PRIORITY_ADJUST = -1  # meta refresh  METAREFRESH_ENABLED = True  METAREFRESH_MAXDELAY = 100  # DNS  DNSCACHE_ENABLED = True  DNSCACHE_SIZE = 10000  DNS_TIMEOUT = 60  # Http缓存  HTTPCACHE_ENABLED = False  HTTPCACHE_DIR = &#39;httpcache&#39;  HTTPCACHE_IGNORE_MISSING = False  HTTPCACHE_STORAGE = &#39;scrapy.extensions.httpcache.FilesystemCacheStorage&#39;  HTTPCACHE_EXPIRATION_SECS = 0  HTTPCACHE_ALWAYS_STORE = False  HTTPCACHE_IGNORE_HTTP_CODES = []  HTTPCACHE_IGNORE_SCHEMES = [&#39;file&#39;]  HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS = []  HTTPCACHE_DBM_MODULE = &#39;anydbm&#39; if six.PY2 else &#39;dbm&#39;  HTTPCACHE_POLICY = &#39;scrapy.extensions.httpcache.DummyPolicy&#39;  HTTPCACHE_GZIP = False  # 并发  CONCURRENT_ITEMS = 100  CONCURRENT_REQUESTS = 16  CONCURRENT_REQUESTS_PER_DOMAIN = 8  CONCURRENT_REQUESTS_PER_IP = 0  REACTOR_THREADPOOL_MAXSIZE = 10  # Depth  DEPTH_LIMIT = 0  DEPTH_STATS_VERBOSE = False  DEPTH_PRIORITY = 0   # 结束爬取  CLOSESPIDER_TIMEOUT = 0  CLOSESPIDER_PAGECOUNT = 0  CLOSESPIDER_ITEMCOUNT = 0  CLOSESPIDER_ERRORCOUNT = 0  # 自动限速  AUTOTHROTTLE_ENABLED = False  AUTOTHROTTLE_DEBUG = False  AUTOTHROTTLE_MAX_DELAY = 60.0  AUTOTHROTTLE_START_DELAY = 5.0  AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0   # Memory    MEMDEBUG_ENABLED = False        # enable memory debugging  MEMDEBUG_NOTIFY = []            # send memory debugging report by mail at engine shutdown  MEMUSAGE_CHECK_INTERVAL_SECONDS = 60.0  MEMUSAGE_ENABLED = True  MEMUSAGE_LIMIT_MB = 0  MEMUSAGE_NOTIFY_MAIL = []  MEMUSAGE_WARNING_MB = 0  # other   AJAXCRAWL_ENABLED = False  COMPRESSION_ENABLED = True</code></pre></li><li><p>组件</p><pre><code class="lang-python">  # 1. Scheduler  SCHEDULER = &#39;scrapy.core.scheduler.Scheduler&#39;  SCHEDULER_DISK_QUEUE = &#39;scrapy.squeues.PickleLifoDiskQueue&#39;  SCHEDULER_MEMORY_QUEUE = &#39;scrapy.squeues.LifoMemoryQueue&#39;  SCHEDULER_PRIORITY_QUEUE = &#39;queuelib.PriorityQueue&#39;  SCHEDULER_DEBUG = False  # 2. Downloader  DOWNLOADER = &#39;scrapy.core.downloader.Downloader&#39;  DOWNLOADER_HTTPCLIENTFACTORY = &#39;scrapy.core.downloader.webclient.ScrapyHTTPClientFactory&#39;  DOWNLOADER_CLIENTCONTEXTFACTORY = &#39;scrapy.core.downloader.contextfactory.ScrapyClientContextFactory&#39;  DOWNLOADER_CLIENT_TLS_METHOD = &#39;TLS&#39; # Use highest TLS/SSL protocol version supported by the platform,                                       # also allowing negotiation  DOWNLOADER_STATS = True  RANDOMIZE_DOWNLOAD_DELAY = True  DOWNLOAD_DELAY = 0  DOWNLOAD_HANDLERS = {}  DOWNLOAD_HANDLERS_BASE = {      &#39;data&#39;: &#39;scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler&#39;,      &#39;file&#39;: &#39;scrapy.core.downloader.handlers.file.FileDownloadHandler&#39;,      &#39;http&#39;: &#39;scrapy.core.downloader.handlers.http.HTTPDownloadHandler&#39;,      &#39;https&#39;: &#39;scrapy.core.downloader.handlers.http.HTTPDownloadHandler&#39;,      &#39;s3&#39;: &#39;scrapy.core.downloader.handlers.s3.S3DownloadHandler&#39;,      &#39;ftp&#39;: &#39;scrapy.core.downloader.handlers.ftp.FTPDownloadHandler&#39;,  }  DOWNLOAD_TIMEOUT = 180      # 3mins  DOWNLOAD_MAXSIZE = 1024*1024*1024   # 1024m  DOWNLOAD_WARNSIZE = 32*1024*1024    # 32m  DOWNLOAD_FAIL_ON_DATALOSS = True  # 3. Item Pipeline  ITEM_PROCESSOR = &#39;scrapy.pipelines.ItemPipelineManager&#39;  ITEM_PIPELINES = {}  ITEM_PIPELINES_BASE = {}  # 4. Spider Middleware  SPIDER_MIDDLEWARES = {}  SPIDER_MIDDLEWARES_BASE = {      # Engine side      &#39;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&#39;: 50,      &#39;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&#39;: 500,      &#39;scrapy.spidermiddlewares.referer.RefererMiddleware&#39;: 700,      &#39;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&#39;: 800,      &#39;scrapy.spidermiddlewares.depth.DepthMiddleware&#39;: 900,      # Spider side  }  # 5. Downloader Middleware  DOWNLOADER_MIDDLEWARES = {}  DOWNLOADER_MIDDLEWARES_BASE = {      # Engine side      &#39;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&#39;: 100,      &#39;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&#39;: 300,      &#39;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&#39;: 350,      &#39;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&#39;: 400,      &#39;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#39;: 500,      &#39;scrapy.downloadermiddlewares.retry.RetryMiddleware&#39;: 550,      &#39;scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware&#39;: 560,      &#39;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&#39;: 580,      &#39;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#39;: 590,      &#39;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&#39;: 600,      &#39;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&#39;: 700,      &#39;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&#39;: 750,      &#39;scrapy.downloadermiddlewares.stats.DownloaderStats&#39;: 850,      &#39;scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware&#39;: 900,      # Downloader side  }  # 6. Extension Middleware  EXTENSIONS = {}  EXTENSIONS_BASE = {      &#39;scrapy.extensions.corestats.CoreStats&#39;: 0,      &#39;scrapy.extensions.telnet.TelnetConsole&#39;: 0,      &#39;scrapy.extensions.memusage.MemoryUsage&#39;: 0,      &#39;scrapy.extensions.memdebug.MemoryDebugger&#39;: 0,      &#39;scrapy.extensions.closespider.CloseSpider&#39;: 0,      &#39;scrapy.extensions.feedexport.FeedExporter&#39;: 0,      &#39;scrapy.extensions.logstats.LogStats&#39;: 0,      &#39;scrapy.extensions.spiderstate.SpiderState&#39;: 0,      &#39;scrapy.extensions.throttle.AutoThrottle&#39;: 0,  }</code></pre></li><li><p>Output</p><pre><code class="lang-python">  # Feeds &amp; Exporter:  FEED_TEMPDIR = None  FEED_URI = None  FEED_URI_PARAMS = None  # a function to extend uri arguments  FEED_FORMAT = &#39;jsonlines&#39;  FEED_STORE_EMPTY = False  FEED_EXPORT_ENCODING = None  FEED_EXPORT_FIELDS = None  FEED_STORAGES = {}  FEED_STORAGES_BASE = {      &#39;&#39;: &#39;scrapy.extensions.feedexport.FileFeedStorage&#39;,      &#39;file&#39;: &#39;scrapy.extensions.feedexport.FileFeedStorage&#39;,      &#39;stdout&#39;: &#39;scrapy.extensions.feedexport.StdoutFeedStorage&#39;,      &#39;s3&#39;: &#39;scrapy.extensions.feedexport.S3FeedStorage&#39;,      &#39;ftp&#39;: &#39;scrapy.extensions.feedexport.FTPFeedStorage&#39;,  }  FEED_EXPORTERS = {}  FEED_EXPORTERS_BASE = {      &#39;json&#39;: &#39;scrapy.exporters.JsonItemExporter&#39;,      &#39;jsonlines&#39;: &#39;scrapy.exporters.JsonLinesItemExporter&#39;,      &#39;jl&#39;: &#39;scrapy.exporters.JsonLinesItemExporter&#39;,      &#39;csv&#39;: &#39;scrapy.exporters.CsvItemExporter&#39;,      &#39;xml&#39;: &#39;scrapy.exporters.XmlItemExporter&#39;,      &#39;marshal&#39;: &#39;scrapy.exporters.MarshalItemExporter&#39;,      &#39;pickle&#39;: &#39;scrapy.exporters.PickleItemExporter&#39;,  }  FEED_EXPORT_INDENT = 0  # file s3/gcs store  FILES_STORE_S3_ACL = &#39;private&#39;  FILES_STORE_GCS_ACL = &#39;&#39;  # image s3/gsc store  IMAGES_STORE_S3_ACL = &#39;private&#39;  IMAGES_STORE_GCS_ACL = &#39;&#39;  # ftp  FTP_USER = &#39;anonymous&#39;  FTP_PASSWORD = &#39;guest&#39;  FTP_PASSIVE_MODE = True</code></pre></li></ul><h3 id="header-12">数据类型</h3><ul><li><p><code>Request</code></p><ul><li><code>class scrapy.http.Request()</code></li><li>表示一个HTTP请求,由<code>Spider</code>生成,<code>Downloader</code>执行</li><li>属性：<ul><li><code>.url</code> : Request对应的请求URL地址</li><li><code>.method</code> : 对应的请求方法,’GET’ ‘POST’等</li><li><code>.headers</code>: 字典类型风格的请求头</li><li><code>.body</code> : 请求内容主体,字符串类型</li><li><code>.meta</code> : 用户添加的扩展信息,在Scrapy内部模块间传递信息使用</li><li><code>.encoding</code></li><li><code>.dont_filter</code>: 默认为False（表示要过滤掉重复Request）</li></ul></li><li>方法：<ul><li><code>.copy()</code></li><li><code>.replace()</code></li></ul></li><li>子类：<ul><li><code>FormRequest</code><ul><li><code>from_response()</code></li></ul></li><li><code>XmlRpcRequest</code></li></ul></li></ul></li><li><p><code>Response</code></p><ul><li><code>class scrapy.http.Response()</code></li><li>表示一个HTTP响应,由<code>Downloader</code>生成,<code>Spider</code>处理</li><li>属性：<ul><li><code>.url</code> : Response对应的URL地址</li><li><code>.status</code> : HTTP状态码,默认是200</li><li><code>.headers</code> : Response对应的头部信息</li><li><code>.body</code> : Response对应的内容信息,字符串类型 </li><li><code>.flags</code> : 一组标记</li><li><code>.request</code> : 产生Response类型对应的Request对象</li><li><code>.meta</code></li><li><code>.text</code></li></ul></li><li>方法：<ul><li><code>.copy()</code></li><li><code>.replace()</code></li><li><code>.urljoin()</code></li><li><code>.xpath()</code></li><li><code>.css()</code></li><li><code>.follow()</code></li></ul></li><li>子类：<ul><li><code>TextResponse</code><ul><li><code>.encoding</code></li><li>子类：<code>XmlResponse</code>，<code>HtmlResponse</code></li></ul></li></ul></li></ul></li><li><p><code>Item</code></p><ul><li><code>class scrapy.item.Item()</code></li><li>表示一个从HTML页面中提取的信息内容,由<code>Spider</code>生成,<code>Item Pipeline</code>处理</li><li>似字典类型,可按照字典类型操作</li></ul></li></ul><h2 id="header-13">Spider</h2><p>Refer <a href="http://doc.scrapy.org/en/latest/topics/spiders.html#scrapy-spider" target="_blank" rel="noopener">Scrapy Spider</a></p><h3 id="header-14">创建运行Spider</h3><ol><li><p>创建Spider： <code>scrapy genspider &lt;-t [template:default is basic]&gt; [spiderName] [domain]</code></p><ul><li>查看有效Spider template：<code>scrapy genspider --list</code><pre><code class="lang-bash">  $ scrapy genspider --list  Available templates:    basic    crawl    csvfeed    xmlfeed</code></pre></li><li>eg: 创建基础Spider<pre><code class="lang-bash">  $ scrapy genspider movie movie.douban.com  Created spider &#39;movie&#39; using template &#39;basic&#39; in module:    douban_demo.spiders.movie</code></pre></li><li>eg: 创建CrawlSpider:<pre><code class="lang-bash">  $ scrapy genspider -t crawl top250 movie.douban.com   Created spider &#39;top250&#39; using template &#39;crawl&#39; in module:    douban_demo.spiders.top250</code></pre></li></ul></li><li><p>运行Spider：<code>scrapy crawl [spiderName] &lt; -a [argName]=[argValue] &gt; &lt; -o [File]&gt; &lt;-t [Format] &gt;</code></p><ul><li><code>-a [argName=argValue]</code>: passed arguments to spider,Spiders can access arguments in their <code>__init__</code> methods</li><li><code>-o [File]</code>: dump scraped items into FILE,recognize file extension as format,also could use <code>-t [Format]</code> set output format</li><li><p>eg:</p><pre><code class="lang-bash">  $ scrapy crawl movie  # 传参给spider，spider中初始化函数中添加参数：  # def __init__(self, category=None, *args, **kwargs)  $ scrapy crawl movie -a category=top  # 输出到文件，根据文件后缀名，以对应格式输出  $ scrapy crawl movie -o movies.json     # [{},{},...]  $ scrapy crawl movie -o movies.jl       # {},{},... =&gt; recommend !  $ scrapy crawl movie -o movies.csv      # xxx,xxx,xxx,...  # -s,-L same as set in setting.py: FEED_EXPORT_ENCODING=&#39;utf-8&#39;, LOG_LEVEL = &#39;INFO&#39;  $ scrapy crawl movie -o movies.json -s FEED_EXPORT_ENCODING=utf-8 -L INFO  # -s CLOSESPIDER_ITEMCOUNT 控制最多爬取10个Item  $ scrapy crawl movie -o movies.json -s CLOSESPIDER_ITEMCOUNT=10  # use - for stdout  $ scrapy crawl movie -t json -o -&gt; movies.json</code></pre></li></ul></li></ol><h3 id="header-15">基类：Spider</h3><p><strong> Sample1: 自动生成的basic spider (<code>scrapy genspider movie movie.douban.com</code>) </strong></p><pre><code class="lang-python"># -*- coding: utf-8 -*-import scrapyclass MovieSpider(scrapy.Spider):    name = &#39;movie&#39;    allowed_domains = [&#39;movie.douban.com&#39;]    start_urls = [&#39;http://movie.douban.com/&#39;]    def parse(self, response):        pass</code></pre><p><strong> Sample2: 使用<code>start_urls/start_requests()</code> &amp; <code>parse(response)</code> =&gt; <code>self.logger/log</code>, <code>yield Item,Request</code> </strong></p><pre><code class="lang-python">class MySpider(scrapy.Spider):    name = &#39;example.com&#39;    allowed_domains = [&#39;example.com&#39;]    start_urls = [        &#39;http://www.example.com/1.html&#39;,        &#39;http://www.example.com/2.html&#39;,        &#39;http://www.example.com/3.html&#39;,    ]    #def start_requests(self):    #    yield scrapy.Request(&#39;http://www.example.com/1.html&#39;, self.parse)    #    yield scrapy.Request(&#39;http://www.example.com/2.html&#39;, self.parse)    #    yield scrapy.Request(&#39;http://www.example.com/3.html&#39;, self.parse)    def parse(self, response):        self.logger.info(&#39;A response from %s just arrived!&#39;, response.url)        # or self.log(&#39;....&#39;)        for h3 in response.xpath(&#39;//h3&#39;).getall():            yield {&quot;title&quot;: h3}        for href in response.xpath(&#39;//a/@href&#39;).getall():            yield scrapy.Request(response.urljoin(href), self.parse)</code></pre><p><strong> Note：</strong></p><ol><li>属性：<ul><li>name</li><li>allowed_domains</li><li>start_urls</li><li>custom_settings</li><li>crawler</li><li>settings</li><li>logger</li></ul></li><li>方法：<ul><li><strong>init</strong>(self, name=None, **kwargs)</li><li>from_crawler(cls, crawler, <em>args, *</em>kwargs)</li><li>start_requests(self)</li><li>parse(self,response)</li><li>log(self, message, level=logging.DEBUG, **kw])</li><li>closed(self,reason)</li></ul></li><li>Key:<ul><li><code>name</code></li><li><code>allowed_domains</code></li><li><code>custom_settings</code></li><li><code>start_urls=[]</code> / <code>start_requests(self)</code> =&gt; yield Request (Note: 这里产生的Request的dont_filter=True)</li><li><code>parse(self,response)</code> =&gt; yield Item,yield Request</li><li><code>closed(self,reason)</code> =&gt; Called when the spider closes(for the <code>spider_closed</code> signal)</li></ul></li></ol><h3 id="header-16">子类：CrawlSpider</h3><p><strong> Sample1：自动生成的crawl spider (<code>scrapy genspider -t crawl top250 movie.douban.com</code>) </strong></p><pre><code class="lang-python"># -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Ruleclass Top250Spider(CrawlSpider):    name = &#39;top250&#39;    allowed_domains = [&#39;movie.douban.com&#39;]    start_urls = [&#39;http://movie.douban.com/&#39;]    rules = (        Rule(LinkExtractor(allow=r&#39;Items/&#39;), callback=&#39;parse_item&#39;, follow=True),    )    def parse_item(self, response):        item = {}        #item[&#39;domain_id&#39;] = response.xpath(&#39;//input[@id=&quot;sid&quot;]/@value&#39;).get()        #item[&#39;name&#39;] = response.xpath(&#39;//div[@id=&quot;name&quot;]&#39;).get()        #item[&#39;description&#39;] = response.xpath(&#39;//div[@id=&quot;description&quot;]&#39;).get()        return item</code></pre><p><strong> Sample2：<code>rules=(...)</code> &amp; self def callback <code>parse_item(self,response)</code> </strong> </p><pre><code class="lang-python"># -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom douban_demo.items import Top250Itemclass Top250Spider(CrawlSpider):    name = &#39;top250&#39;    allowed_domains = [&#39;movie.douban.com&#39;]    start_urls = [&#39;https://movie.douban.com/top250&#39;]    rules = (        Rule(            LinkExtractor(allow=r&#39;\?start=\d+.*&#39;,restrict_xpaths=&#39;//div[@class=&quot;paginator&quot;]&#39;)            , callback=&#39;parse_item&#39;, follow=True),    )    def parse_item(self, response):        print(response.url)        records=response.xpath(&#39;//ol[@class=&quot;grid_view&quot;]//div[@class=&quot;item&quot;]/div[@class=&quot;info&quot;]&#39;)        for r in records:            item=Top250Item()            link=r.xpath(&#39;./div[@class=&quot;hd&quot;]/a/@href&#39;).get()            item[&#39;id&#39;]=link.split(&#39;/&#39;)[-2]            item[&#39;title&#39;]=r.xpath(&#39;./div[@class=&quot;hd&quot;]/a/span[@class=&quot;title&quot;]/text()&#39;).extract_first()            item[&#39;rate&#39;]=r.xpath(&#39;./div[@class=&quot;bd&quot;]/div[@class=&quot;star&quot;]/span[@class=&quot;rating_num&quot;]/text()&#39;).extract_first()            item[&#39;quote&#39;]=r.xpath(&#39;./div[@class=&quot;bd&quot;]/p[@class=&quot;quote&quot;]/span/text()&#39;).extract_first()            yield item</code></pre><p><strong> Note: </strong></p><ol><li>属性：<ul><li>rules=()<ul><li><code>Rule(...)</code>: defines a certain behaviour for crawling the site (内部通过<code>request.meta</code>的<code>rule</code>传递)<ul><li><code>link_extractor</code>: <code>scrapy.linkextractors.LinkExtractor</code> defines how links will be extracted from each crawled page</li><li><code>callback=None</code>: callable,handle response =&gt; yield Item,yield Request</li><li><code>cb_kwargs=None</code>: dict, passed args to the callback function</li><li><code>follow=None</code>: True/False, if still extract the response with this rule (callback:None =&gt; default follow:True; els default follow:False)</li><li><code>process_links=None</code>: callable,used for filtering extract links.</li><li><code>process_request=None</code>: callable,used for filtering extract requests.</li></ul></li><li><code>LinkExtractor(...)</code>: used for extracting links from response<ul><li>allow=()</li><li>deny=()</li><li>allow_domains=()</li><li>deny_domains=()</li><li>deny_extensions=None</li><li>restrict_xpaths=()</li><li>restrict_css=()</li><li>tags=(‘a’, ‘area’)</li><li>attrs=(‘href’, )</li><li>canonicalize=False</li><li>unique=True</li><li>process_value=None</li><li>strip=True</li></ul></li></ul></li></ul></li><li>方法：<ul><li><code>parse_start_url(self, response)</code>： call for start_urls responses =&gt; yield <code>Item</code>/<code>Request</code></li><li><code>process_results(self, response, results)</code>：call for parse_start_url results =&gt; yield <code>Item</code>/<code>Request</code></li></ul></li><li>Key：<ul><li><code>start_urls=[]</code> / <code>start_requests(self)</code> =&gt; yield Request</li><li>for start requests responses: <code>parse_start_url(self,response)</code>,<code>process_results(self,respons,results)</code> =&gt; yield <code>Item</code>/<code>Request</code></li><li>after start requests follow the rules: <code>rules=(Rule(LinkExtractor(...),callback=&#39;parse_item&#39;,follow=True),...)</code> =&gt; yield <code>Request</code></li><li>def callback func(note:avoid using <code>parse</code> as callback): <code>parse_item(self,response)</code> =&gt; yield <code>Item</code>/<code>Request</code></li></ul></li></ol><p><strong> Sample: LinkExtractor </strong></p><pre><code class="lang-python">$ scrapy shell https://movie.douban.com/top250/...&gt;&gt;&gt; from scrapy.linkextractors import LinkExtractor&gt;&gt;&gt; le=LinkExtractor(allow=r&#39;\?start=\d+.*&#39;,restrict_xpaths=&#39;//div[@class=&quot;paginator&quot;]&#39;)&gt;&gt;&gt; le.extract_links(response)[Link(url=&#39;https://movie.douban.com/top250?start=25&amp;filter=&#39;, text=&#39;2&#39;, fragment=&#39;&#39;, nofollow=False), Link(url=&#39;https://movie.douban.com/top250?start=50&amp;filter=&#39;, text=&#39;3&#39;, fragment=&#39;&#39;, nofollow=False), Link(url=&#39;https://movie.douban.com/top250?start=75&amp;filter=&#39;, text=&#39;4&#39;, fragment=&#39;&#39;, nofollow=False), Link(url=&#39;https://movie.douban.com/top250?start=100&amp;filter=&#39;, text=&#39;5&#39;, fragment=&#39;&#39;, nofollow=False), Link(url=&#39;https://movie.douban.com/top250?start=125&amp;filter=&#39;, text=&#39;6&#39;, fragment=&#39;&#39;, nofollow=False), Link(url=&#39;https://movie.douban.com/top250?start=150&amp;filter=&#39;, text=&#39;7&#39;, fragment=&#39;&#39;, nofollow=False), Link(url=&#39;https://movie.douban.com/top250?start=175&amp;filter=&#39;, text=&#39;8&#39;, fragment=&#39;&#39;, nofollow=False), Link(url=&#39;https://movie.douban.com/top250?start=200&amp;filter=&#39;, text=&#39;9&#39;, fragment=&#39;&#39;, nofollow=False), Link(url=&#39;https://movie.douban.com/top250?start=225&amp;filter=&#39;, text=&#39;10&#39;, fragment=&#39;&#39;, nofollow=False)]&gt;&gt;&gt; le.allow_res[re.compile(&#39;\\?start=\\d+.*&#39;)]&gt;&gt;&gt; le.restrict_xpaths(&#39;//div[@class=&quot;paginator&quot;]&#39;,)</code></pre><h3 id="header-17">子类：XMLFeedSpider</h3><p><strong> Sample1：</strong></p><pre><code class="lang-python">from scrapy.spiders import XMLFeedSpiderfrom myproject.items import TestItemclass MySpider(XMLFeedSpider):    name = &#39;example.com&#39;    allowed_domains = [&#39;example.com&#39;]    start_urls = [&#39;http://www.example.com/feed.xml&#39;]    iterator = &#39;iternodes&#39;  # This is actually unnecessary, since it&#39;s the default value    itertag = &#39;item&#39;        # change it accordingly    def parse_node(self, response, node):        self.logger.info(&#39;Hi, this is a &lt;%s&gt; node!: %s&#39;, self.itertag, &#39;&#39;.join(node.getall()))        item = TestItem()        item[&#39;id&#39;] = node.xpath(&#39;@id&#39;).get()        item[&#39;name&#39;] = node.xpath(&#39;name&#39;).get()        item[&#39;description&#39;] = node.xpath(&#39;description&#39;).get()        return item</code></pre><p><strong> Sample2: 用xmlfeed爬取新浪博客的订阅信息(<code>scrapy genspider -crawl xmlfeed sinaRss sina.com.cn</code>) </strong></p><pre><code class="lang-xml">&lt;rss xmlns:sns=&quot;http://blog.sina.com.cn/sns&quot; version=&quot;2.0&quot;&gt;    &lt;channel&gt;        &lt;title&gt;科幻星系&lt;/title&gt;        &lt;description/&gt;        &lt;link&gt;http://blog.sina.com.cn/sfw&lt;/link&gt;        &lt;item&gt;            &lt;title&gt;手机进化凶猛背后的“凄凉”电池何时才能飞奔起来？&lt;/title&gt;            &lt;link&gt;http://blog.sina.com.cn/s/blog_4a46c3960102zgcw.html&lt;/link&gt;            &lt;description&gt;...&lt;/description&gt;        &lt;/item&gt;        &lt;item&gt;            &lt;title&gt;中国5G，如何避免重复投资？&lt;/title&gt;            &lt;link&gt;http://blog.sina.com.cn/s/blog_4a46c3960102zgcb.html&lt;/link&gt;            &lt;description&gt;...&lt;/description&gt;        &lt;/item&gt;        &lt;item&gt;            &lt;title&gt;与英特尔分道扬镳，苹果的5G业务掉队了吗？？&lt;/title&gt;            &lt;link&gt;http://blog.sina.com.cn/s/blog_4a46c3960102zgbj.html&lt;/link&gt;            &lt;description&gt;...&lt;/description&gt;        &lt;/item&gt;    &lt;/channel&gt;&lt;/rss&gt;</code></pre><pre><code class="lang-python"># -*- coding: utf-8 -*-from scrapy.spiders import XMLFeedSpiderclass SinaRssSpider(XMLFeedSpider):    name = &#39;sinaRss&#39;    allowed_domains = [&#39;sina.com.cn&#39;]    start_urls = [&#39;http://blog.sina.com.cn/rss/1246151574.xml&#39;]    iterator = &#39;iternodes&#39; # This is actually unnecessary, since it&#39;s the default value    itertag = &#39;item&#39;       # change it accordingly    def parse_node(self, response, selector):        item = {}        item[&#39;title&#39;] = selector.xpath(&#39;title/text()&#39;).get()        item[&#39;link&#39;] = selector.xpath(&#39;link/text()&#39;).get()        return item</code></pre><p>使用 <code>scrapy parse</code> 查看：</p><pre><code class="lang-bash">$ scrapy parse --spider=sinaRss http://blog.sina.com.cn/rss/1246151574.xml...&gt;&gt;&gt; STATUS DEPTH LEVEL 1 &lt;&lt;&lt;# Scraped Items  ------------------------------------------------------------[{&#39;link&#39;: &#39;http://blog.sina.com.cn/s/blog_4a46c3960102zgcw.html&#39;,  &#39;title&#39;: &#39;手机进化凶猛背后的“凄凉”电池何时才能飞奔起来？&#39;}, {&#39;link&#39;: &#39;http://blog.sina.com.cn/s/blog_4a46c3960102zgcb.html&#39;,  &#39;title&#39;: &#39;中国5G，如何避免重复投资？&#39;}, {&#39;link&#39;: &#39;http://blog.sina.com.cn/s/blog_4a46c3960102zgbj.html&#39;,  &#39;title&#39;: &#39;与英特尔分道扬镳，苹果的5G业务掉队了吗？&#39;}, {&#39;link&#39;: &#39;http://blog.sina.com.cn/s/blog_4a46c3960102zga4.html&#39;,  &#39;title&#39;: &#39;为什么越来越多的知名品牌热衷打造快闪店？&#39;}, {&#39;link&#39;: &#39;http://blog.sina.com.cn/s/blog_4a46c3960102zg9a.html&#39;,  &#39;title&#39;: &#39;电商专供还是电商专坑,背后的这些猫腻你知道多少?&#39;}, {&#39;link&#39;: &#39;http://blog.sina.com.cn/s/blog_4a46c3960102zg8n.html&#39;,  &#39;title&#39;: &#39;八年坎坷崎岖路：安卓平板为何终究是扶不起的“阿斗”&#39;}, {&#39;link&#39;: &#39;http://blog.sina.com.cn/s/blog_4a46c3960102zg87.html&#39;,  &#39;title&#39;: &#39;火爆的直播能让电视购物焕发第二春吗？&#39;}, {&#39;link&#39;: &#39;http://blog.sina.com.cn/s/blog_4a46c3960102zg7f.html&#39;,  &#39;title&#39;: &#39;各大厂商发力5G新机，未来全球手机市场或将呈现新格局&#39;}, {&#39;link&#39;: &#39;http://blog.sina.com.cn/s/blog_4a46c3960102zg6r.html&#39;,  &#39;title&#39;: &#39;研发2nm芯片，台积电如何做到天下第一？&#39;}, {&#39;link&#39;: &#39;http://blog.sina.com.cn/s/blog_4a46c3960102zg6j.html&#39;,  &#39;title&#39;: &#39;芬兰采购中国无人机，值得骄傲吗？&#39;}]# Requests  -----------------------------------------------------------------[]</code></pre><p><strong> Note: </strong></p><ol><li>属性：<ul><li><code>iterator = &#39;iternodes&#39;</code><ul><li>‘iternodes’(default): a fast iterator based on regular expressions </li><li>‘html’: an iterator which uses Selector (load all DOM in memory)</li><li>‘xml’: an iterator which uses Selector (load all DOM in memory)</li></ul></li><li><code>itertag = &#39;item&#39;</code>: the name of the node (or element) to iterate in</li><li><code>namespaces = ()</code>: A list of <code>(prefix, uri)</code> tuples</li></ul></li><li>方法：<ul><li><code>parse(self, response)</code><ul><li>call <code>adapt_response(self, response)</code>: return <code>response</code></li><li>call <code>parse_nodes(self, response, nodes)</code> =&gt; yield <code>Item</code>/<code>Request</code><ul><li>call <code>parse_node(self, response, selector)</code>: return <code>Item</code>/<code>Request</code></li><li>call <code>process_results(self, response, results)</code>: filter <code>Item</code>/<code>Request</code></li></ul></li></ul></li></ul></li><li>Key:<ul><li>pre: <code>adapt_response(self, response)</code></li><li>process item: <code>parse_node(self, response, selector)</code></li><li>post: <code>process_results(self, response, results)</code></li></ul></li></ol><h3 id="header-18">子类：CSVFeedSpider</h3><p><strong> Sample：</strong></p><pre><code class="lang-python">from scrapy.spiders import CSVFeedSpiderfrom myproject.items import TestItemclass MySpider(CSVFeedSpider):    name = &#39;example.com&#39;    allowed_domains = [&#39;example.com&#39;]    start_urls = [&#39;http://www.example.com/feed.csv&#39;]    delimiter = &#39;;&#39;    quotechar = &quot;&#39;&quot;    headers = [&#39;id&#39;, &#39;name&#39;, &#39;description&#39;]    def parse_row(self, response, row):        self.logger.info(&#39;Hi, this is a row!: %r&#39;, row)        item = TestItem()        item[&#39;id&#39;] = row[&#39;id&#39;]        item[&#39;name&#39;] = row[&#39;name&#39;]        item[&#39;description&#39;] = row[&#39;description&#39;]        return item</code></pre><p><strong> Note: </strong></p><ol><li>属性：<ul><li><code>delimiter</code>: separator character for each field, default is <code>,</code></li><li><code>quotechar</code>: enclosure character for each field,default is <code>&quot;</code></li><li><code>headers</code>: A list of the column names in the CSV file</li></ul></li><li>方法：<ul><li><code>parse(self, response)</code><ul><li>call <code>adapt_response(self, response)</code>: return <code>response</code></li><li>call <code>parse_rows(self, response)</code> =&gt; yield <code>Item</code>/<code>Request</code><ul><li>call <code>parse_row(self, response, row)</code>: return <code>Item</code>/<code>Request</code></li><li>call <code>process_results(self, response, results)</code>: filter <code>Item</code>/<code>Request</code>   </li></ul></li></ul></li></ul></li><li>Key：<ul><li>pre: <code>adapt_response(self, response)</code></li><li>process item: <code>parse_node(self, response, row)</code></li><li>post: <code>process_results(self, response, results)</code></li></ul></li></ol><h3 id="header-19">子类：SitemapSpider</h3><p><strong> Sample1：</strong></p><pre><code class="lang-python">from scrapy.spiders import SitemapSpiderclass MySpider(SitemapSpider):    sitemap_urls = [&#39;http://www.example.com/robots.txt&#39;]    sitemap_rules = [        (&#39;/shop/&#39;, &#39;parse_shop&#39;),    ]    sitemap_follow = [&#39;/sitemap_shops&#39;]    def sitemap_filter(self, entries):        for entry in entries:            date_time = datetime.strptime(entry[&#39;lastmod&#39;], &#39;%Y-%m-%d&#39;)            if date_time.year &gt;= 2005:                yield entry    def parse_shop(self, response):        pass # ... scrape shop here ...</code></pre><p><strong> Sample2: 马蜂窝 sitemapindex </strong></p><ol><li><p>View <code>http://www.mafengwo.cn/sitemapIndex.xml</code>:</p><pre><code class="lang-xml"> &lt;sitemapindex xmlns=&quot;http://www.sitemaps.org/schemas/sitemap/0.9&quot;&gt;     &lt;sitemap&gt;         &lt;loc&gt;http://www.mafengwo.cn/article-0.xml&lt;/loc&gt;         &lt;lastmod&gt;2019-03-15&lt;/lastmod&gt;     &lt;/sitemap&gt;     &lt;sitemap&gt;         &lt;loc&gt;http://www.mafengwo.cn/article-1.xml&lt;/loc&gt;         &lt;lastmod&gt;2019-03-15&lt;/lastmod&gt;     &lt;/sitemap&gt;     &lt;sitemap&gt;         &lt;loc&gt;http://www.mafengwo.cn/article-2.xml&lt;/loc&gt;         &lt;lastmod&gt;2019-03-15&lt;/lastmod&gt;     &lt;/sitemap&gt;     &lt;sitemap&gt;         &lt;loc&gt;http://www.mafengwo.cn/articleList-0.xml&lt;/loc&gt;         &lt;lastmod&gt;2019-03-15&lt;/lastmod&gt;     &lt;/sitemap&gt;     &lt;sitemap&gt;         &lt;loc&gt;http://www.mafengwo.cn/shop-0.xml&lt;/loc&gt;         &lt;lastmod&gt;2019-03-15&lt;/lastmod&gt;     &lt;/sitemap&gt; &lt;/sitemapindex&gt;</code></pre></li><li><p>View <code>http://www.mafengwo.cn/shop-0.xml</code>:</p><pre><code class="lang-xml"> &lt;urlset xmlns=&quot;http://www.sitemaps.org/schemas/sitemap/0.9&quot;&gt;     &lt;url&gt;         &lt;loc&gt;http://www.mafengwo.cn/v100029&lt;/loc&gt;         &lt;lastmod&gt;2019-07-03 02:51:02&lt;/lastmod&gt;         &lt;changefreq&gt;weekly&lt;/changefreq&gt;         &lt;priority&gt;0.7&lt;/priority&gt;     &lt;/url&gt;     &lt;url&gt;         &lt;loc&gt;http://www.mafengwo.cn/v100027&lt;/loc&gt;         &lt;lastmod&gt;2019-07-03 02:51:02&lt;/lastmod&gt;         &lt;changefreq&gt;weekly&lt;/changefreq&gt;         &lt;priority&gt;0.7&lt;/priority&gt;     &lt;/url&gt;     &lt;url&gt;         &lt;loc&gt;http://www.mafengwo.cn/shop/mdd.php?mddid=10186&lt;/loc&gt;         &lt;lastmod&gt;2019-07-03 02:51:02&lt;/lastmod&gt;         &lt;changefreq&gt;weekly&lt;/changefreq&gt;         &lt;priority&gt;0.7&lt;/priority&gt;     &lt;/url&gt;     &lt;url&gt;         &lt;loc&gt;http://www.mafengwo.cn/shop/mdd.php?mddid=10030&lt;/loc&gt;         &lt;lastmod&gt;2019-07-03 02:51:02&lt;/lastmod&gt;         &lt;changefreq&gt;weekly&lt;/changefreq&gt;         &lt;priority&gt;0.7&lt;/priority&gt;     &lt;/url&gt; &lt;/urlset&gt;</code></pre></li><li><p>Create Spider: <code>scrapy genspider siteUpdate mafengwo.cn</code></p><pre><code class="lang-python"> # -*- coding: utf-8 -*- from scrapy.spiders import SitemapSpider class SiteUpdateSpider(SitemapSpider):     name = &#39;siteUpdate&#39;     allowed_domains = [&#39;mafengwo.cn&#39;]     sitemap_urls=[&#39;http://www.mafengwo.cn/sitemapIndex.xml&#39;]     sitemap_rules=[         (r&#39;/v\d+&#39;,&#39;parse_shop&#39;)         # for parse web page(eg:html)     ]     sitemap_follow=[r&#39;/shop-\d+.xml&#39;]   # for scrapy deep sitemap loc     def sitemap_filter(self, entries):          for entry in entries:              # 1. entry: sitemap object(              # &lt;sitemap&gt;              #  &lt;loc&gt;http://www.mafengwo.cn/shop-0.xml&lt;/loc&gt;              #  &lt;lastmod&gt;2019-07-03&lt;/lastmod&gt;              # &lt;/sitemap&gt;              # )              # 2. entry: url object(              # &lt;url&gt;              #  &lt;loc&gt;http://www.mafengwo.cn/v100292&lt;/loc&gt;              #  &lt;lastmod&gt;2019-07-03 02:51:02&lt;/lastmod&gt;              #  &lt;changefreq&gt;weekly&lt;/changefreq&gt;              #  &lt;priority&gt;0.7&lt;/priority&gt;              # &lt;/url&gt;              # )              if entry[&#39;loc&#39;].find(&#39;.xml&#39;)!=-1 or entry[&#39;loc&#39;].find(&#39;mddid&#39;)==-1:                  # print(&quot;entry&quot;, entry)                  yield entry     def parse_shop(self,response):         # get response from detail web url page(not sitemap loc)         # eg: http://www.mafengwo.cn/v100292  (html)         if response.status==200:             item={}             item[&#39;title&#39;]=response.css(&#39;.t1&#39;).xpath(&#39;string(.)&#39;).get()             # 使用split()去除`\xa0`，即`&amp;nbsp`（编码原因变成了`\xa0`字符，`strip()`和`replace()`均无法有效去除该字符）             intro=&quot;&quot;.join(response.css(&#39;.address p&#39;).xpath(&#39;string(.)&#39;).getall()).split()             item[&#39;introduce&#39;]=&quot; &quot;.join(intro)             return item</code></pre></li><li><p>execute <code>scrapy crawl siteUpdate -o mafengwo.jl -s FEED_EXPORT_ENCODING=utf-8</code></p></li></ol><p><strong> Note: </strong></p><ol><li>属性：<ul><li><code>sitemap_urls = ()</code>: 可以指向robots.txt（会从中提取Sitemap网址）／sitemap网址</li><li><code>sitemap_rules = [(&#39;&#39;, &#39;parse&#39;)]</code>： A list of tuples <code>(regex, callback)</code>，regex用来匹配Sitemap中列出的网址</li><li><code>sitemap_follow = [&#39;&#39;]</code>：适用于SitemapIndex文件，符合这里设置的regex的sitemap会深入抓取（默认<code>&#39;&#39;</code>，即都会）</li><li><code>sitemap_alternate_links = False</code>：是否url使用列出的备用链接</li></ul></li><li>方法：<ul><li><code>start_requests(self)</code>: or use <code>sitemap_urls</code><ul><li><code>sitemap_filter(self,entries)</code>: entries get from the response body</li><li>if <code>sitemapindex</code> &amp; match <code>site_follow</code>: yield <code>Request</code> for <code>sitemap</code></li><li>if <code>urlset</code> &amp; match <code>site_rules</code>: yield <code>Request</code> for web page =&gt; callback func: <code>parse</code> =&gt; yield <code>Item</code>/<code>Request</code></li></ul></li></ul></li></ol><h2 id="header-20">Item</h2><ul><li>属性：<ul><li>fields={}</li></ul></li><li>方法：<ul><li>copy(item)</li><li>keys()</li><li>values()</li><li>items()</li><li>pop(key)</li><li>clear()</li><li>get(key,default)</li><li>setdefault(key,default)</li><li>update(…)</li><li>popitem(…)</li></ul></li></ul><p><strong> Sample: </strong></p><ol><li><p>自定义一个Item类：</p><pre><code class="lang-python"> import scrapy class Product(scrapy.Item):     # `class Field(dict)`,即dict的一个封装容器     name = scrapy.Field()     price = scrapy.Field()     stock = scrapy.Field()     last_updated = scrapy.Field(serializer=str)</code></pre></li><li><p>常用方法：</p><pre><code class="lang-python"> # 1. Create item &gt;&gt;&gt; product = Product({&#39;name&#39;: &#39;Laptop PC&#39;, &#39;price&#39;: 1500}) # Product(name=&#39;Laptop PC&#39;, price=1500) &gt;&gt;&gt; product = Product({&#39;name&#39;: &#39;Laptop PC&#39;, &#39;lala&#39;: 1500}) # KeyError: &#39;Product does not support field: lala&#39; &gt;&gt;&gt; product = Product(name=&#39;Desktop PC&#39;, price=1000) # Product(name=&#39;Desktop PC&#39;, price=1000) # Create Item from Item &gt;&gt;&gt; product2 = Product(product)     # Product(name=&#39;Desktop PC&#39;, price=1000) &gt;&gt;&gt; product3 = product2.copy()      # Product(name=&#39;Desktop PC&#39;, price=1000) # Creating dicts from items &gt;&gt;&gt; dict(product)                   # create a dict from all populated values {&#39;price&#39;: 1000, &#39;name&#39;: &#39;Desktop PC&#39;} # 2. Get field values &gt;&gt;&gt; product[&#39;name&#39;] Desktop PC &gt;&gt;&gt; product.get(&#39;name&#39;) Desktop PC &gt;&gt;&gt; product[&#39;last_updated&#39;] Traceback (most recent call last):     ... KeyError: &#39;last_updated&#39; &gt;&gt;&gt; product.get(&#39;last_updated&#39;, &#39;not set&#39;) not set # 3. Set field value &gt;&gt;&gt; product[&#39;name&#39;] = &#39;IPad&#39; &gt;&gt;&gt; product[&#39;name&#39;] IPad &gt;&gt;&gt; product[&#39;lala&#39;] = &#39;test&#39; # setting unknown field Traceback (most recent call last):     ... KeyError: &#39;Product does not support field: lala&#39; # 4. Access all populated values &gt;&gt;&gt; product.keys() [&#39;price&#39;, &#39;name&#39;] &gt;&gt;&gt; product.values() [&quot;1000&quot;,&quot;IPad&quot;] &gt;&gt;&gt; product.items() [(&#39;price&#39;, 1000), (&#39;name&#39;, &#39;IPad&#39;)] # 5. Check if has value &gt;&gt;&gt; &#39;name&#39; in product  # is name field populated? True &gt;&gt;&gt; &#39;last_updated&#39; in product  # is last_updated populated? False # 6. Check if has field &gt;&gt;&gt; &#39;last_updated&#39; in product.fields  # is last_updated a declared field? True &gt;&gt;&gt; &#39;lala&#39; in product.fields  # is lala a declared field? False</code></pre></li></ol><h3 id="header-21">ItemLoader</h3><p>Refer <a href="http://doc.scrapy.org/en/latest/topics/loaders.html" target="_blank" rel="noopener">Item Loaders</a></p><p>方便对数据进行格式化，填充Item（字段赋值）<br>（Item提供保存抓取到数据的容器，Itemloader提供的是填充容器的机制）</p><p><strong> Sample1: </strong></p><ol><li><p>Item：</p><pre><code class="lang-python"> class MovieItem(scrapy.Item):     id = scrapy.Field()     title = scrapy.Field()     rate = scrapy.Field()     url = scrapy.Field()     cover = scrapy.Field()     playable = scrapy.Field()     crawl_date = scrapy.Field()</code></pre></li><li><p>Spider parse：</p><pre><code class="lang-python"> # -*- coding: utf-8 -*- import scrapy from douban.items import MovieItem from scrapy.loader import ItemLoader from scrapy.loader.processors import TakeFirst, MapCompose, Join import datetime class HotMovieSpider(scrapy.Spider):     name = &#39;hotMovie&#39;     allowed_domains = [&#39;movie.douban.com&#39;]     start_urls = [&#39;http://movie.douban.com/&#39;]     def parse(self, response):         # loader=ItemLoader(item=MovieItem(),response=response)         movieSelectors = response.xpath(&quot;//*[@id=&#39;screening&#39;]//li[@data-title]&quot;)         for s in movieSelectors:             loader = ItemLoader(item=MovieItem(), selector=s)             loader.add_css(&#39;title&#39;, &#39;::attr(data-title)&#39;, TakeFirst(), MapCompose(str.strip))             loader.add_xpath(&#39;rate&#39;, &#39;./@data-rate&#39;, TakeFirst())             loader.add_xpath(&#39;url&#39;, &quot;.//li[@class=&#39;poster&#39;]/a/@href&quot;, TakeFirst())             loader.add_xpath(&#39;cover&#39;, &quot;.//li[@class=&#39;poster&#39;]//img/@src&quot;, TakeFirst())             loader.add_css(&#39;id&#39;, &quot;::attr(data-trailer)&quot;, TakeFirst(), re=r&#39;\d+&#39;)             loader.add_value(&#39;crawl_date&#39;, datetime.datetime.now())             yield loader.load_item()</code></pre></li><li><p>excute <code>scrapy crawl hotMovie -o movie.jl</code>, yield item sample:</p><pre><code class="lang-bash"> # 注：提取数据时不管使用何种processors，都是列表形式填充到Item {     &quot;title&quot;: [&quot;蜘蛛侠：英雄远征 Spider-Man: Far From Home&quot;],      &quot;rate&quot;: [&quot;8.0&quot;],      &quot;url&quot;: [&quot;https://movie.douban.com/subject/26931786/?from=showing&quot;],      &quot;cover&quot;: [&quot;https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2558293106.jpg&quot;],      &quot;id&quot;: [&quot;26931786&quot;],     &quot;crawl_date&quot;: [&quot;2019-06-30 11:43:19&quot;] }</code></pre></li><li><p>字段值去除列表形式，eg:</p><pre><code class="lang-bash"> # 注：提取数据时不管使用何种processors，都是列表形式填充到Item {     &quot;title&quot;: &quot;蜘蛛侠：英雄远征 Spider-Man: Far From Home&quot;,      &quot;rate&quot;: &quot;8.0&quot;],      &quot;url&quot;: &quot;https://movie.douban.com/subject/26931786/?from=showing&quot;,      &quot;cover&quot;: &quot;https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2558293106.jpg&quot;,      &quot;id&quot;: &quot;26931786&quot;,     &quot;crawl_date&quot;: &quot;2019-06-30 11:43:19&quot; }</code></pre><ul><li><p>方式一：修改<code>ItemLoader</code>的<code>default_output_processor</code>为<code>Join()</code>(或其他处理List的processor)</p><pre><code class="lang-python">  def parse(self,response):      # ...      # loader.add_value(&#39;crawl_date&#39;, datetime.datetime.now())      loader.add_value(&#39;crawl_date&#39;, str(datetime.datetime.now()))  # Join() only used for str List      loader.default_output_processor = Join()  # add for convert field value List to String      yield loader.load_item()</code></pre></li><li>方式二：配置<code>Item</code>的<code>Field(output_processor=xxxx)</code> (优先级高于ItemLoader中的default)<pre><code class="lang-python">  from scrapy.loader.processors import Join  class MovieItem(scrapy.Item):      # ...      title = scrapy.Field(output_processor=Join())</code></pre></li></ul></li></ol><p><strong> Note: </strong></p><ul><li>属性：<ul><li><code>item</code></li><li><code>selector</code></li><li><code>context</code></li><li><code>parent</code></li><li><code>default_item_class = Item</code></li><li><code>default_input_processor = Identity()</code></li><li><code>default_output_processor = Identity()</code></li><li><code>default_selector_class = Selector</code></li></ul></li><li>方法：<ul><li><code>__init__(self, item=None, selector=None, response=None, parent=None, **context)</code></li><li><code>add_value/replace_value(field_name,value,*processors,**kw)</code>, <code>get_value(value, *processors, **kw)</code></li><li><code>add_xpath/replace_xpath(field_name, xpath, *processors, **kw)</code>, <code>get_xpath(xpath, *processors, **kw)</code></li><li><code>add_css/replace_css(field_name, css, *processors, **kw)</code>, <code>get_css(css, *processors, **kw)</code></li><li><code>nested_xpath(xpath,**context)</code>, <code>nested_css(css,**context)</code> </li><li><code>get_input_processor/get_output_processor(field_name)</code></li><li><code>get_output_value/get_collected_values(field_name)</code></li><li><code>load_item()</code></li></ul></li><li><p>processors:</p><ul><li>内置的processor<ul><li><code>Identity</code>: 不进行任何处理，直接返回原来的数据</li><li><code>TakeFirst</code>: 返回第一个非空值</li><li><code>Join</code>: 返回用分隔符连接后的值（默认是使用空格连接）</li><li><code>Compose</code>： 返回多个函数组合处理后的数据（默认遇到None值时停止处理，可传入<code>stop_on_none = False</code>修改）</li><li><code>MapCompose</code>：与<code>Compose</code>类似，只是输入值是被迭代的处理传入各个函数</li><li><code>SelectJmes</code>：使用jsonpath,返回json对象某个field值（Requires <code>jmespath</code>） </li></ul></li><li>可直接使用内置的processors，也可使用自定义</li><li><p>在<code>Compose／MapCompose</code>中的函数也可使用lambda表达式，更简便</p><pre><code class="lang-python">  &gt;&gt;&gt; from scrapy.loader.processors import MapCompose  &gt;&gt;&gt; def filter_world(x):  ...     return None if x == &#39;world&#39; else x  &gt;&gt;&gt; proc = MapCompose(filter_world, str.upper)  &gt;&gt;&gt; proc([&#39;hello&#39;, &#39;world&#39;, &#39;this&#39;, &#39;is&#39;, &#39;scrapy&#39;])  &gt;&gt;&gt; [&#39;HELLO&#39;, &#39;THIS&#39;, &#39;IS&#39;, &#39;SCRAPY&#39;]  &gt;&gt;&gt; proc2 = MapCompose(lambda i : i.replace(&#39;=&#39;,&#39;:&#39;),str.strip)  &gt;&gt;&gt; proc2([&#39;a=1&#39;,&#39; b = Tom &#39;,&#39;c:OK&#39;])  [&#39;a:1&#39;, &#39;b : Tom&#39;, &#39;c:OK&#39;]</code></pre></li><li>使用：<ul><li>ItemLoader <code>add/replace/get_value/xpath/css(...)</code>提取数据时传入</li><li>ItemLoader <code>default_input/output_processor</code>配置</li><li>Item Field <code>input/output_processor</code>配置      </li></ul></li></ul></li><li>步骤：    <ul><li><code>add_value/add_xpath/add_css(...)</code>提取数据<ul><li>可传入processors处理提取的数据</li><li>可传入参数<code>re=&#39;regex&#39;</code>正则表达式来过滤匹配值</li><li>默认提取的数据，填充进去的对象都是List类型(即每个字段存储的都是List类型)</li></ul></li><li>=&gt; <code>input_processor</code> &amp; <code>output_processor</code>数据预处理（填充到Item的Field前的处理）<ul><li>优先使用Item对象的Field字段配置的<code>input/output_processor</code>，未配置则使用ItemLoader中的<code>default_input／output_processor</code></li><li>Item Loader中的<code>default_input／output_processor</code>默认都是<code>Identity</code>，即维持原样，不处理</li></ul></li><li>=&gt; <code>load_item()</code>填充到Item对象</li></ul></li></ul><h2 id="header-22">Middleware</h2><ol><li><p><code>MiddlewareManager</code>:</p><ul><li>方法：<ul><li>from_settings(cls, settings, crawler=None)</li><li>from_crawler(cls, crawler)</li><li>open_spider(self, spider)</li><li>close_spider(self, spider)</li></ul></li><li>子类：<ul><li><code>ItemPipelineManager</code>: item pipeline<ul><li>process_item(self, item, spider)</li></ul></li><li><code>SpiderMiddlewareManager</code>: spider middleware<ul><li>scrape_response(self, scrape_func, response, request, spider)<ul><li>process_spider_input(response)</li><li>process_spider_exception(_failure)</li><li>process_spider_output(result)</li></ul></li><li>process_start_requests(self, start_requests, spider)</li></ul></li><li><code>DownloaderMiddlewareManager</code>: downloader middleware<ul><li>download(self, download_func, request, spider)<ul><li>process_request(request)</li><li>process_response(response)</li><li>process_exception(_failure)</li><li>Note: @defer.inlineCallbacks</li></ul></li></ul></li><li><code>ExtensionManager</code>: extension</li></ul></li></ul></li><li><p>Project Middleware:</p><ul><li>ItemPipeline: <code>pipelines.py</code><ul><li><code>open_spider(self, spider)</code></li><li><code>process_item(self, item, spider)</code></li><li><code>close_spider(self, spider)</code></li></ul></li><li>SpiderMiddleware: <code>middleware.py</code><ul><li><code>spider_opened(self, spider)</code></li><li><code>process_start_requests(self, start_requests, spider)</code><ul><li>yied Request (no items)</li></ul></li><li><code>process_spider_input(self, response, spider)</code><ul><li>return None</li><li>or raise Exception</li></ul></li><li><code>process_spider_output(self, response, result, spider)</code><ul><li>yield Request/Item</li></ul></li><li><code>process_spider_exception(self, response, exception, spider)</code></li></ul></li><li>DownloaderMiddleware: <code>middleware.py</code>    <ul><li>spider_opened(self, spider)</li><li>process_request(self, request, spider): <ul><li>return None(continue processing)/Response/Request</li><li>or raise IgnoreRequest =&gt; call downloader middleware <code>process_exception()</code></li></ul></li><li>process_response(self, request, response, spider)<ul><li>return Response/Request</li><li>or raise IgnoreRequest</li></ul></li><li>process_exception(self, request, exception, spider)<ul><li>return None/Response/Request</li><li>Note: return None will continue processing this exception, return Response/Request stops process_exception() chain</li></ul></li></ul></li></ul></li><li><p><code>scrapy/settings/default_settings.py</code>:</p><pre><code class="lang-python"> ITEM_PIPELINES_BASE = {} SPIDER_MIDDLEWARES_BASE = {     # Engine side     &#39;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&#39;: 50,     &#39;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&#39;: 500,     &#39;scrapy.spidermiddlewares.referer.RefererMiddleware&#39;: 700,     &#39;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&#39;: 800,     &#39;scrapy.spidermiddlewares.depth.DepthMiddleware&#39;: 900,     # Spider side } DOWNLOADER_MIDDLEWARES_BASE = {     # Engine side     &#39;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&#39;: 100,     &#39;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&#39;: 300,     &#39;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&#39;: 350,     &#39;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&#39;: 400,     &#39;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#39;: 500,     &#39;scrapy.downloadermiddlewares.retry.RetryMiddleware&#39;: 550,     &#39;scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware&#39;: 560,     &#39;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&#39;: 580,     &#39;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#39;: 590,     &#39;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&#39;: 600,     &#39;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&#39;: 700,     &#39;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&#39;: 750,     &#39;scrapy.downloadermiddlewares.stats.DownloaderStats&#39;: 850,     &#39;scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware&#39;: 900,     # Downloader side } EXTENSIONS_BASE = {     &#39;scrapy.extensions.corestats.CoreStats&#39;: 0,     &#39;scrapy.extensions.telnet.TelnetConsole&#39;: 0,     &#39;scrapy.extensions.memusage.MemoryUsage&#39;: 0,     &#39;scrapy.extensions.memdebug.MemoryDebugger&#39;: 0,     &#39;scrapy.extensions.closespider.CloseSpider&#39;: 0,     &#39;scrapy.extensions.feedexport.FeedExporter&#39;: 0,     &#39;scrapy.extensions.logstats.LogStats&#39;: 0,     &#39;scrapy.extensions.spiderstate.SpiderState&#39;: 0,     &#39;scrapy.extensions.throttle.AutoThrottle&#39;: 0, }</code></pre></li><li><p>project <code>settings.py</code>：</p><pre><code class="lang-python"> ITEM_PIPELINES = {     #&#39;douban.pipelines.DoubanPipeline&#39;: 300, } SPIDER_MIDDLEWARES = {     #&#39;douban.middlewares.DoubanSpiderMiddleware&#39;: 543, } DOWNLOADER_MIDDLEWARES = {     #&#39;douban.middlewares.DoubanDownloaderMiddleware&#39;: 543, } EXTENSIONS = {     #&#39;scrapy.extensions.telnet.TelnetConsole&#39;: None, }</code></pre></li></ol><h3 id="header-23">Item Pipeline</h3><ul><li>project下pipeline.py文件</li><li>不需要继承特定的基类，只需要实现特定的方法：<ul><li><code>open_spider</code>:爬虫运行前执行的操作</li><li><code>process_item</code>：爬虫获取到的每项item数据的处理方法</li><li><code>close_spider</code>:爬虫运行结束时执行的操作</li><li><code>from_crawler</code>：pipeline类方法，是创建item pipeline的回调方法，通常该方法用于读取setting中的配置参数</li><li>注：其中<code>process_item</code>必须实现</li></ul></li></ul><p><strong> Sample: </strong></p><ol><li><p>Duplicates filter</p><pre><code class="lang-python"> from scrapy.exceptions import DropItem class DuplicatesPipeline(object):     def __init__(self):         self.ids_seen = set()     def process_item(self, item, spider):         if item[&#39;id&#39;] in self.ids_seen:             raise DropItem(&quot;Duplicate item found: %s&quot; % item)         else:             self.ids_seen.add(item[&#39;id&#39;])             return item</code></pre></li><li><p>Take screenshot of item</p><pre><code class="lang-python"> # 从方法返回Deferred process_item() # Pipeline请求本地运行的Splash实例，获取项目网址的屏幕截图 # 在Deferred回调函数中保存截图，yield Item import scrapy import hashlib from urllib.parse import quote class ScreenshotPipeline(object):     &quot;&quot;&quot;Pipeline that uses Splash to render screenshot of     every Scrapy item.&quot;&quot;&quot;     SPLASH_URL = &quot;http://localhost:8050/render.png?url={}&quot;     def process_item(self, item, spider):         encoded_item_url = quote(item[&quot;url&quot;])         screenshot_url = self.SPLASH_URL.format(encoded_item_url)         request = scrapy.Request(screenshot_url)         dfd = spider.crawler.engine.download(request, spider)         dfd.addBoth(self.return_item, item)         return dfd     def return_item(self, response, item):         if response.status != 200:             # Error happened, return item.             return item         # Save screenshot to file, filename will be hash of url.         url = item[&quot;url&quot;]         url_hash = hashlib.md5(url.encode(&quot;utf8&quot;)).hexdigest()         filename = &quot;{}.png&quot;.format(url_hash)         with open(filename, &quot;wb&quot;) as f:             f.write(response.body)         # Store filename in item.         item[&quot;screenshot_filename&quot;] = filename         return item</code></pre></li><li><p>Write items to file</p><pre><code class="lang-python"> class ItemFilePipeline(object):     def __init__(self):         self.filepath=settings[&#39;ITEM_STORE&#39;]     def open_spider(self,spider):         filename=os.path.join(self.filepath,spider.name+&#39;.json&#39;)         self.file=open(filename,&#39;w&#39;,encoding=&#39;utf-8&#39;)         self.file.write(&#39;[\n&#39;)     def process_item(self,item,spider):         #print(item)         record=json.dumps(dict(item),ensure_ascii=False)         #print(record)         self.file.write(record+&quot;,\n&quot;)         return item     def close_spider(self,spider):         self.file.write(&#39;]\n&#39;)         self.file.close()</code></pre></li><li><p>Write items to MongoDB</p><pre><code class="lang-python"> class MongoPipeline(object):     def __init__(self):         self.mongo_uri=settings[&#39;MONGO_CONN_STR&#39;]         self.mongo_db=settings.get(&#39;MONGO_DB&#39;,&#39;scrapy&#39;)     def process_item(self, item, spider):         record=dict(item)         record[&#39;_id&#39;]=record[&#39;id&#39;]         record.pop(&#39;id&#39;)         result=self.db[spider.name].update_one({&#39;_id&#39;:record[&#39;_id&#39;]},{&#39;$set&#39;:record},upsert=True)         # print(result.raw_result)         return item     def open_spider(self,spider):         self.client = pymongo.MongoClient(self.mongo_uri)         self.db = self.client[self.mongo_db]         #print(self.client.list_database_names())         #print(self.db.list_collection_names())     def close_spider(self,spider):         self.client.close()</code></pre></li></ol><h3 id="header-24">Media Pipeline</h3><ol><li><p>FilesPipeline（继承自MediaPipeline）</p><ul><li><code>DEFAULT_FILES_URLS_FIELD = &#39;file_urls&#39;</code><ul><li>默认从Item的<code>file_urls=[]</code>（在Spider中抓取填充该字段）获取文件的URLs，下载文件</li><li>可在settings配置<code>FILES_URLS_FIELD</code>另外指定</li><li>也可在<code>get_media_requests(...)</code>中动态修改</li></ul></li><li><code>DEFAULT_FILES_RESULT_FIELD＝&#39;files&#39;</code><ul><li>默认将下载的文件信息存储到Item的<code>files={}</code>字段,包括<code>url</code>,<code>path</code>,<code>checksum</code></li><li>可在settings配置<code>FILES_RESULT_FIELD</code>另外指定</li><li>也可在<code>item_completed(...)</code>中动态修改</li></ul></li><li><code>EXPIRES=90</code><ul><li>文件多少天后过期（避免重复下载最近的文件）, 默认设置为90天后文件过期</li><li>可在settings配置<code>FILES_EXPIRES</code>另外指定</li></ul></li><li>settings中配置：<code>FILES_STORE</code> 指定文件存储位置文件系统(或者亚马逊S3）<ul><li>文件存储：<code>&lt;FILES_STORE&gt;/file_path(...)</code></li></ul></li><li>Overridable methods:<ul><li><code>get_media_requests(self, item, info)</code><ul><li>return Request (get from file_urls field)</li></ul></li><li><code>file_downloaded(self, response, request, info)</code><ul><li>persist_file</li><li>return checksum</li></ul></li><li><code>item_completed(self, results, item, info)</code><ul><li>store the files information which downloaded successfully into Item field</li><li>return item</li><li>eg:<pre><code class="lang-python">  # results＝(True, {dict}：  [(True,    {&#39;checksum&#39;: &#39;2b00042f7481c7b056c4b410d28f33cf&#39;,     &#39;path&#39;: &#39;full/0a79c461a4062ac383dc4fade7bc09f1384a3910.jpg&#39;,     &#39;url&#39;: &#39;http://www.example.com/files/product1.pdf&#39;}),   (False,    Failure(...))]  # store the successful files information to Item files field  # item[&#39;files&#39;]:  [{&#39;checksum&#39;: &#39;2b00042f7481c7b056c4b410d28f33cf&#39;,     &#39;path&#39;: &#39;full/0a79c461a4062ac383dc4fade7bc09f1384a3910.jpg&#39;,     &#39;url&#39;: &#39;http://www.example.com/files/product1.pdf&#39;}]</code></pre></li></ul></li><li><code>file_path(self, request, response=None, info=None)</code><ul><li>return path </li><li><code>&#39;full/%s%s&#39; % (media_guid, media_ext)</code></li><li>eg: <code>full/0a79c461a4062ac383dc4fade7bc09f1384a3910.jpg</code></li></ul></li></ul></li></ul></li><li><p>ImagesPipeline（继承自FilesPipeline）</p><ul><li><code>DEFAULT_IMAGES_URLS_FIELD=&#39;image_urls&#39;</code><ul><li>默认从Item的<code>image_urls=[]</code>（在Spider中抓取填充该字段）获取Image的URLs，下载Image</li><li>可在settings配置<code>IMAGES_URLS_FIELD</code>另外指定</li></ul></li><li><code>DEFAULT_IMAGES_RESULT_FIELD = &#39;images&#39;</code><ul><li>默认将下载的文件信息存储到Item的<code>images={}</code>字段,包括<code>url</code>,<code>path</code>,<code>checksum</code></li><li>可在settings配置<code>IMAGES_RESULT_FIELD</code>另外指定</li></ul></li><li><code>EXPIRES = 90</code><ul><li>文件多少天后过期（避免重复下载最近的文件）, 默认设置为90天后文件过期</li><li>可在settings配置<code>IMAGES_EXPIRES</code>另外指定</li></ul></li><li><code>MIN_WIDTH = 0</code>,<code>MIN_HEIGHT = 0</code><ul><li>Filtering out small images 只下载大于某长宽的图片</li><li>settings: <code>IMAGES_MIN_WIDTH</code>,<code>IMAGES_MIN_HEIGHT</code></li></ul></li><li><code>THUMBS = {}</code><ul><li>配置缩略图，默认无</li><li>可在settings中配置<code>IMAGES_THUMBS={size_name:(x,y),...}</code>另外指定，eg: <pre><code class="lang-python">  IMAGES_THUMBS = {      &#39;small&#39;: (50, 50),      &#39;big&#39;: (270, 270),  }  # 则下载的Images存储路径为（checksum相同）：  # &lt;IMAGES_STORE&gt;/full/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg  # &lt;IMAGES_STORE&gt;/thumbs/small/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg  # &lt;IMAGES_STORE&gt;/thumbs/big/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg</code></pre></li></ul></li><li>settings中配置： <code>IMAGES_STORE</code><ul><li>Image 原图存储：<code>&lt;IMAGES_STORE&gt;/file_path(...)</code></li><li>Image 缩略图存储：<code>&lt;IMAGES_STORE&gt;/thumb_path(...)</code></li></ul></li><li>methods:<ul><li>convert_image(self, image, size=None)<ul><li>将图片转换成常见格式（JPG）和模式（RGB）</li><li>修正Image大小(生成缩略图)</li></ul></li><li>get_images(self, response, request, info)<ul><li>调用convert_image转换image</li><li>yield path, image, buf （若配置了thumbs，也会yield对应转换后的images）</li></ul></li><li>get_media_requests(self, item, info)<ul><li>同FilesPipeline</li></ul></li><li>file_downloaded/image_downloaded(self, response, request, info)<ul><li>同FilesPipeline</li></ul></li><li>item_completed(self, results, item, info)<ul><li>同FilesPipeline</li></ul></li><li>file_path(self, request, response=None, info=None)<ul><li>return path</li><li><code>&#39;full/%s.jpg&#39; % (image_guid)</code></li></ul></li><li>thumb_path(self, request, thumb_id, response=None, info=None)<ul><li>return <code>&#39;thumbs/%s/%s.jpg&#39; % (thumb_id, thumb_guid)</code></li></ul></li></ul></li></ul></li><li><p>Note: 生效需要settings中配置<code>ITEM_PIPELINES = { ... }</code>      </p></li></ol><p><strong> Sample : Douban Top 250 Item and Cover(image) download </strong></p><ul><li>Store items to file</li><li>Download item’s cover (image)</li></ul><ol><li><p>item.py</p><pre><code class="lang-python"> class CoverItem(scrapy.Item):     name=scrapy.Field()     url=scrapy.Field()     path=scrapy.Field()     #images=scrapy.Field()     checksum=scrapy.Field()</code></pre></li><li><p>spider/cover.py</p><pre><code class="lang-python"> # -*- coding: utf-8 -*- import scrapy from scrapy.linkextractors import LinkExtractor from scrapy.spiders import CrawlSpider, Rule from douban_demo.items import CoverItem from scrapy.exceptions import DropItem class CoverSpider(CrawlSpider):     name = &#39;cover&#39;     allowed_domains = [&#39;movie.douban.com&#39;]     start_urls = [&#39;https://movie.douban.com/top250/&#39;]     rules = (         Rule(LinkExtractor(allow=r&#39;\?start=\d+.*&#39;,restrict_xpaths=&#39;//div[@class=&quot;paginator&quot;]&#39;), callback=&#39;parse_item&#39;, follow=True),     )     def parse_item(self, response):         print(response.url)         records=response.xpath(&#39;//ol[@class=&quot;grid_view&quot;]//div[@class=&quot;item&quot;]/div[@class=&quot;pic&quot;]//img&#39;)         for r in records:             item=CoverItem()             item[&#39;name&#39;]=r.xpath(&#39;./@alt&#39;).get()             item[&#39;url&#39;]=r.xpath(&#39;./@src&#39;).get()             print(item[&#39;url&#39;])             yield item</code></pre></li><li><p>pipeline.py</p><pre><code class="lang-python"> import scrapy from scrapy.pipelines.images import ImagesPipeline class CoverImagePipeline(ImagesPipeline):     def get_media_requests(self,item,info):         ext=item[&#39;url&#39;].split(&#39;.&#39;)[-1]         yield scrapy.Request(item[&#39;url&#39;],meta={&#39;image_name&#39;:item[&#39;name&#39;]+&quot;.&quot;+ext})     def item_completed(self,results, item, info):         #item[&#39;images&#39;]=results         r = [(x[&#39;path&#39;],x[&#39;checksum&#39;]) for ok, x in results if ok]         if not r:             raise DropItem(&quot;Item contains no images&quot;)         item[&#39;path&#39;] = r[0][0]         item[&#39;checksum&#39;]=r[0][1]         return item     def file_path(self,request,response=None,info=None):         return &#39;full/%s&#39; % request.meta[&#39;image_name&#39;] class CoverItemPipeline(object):     def __init__(self):         self.filepath=settings[&#39;COVER_FILE&#39;]     def open_spider(self,spider):         self.file=open(self.filepath,&#39;w&#39;,encoding=&#39;utf-8&#39;)     def process_item(self,item,spider):         #print(item)         record=json.dumps(dict(item),ensure_ascii=False)         #print(record)         self.file.write(record+&quot;\n&quot;)         return item     def close_spider(self,spider):         self.file.close()</code></pre></li><li><p>setting.py</p><pre><code class="lang-python"> COVER_FILE=&#39;D:\Space\python\images\cover.txt&#39; ITEM_PIPELINES = {     #&#39;douban_demo.pipelines.DoubanDemoPipeline&#39;: 300,     &#39;douban_demo.pipelines.CoverImagePipeline&#39;:310,     &#39;douban_demo.pipelines.CoverItemPipeline&#39;:320 } IMAGES_STORE=&#39;D:\Space\python\images&#39; IMAGES_EXPIRES = 30 IMAGES_THUMBS = {     &#39;small&#39;: (50, 50),     &#39;big&#39;: (250, 250), }</code></pre></li><li><p>run.py</p><pre><code class="lang-python"> from scrapy import cmdline cmdline.execute(&quot;scrapy crawl cover -s LOG_ENABLED=False&quot;.split()) #from scrapy.cmdline import execute #execute([&#39;scrapy&#39;, &#39;crawl&#39;, &#39;cover&#39;])</code></pre></li><li><p>execute</p><pre><code class="lang-bash"> scrapy crawl cover # or execute the run.py: # python run</code></pre></li><li><p>sample item：</p><pre><code class="lang-python">{  &quot;_id&quot; : &quot;1900841&quot;,  &quot;quote&quot; : &quot;别样人生。&quot;,  &quot;rate&quot; : &quot;9.1&quot;,  &quot;title&quot; : &quot;窃听风暴&quot;,  &quot;cover&quot; : {      &quot;name&quot; : &quot;窃听风暴&quot;,      &quot;url&quot; : &quot;https://img1.doubanio.com/view/photo/s_ratio_poster/public/p1808872109.jpg&quot;,      &quot;path&quot; : &quot;full/窃听风暴.jpg&quot;,      &quot;checksum&quot; : &quot;c7ac16a9361d57718543ccea182543a9&quot;  } }</code></pre></li></ol><h3 id="header-25">Item Exporters</h3><p>数据导出器(Exporter)：</p><ul><li><p>scrapy内置了6中数据导出格式：<code>json</code>,<code>json lines</code>,<code>CSV</code>,<code>xml</code>,<code>pickle</code>,<code>marshal</code></p><pre><code class="lang-python">  FEED_EXPORTERS = {}  FEED_EXPORTERS_BASE = {      &#39;json&#39;: &#39;scrapy.exporters.JsonItemExporter&#39;,      &#39;jsonlines&#39;: &#39;scrapy.exporters.JsonLinesItemExporter&#39;,      &#39;jl&#39;: &#39;scrapy.exporters.JsonLinesItemExporter&#39;,      &#39;csv&#39;: &#39;scrapy.exporters.CsvItemExporter&#39;,      &#39;xml&#39;: &#39;scrapy.exporters.XmlItemExporter&#39;,      &#39;pickle&#39;: &#39;scrapy.exporters.PickleItemExporter&#39;,      &#39;marshal&#39;: &#39;scrapy.exporters.MarshalItemExporter&#39;,  }</code></pre></li><li><p>其他相关配置</p><pre><code class="lang-python">  FEED_TEMPDIR = None  FEED_URI = None             # 导出文件路径,eg: &#39;export_data\%(name)s.data&#39;（name自动替换成spider的name）  FEED_URI_PARAMS = None      # a function to extend uri arguments  FEED_FORMAT = &#39;jsonlines&#39;   # 导出文件的格式,即默认导出器类型，eg: &#39;csv&#39;  FEED_STORE_EMPTY = False  FEED_EXPORT_ENCODING = None # 导出文件的编码格式  FEED_EXPORT_FIELDS = None   # 默认导出全部字段,对字段进行排序,eg [&#39;name&#39;,&#39;author&#39;,&#39;price&#39;]  FEED_STORAGES = {}  FEED_STORAGES_BASE = {      &#39;&#39;: &#39;scrapy.extensions.feedexport.FileFeedStorage&#39;,      &#39;file&#39;: &#39;scrapy.extensions.feedexport.FileFeedStorage&#39;,      &#39;stdout&#39;: &#39;scrapy.extensions.feedexport.StdoutFeedStorage&#39;,      &#39;s3&#39;: &#39;scrapy.extensions.feedexport.S3FeedStorage&#39;,      &#39;ftp&#39;: &#39;scrapy.extensions.feedexport.FTPFeedStorage&#39;,  }  FEED_EXPORT_INDENT = 0</code></pre></li><li><p>基类：<code>BaseItemExporter</code></p><ul><li><code>serialize_field(self, field, name, value)</code></li><li><code>start_exporting(self)</code>：导出开始时被调用，用于初始化（类似pipelines的open_spider）</li><li><code>export_item(self, item)</code>：用于处理每项数据（类似pipelines的process_item），必须实现(默认raise NotImplementedError)</li><li><code>finish_exporting(self)</code>：导出完成后调用，用于收尾工作（类似pipelines的close_spider）</li></ul></li><li><p>子类：    </p><ul><li>JsonLinesItemExporter</li><li>JsonItemExporter</li><li>XmlItemExporter</li><li>CsvItemExporter</li><li>PickleItemExporter</li><li>MarshalItemExporter</li><li>PprintItemExporter</li><li>PythonItemExporter</li><li>JsonItemExporter Vs. JsonLinesItemExporter<ul><li>JsonItemExporter: 每次把数据添加到内存中，最后统一写入到磁盘文件中(耗内存),整个文件(<code>.json</code>)：<code>[{},{},...]</code></li><li>JsonLinesItemExporter：每次调用export_item的时候就把item存储到磁盘中（即一个字典一行，不耗内存），整个文件(<code>.jl</code>)：<code>{},{}...</code> (不是一个满足json格式的文件)</li></ul></li></ul></li><li>过程：<ul><li>运行爬虫（<code>scrapy crawl</code>）时指定<code>-o filepath</code> 导出到文件，则会使用Exporter</li><li>导出器类型：根据保存文件后缀确定，若指定<code>-t format</code>，则使用指定的</li><li>确定导出器类型后，再从settings中查找对应导出器进行导出</li><li>eg:<pre><code class="lang-bash">  $ scrapy crawl movie -o text.json  $ scrapy crawl movie -t json -o test.json</code></pre></li></ul></li></ul><h2 id="header-26">应用</h2><h3 id="header-27">基于Excel爬取</h3><pre><code class="lang-python"># -*- coding: utf-8 -*-import scrapyimport csvfrom scrapy.item import Item, Fieldfrom scrapy.loader import ItemLoaderfrom scrapy.loader.processors import Identity, Join, MapComposeimport datetimeclass MoviecsvSpider(scrapy.Spider):    name = &#39;movieCsv&#39;    allowed_domains = [&#39;movie.douban.com&#39;]    # start_urls = [&#39;http://movie.douban.com/&#39;]    def start_requests(self):        with open(&quot;movie.csv&quot;, &#39;rU&#39;) as f:            reader = csv.DictReader(f)            for line in reader:                print(line)                # OrderedDict([(&#39;src_url&#39;, &#39;http://movie.douban.com/&#39;),                # (&#39;src_selector&#39;, &#39;#screening li[data-title]&#39;),                # (&#39;title&#39;, &#39;::attr(data-title)&#39;),                # (&#39;rate&#39;, &#39;::attr(data-rate)&#39;),                # (&#39;url&#39;, &#39;li.poster&gt;a::attr(href)&#39;),                # (&#39;cover&#39;, &#39;li.poster img::attr(src)&#39;),                # (&#39;id&#39;, &#39;::attr(data-trailer)&#39;)]                yield scrapy.Request(url=line.pop(&#39;src_url&#39;), callback=self.parse, meta={&#39;rule&#39;: line})    def parse(self, response):        line = response.meta[&#39;rule&#39;]        src_selector = response.css(line.pop(&#39;src_selector&#39;))        for s in src_selector:            item = Item()            loader = ItemLoader(item=item, selector=s)            for name, exp in line.items():                if exp:                    item.fields[name] = Field()                    loader.add_css(name, exp)            item.fields[&#39;crawl_date&#39;] = Field() # Field(output_processor=Identity())            loader.add_value(&#39;crawl_date&#39;, datetime.datetime.now(), str)            loader.default_output_processor = Join()            yield loader.load_item()</code></pre><pre><code class="lang-bash">$ cat movie.csvsrc_url,src_selector,title,rate,url,cover,idhttp://movie.douban.com/,#screening li[data-title],::attr(data-title),::attr(data-rate),li.poster&gt;a::attr(href),li.poster img::attr(src),::attr(data-trailer)</code></pre><p>execute <code>scrapy crawl movieCsv -o movie.jl</code>, result sample:</p><pre><code class="lang-bash">{    &quot;title&quot;: &quot;监护风云 Jusqu’à la Garde&quot;,     &quot;rate&quot;: &quot;7.3&quot;,     &quot;url&quot;: &quot;https://movie.douban.com/subject/26995532/?from=showing&quot;,     &quot;cover&quot;: &quot;https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2560052923.jpg&quot;,     &quot;id&quot;: &quot;https://movie.douban.com/subject/26995532/trailer&quot;,     &quot;crawl_date&quot;: &quot;2019-06-30 16:21:15.310231&quot;}</code></pre><h3 id="header-28">Login</h3><ul><li>Method1: spider start_requests: post form request to login first</li><li>Method2: login in web page, then copy the cookie to settings <code>DEFAULT_REQUEST_HEADERS={&#39;Cookie&#39;:&#39;xx=xxx...&#39;}</code></li></ul><p><strong> Sample: post form request to login in spider </strong></p><pre><code class="lang-python">class LessonSpider(scrapy.Spider):    name = &#39;lesson&#39;    allowed_domains = [&#39;class.121talk.cn&#39;]    start_urls=[&#39;https://class.121talk.cn/business/Index&#39;]    login_url=&#39;https://class.121talk.cn/business/Index/login&#39;    course_url=&#39;https://class.121talk.cn/business/Teachers/detail/id/3313&#39;    def __init__(self,username=None,password=None,*args, **kwargs):        super(LessonSpider, self).__init__(*args, **kwargs)        if username is None or password is None:            raise Exception(&#39;No username or password to login&#39;)        self.username=username        self.password=password    # login - method1:    def start_requests(self):        print(&#39;start_request&#39;)        yield scrapy.FormRequest(self.login_url            ,formdata={&#39;username&#39;:self.username,&#39;password&#39;:self.password}            ,callback=self.after_login)    # login - method2:    # def parse(self, response):    #     yield scrapy.FormRequest.from_response(response    #         ,url=self.login_url    #         ,formdata={&#39;username&#39;:self.username,&#39;password&#39;:self.password}    #         #,meta={&#39;cookiejar&#39;:1}    #         ,callback=self.after_login)    def after_login(self,response):        print(&#39;after_login&#39;)        print(&#39;login:&#39;,response)        print(&#39;login headers:&#39;,response.headers)        print(&#39;login cookie:&#39;,response.request.headers.getlist(&#39;Cookie&#39;))        print(&#39;login Set-Cookie:&#39;,response.headers.getlist(&#39;Set-Cookie&#39;))        result=json.loads(response.body)        print(&quot;login result:&quot;,result)        if result.get(&#39;status&#39;):            yield scrapy.Request(self.course_url            #,meta={&#39;cookiejar&#39;:response.meta[&#39;cookiejar&#39;]}            ,callback=self.parse_course)</code></pre><p>run:</p><pre><code class="lang-bash">$ scrapy crawl lesson -a username=xxxx -a password=xxx</code></pre><h3 id="header-29">常见问题</h3><ol><li><p>ImportError: No module named win32api.</p><pre><code class="lang-bash"> pip install pypiwin32</code></pre></li><li><p>AttributeError: ‘TelnetConsole’ object has no attribute ‘port’</p><pre><code> set TELNETCONSOLE_PORT setting to None (instead of default [6023, 6073]).  If that doesn&#39;t work and if you don&#39;t need the telnet console, simply disable the extension altogether with setting `TELNETCONSOLE_ENABLED=False`</code></pre></li><li><p>AttributeError: ‘module’ object has no attribute ‘F_GETFD’ </p><pre><code> 找到python3/Lib 中将fcntl.py改名成fcntl_ex.py再运行</code></pre></li><li><p>403 forbidden :  <a href="https://www.jianshu.com/p/31c7426c0da8" target="_blank" rel="noopener">https://www.jianshu.com/p/31c7426c0da8</a></p><pre><code class="lang-python"> # setting.py: set &#39;User-Agent&#39; #  method1: Override the default request headers: DEFAULT_REQUEST_HEADERS = {     &#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:60.0) Gecko/20100101 Firefox/60.0&#39;    #&#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,    #&#39;Accept-Language&#39;: &#39;en&#39;, } #  method2: USER_AGENT=&#39;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:60.0) Gecko/20100101 Firefox/60.0&#39;</code></pre></li><li><p>proxy:  <a href="https://blog.csdn.net/wuchenlhy/article/details/80683829" target="_blank" rel="noopener">https://blog.csdn.net/wuchenlhy/article/details/80683829</a></p><ul><li><p>method1: middlewares.py:</p><pre><code class="lang-python">  # downloader middleware:  class ProxyMiddleware(object):      def process_request(self, request, spider):          spider.logger.info(&#39;Set proxy.....&#39;)          request.meta[&#39;proxy&#39;] = &quot;http://xxxxxxx&quot;  # setting.py:  DOWNLOADER_MIDDLEWARES = {       &#39;douban_demo.middlewares.ProxyMiddleware&#39;:100  #    &#39;douban_demo.middlewares.DoubanDemoDownloaderMiddleware&#39;: 543,  }</code></pre></li><li>method2: spider/xxxx.py:<pre><code class="lang-python">  def start_requests(self):      start_url=&quot;http://xxx&quot;      return [scrapy.Request(start_url,callback=self.parse,meta={&#39;proxy&#39;:&#39;http://xxx&#39;})]</code></pre></li></ul></li><li><p>scrapy 爬虫使用FilesPipeline 下载 出现302</p><pre><code> [scrapy] WARNING: File (code: 302): Error downloading file from 在settings文件中没有设置MEDIA_ALLOW_REDIRECTS参数的话，默认会将值赋值成False  如果在下载的过程中如果有重定向过程，将不再重定向settings文件中  设置 MEDIA_ALLOW_REDIRECTS =True</code></pre></li></ol><h2 id="header-30">Scrapy-Redis</h2><ul><li>单机架构：本机Scheduler调度本机的一个Requests队列</li><li>分布式架构：各机Scheduler调度一个共享Requests队列</li><li><code>Scrapy-Redis</code>: <ul><li>在Scrapy基础上，重新实现了Scrapy的Scheduler，Queue等组建，使用Redis维护共享队列</li><li>如果Requests队列为空，则会从第一个启动的爬虫的start_urls开始；不为空，则继续从队列中调度出Request进行爬取解析</li><li>Refer <a href="https://github.com/rolando/scrapy-redis" target="_blank" rel="noopener">Github Scrapy-Redis</a></li><li>安装：<code>pip install scrapy-redis</code></li></ul></li></ul><h3 id="header-31">Sample1: use scrapy Spider</h3><ol><li><p>添加Scrapy-redis相关配置（settings.py）</p><pre><code class="lang-python"> # Start Scrapy-Redis Settings: SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot; DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot; SCHEDULER_PERSIST = True #SCHEDULER_FLUSH_ON_START=False ITEM_PIPELINES = {     &#39;scrapy_redis.pipelines.RedisPipeline&#39;: 300 } #REDIS_ITEMS_KEY = &#39;%(spider)s:items&#39; #REDIS_ITEMS_SERIALIZER = &#39;json.dumps&#39; #REDIS_HOST = &#39;localhost&#39; #REDIS_PORT = 6379 #REDIS_PARAMS  = {} REDIS_URL = &#39;redis://root:123456@localhost:6379&#39; REDIS_ENCODING = &#39;utf-8&#39; #REDIS_START_URLS_KEY = &#39;%(name)s:start_urls&#39; #REDIS_START_URLS_AS_SET = False # End Srapy-Redis Settings!</code></pre></li><li><p>Spider：</p><pre><code class="lang-python"> # -*- coding: utf-8 -*- import scrapy from scrapy.linkextractors import LinkExtractor from scrapy.spiders import CrawlSpider, Rule from douban.items import Top250Item class Top250Spider(CrawlSpider):     name = &#39;top250&#39;     allowed_domains = [&#39;movie.douban.com&#39;]     start_urls = [&#39;https://movie.douban.com/top250&#39;]     rules = (         Rule(             LinkExtractor(allow=r&#39;\?start=\d+.*&#39;, restrict_xpaths=&#39;//div[@class=&quot;paginator&quot;]&#39;)             , callback=&#39;parse_item&#39;, follow=True),     )     def parse_item(self, response):         print(response.url)         records = response.xpath(&#39;//ol[@class=&quot;grid_view&quot;]//div[@class=&quot;item&quot;]&#39;)         for r in records:             infoPath = r.xpath(&#39;./div[@class=&quot;info&quot;]&#39;)             picPath = r.xpath(&#39;./div[@class=&quot;pic&quot;]//img&#39;)             item = Top250Item()             link = infoPath.xpath(&#39;./div[@class=&quot;hd&quot;]/a/@href&#39;).get()             item[&#39;id&#39;] = link.split(&#39;/&#39;)[-2]             item[&#39;title&#39;] = infoPath.xpath(&#39;./div[@class=&quot;hd&quot;]/a/span[@class=&quot;title&quot;]/text()&#39;).extract_first()             item[&#39;rate&#39;] = infoPath.xpath(                 &#39;./div[@class=&quot;bd&quot;]/div[@class=&quot;star&quot;]/span[@class=&quot;rating_num&quot;]/text()&#39;).extract_first()             item[&#39;quote&#39;] = infoPath.xpath(&#39;./div[@class=&quot;bd&quot;]/p[@class=&quot;quote&quot;]/span/text()&#39;).extract_first()             item[&#39;cover&#39;] = {                 &#39;name&#39;: picPath.xpath(&#39;./@alt&#39;).get()                 , &#39;url&#39;: picPath.xpath(&#39;./@src&#39;).get()             }             yield item</code></pre></li><li><p>执行 <code>scrapy crawl top250</code></p></li><li><p>查看Redis:</p><pre><code class="lang-bash"> # 1. Processing redis:6379&gt; keys * 1) &quot;top250:requests&quot; 2) &quot;top250:items&quot; 3) &quot;top250:dupefilter&quot; redis:6379&gt; type top250:requests zset redis:6379&gt; zrange top250:requests 0 -1 withscores 1) &quot;\x80\x04\x95a\x01\x00\x00\x00\x00\x00\x00}\x94(\x8c\x03url\x94\x8c1https://movie.douban.com/top250?start=225&amp;filter=\x94\x8c\bcallback\x94\x8c\x14_response_downloaded\x94\x8c\aerrback\x94N\x8c\x06method\x94\x8c\x03GET\x94\x8c\aheaders\x94}\x94C\aReferer\x94]\x94C\x1fhttps://movie.douban.com/top250\x94as\x8c\x04body\x94C\x00\x94\x8c\acookies\x94}\x94\x8c\x04meta\x94}\x94(\x8c\x04rule\x94K\x00\x8c\tlink_text\x94\x8c\nlxml.etree\x94\x8c\x15_ElementUnicodeResult\x94\x93\x94\x8c\x0210\x94\x85\x94\x81\x94\x8c\x05depth\x94K\x01u\x8c\t_encoding\x94\x8c\x05utf-8\x94\x8c\bpriority\x94K\x00\x8c\x0bdont_filter\x94\x89\x8c\x05flags\x94]\x94u.&quot; 2) &quot;0&quot; 3) &quot;\x80\x04\x95u\x01\x00\x00\x00\x00\x00\x00}\x94(\x8c\x03url\x94\x8c/https://movie.douban.com/top250?start=0&amp;filter=\x94\x8c\bcallback\x94\x8c\x14_response_downloaded\x94\x8c\aerrback\x94N\x8c\x06method\x94\x8c\x03GET\x94\x8c\aheaders\x94}\x94C\aReferer\x94]\x94C0https://movie.douban.com/top250?start=25&amp;filter=\x94as\x8c\x04body\x94C\x00\x94\x8c\acookies\x94}\x94\x8c\x04meta\x94}\x94(\x8c\x04rule\x94K\x00\x8c\tlink_text\x94\x8c\nlxml.etree\x94\x8c\x15_ElementUnicodeResult\x94\x93\x94\x8c\a&lt;\xe5\x89\x8d\xe9\xa1\xb5\x94\x85\x94\x81\x94\x8c\x05depth\x94K\x02u\x8c\t_encoding\x94\x8c\x05utf-8\x94\x8c\bpriority\x94K\x00\x8c\x0bdont_filter\x94\x89\x8c\x05flags\x94]\x94u.&quot; 4) &quot;0&quot; # 2. Done redis:6379&gt; keys * 1) &quot;top250:items&quot; 2) &quot;top250:dupefilter&quot; # 3. check requests footprinter (if set SCHEDULER_PERSIST=True): redis:6379&gt; type top250:dupefilter set redis:6379&gt; smembers top250:dupefilter  1) &quot;a6d5976e3143b3d8445e1f70a9250e05a2147ba0&quot;  2) &quot;1eaddf9a0730560642a4d1b2eb7e90ec26ea9c0e&quot;  3) &quot;7efe48768f3d586dcef1245e877eda8c9377385b&quot;  4) &quot;368a5242083cc9dab290d77cbe6a81107c882290&quot;  5) &quot;a7db0795dad78984b0e6622ab7699b53358be585&quot;  6) &quot;c4f38f7d4635b51955cc4129dbbba9c33b202242&quot;  7) &quot;5153c8f0e792e26f62c13e110e7a8a522392f817&quot;  8) &quot;41432c2cf211502120954135e7a9eacc24d15a30&quot;  9) &quot;0a8d961c5cf075725ce493439e64ecef9797cea6&quot; 10) &quot;e8f772a1cff43b734c16f4298dff62dc2ba2cfc7&quot; # 4. check items (if set scrapy_redis.pipelines.RedisPipeline): redis:6379&gt; type top250:items list redis:6379&gt; llen top250:items (integer) 250 lindex top250:items 0 &quot;{\&quot;id\&quot;: \&quot;1851857\&quot;, \&quot;title\&quot;: \&quot;\\u8759\\u8760\\u4fa0\\uff1a\\u9ed1\\u6697\\u9a91\\u58eb\&quot;, \&quot;rate\&quot;: \&quot;9.1\&quot;, \&quot;quote\&quot;: \&quot;\\u65e0\\u5c3d\\u7684\\u9ed1\\u6697\\u3002\&quot;, \&quot;cover\&quot;: {\&quot;name\&quot;: \&quot;\\u8759\\u8760\\u4fa0\\uff1a\\u9ed1\\u6697\\u9a91\\u58eb\&quot;, \&quot;url\&quot;: \&quot;https://img3.doubanio.com/view/photo/s_ratio_poster/public/p462657443.jpg\&quot;}}&quot;</code></pre></li></ol><h3 id="header-32">Sample2: 动态start_urls</h3><ol><li><p>Spider: 使用RedisSpider或者RedisCrawlSpider</p><pre><code class="lang-python"> from scrapy_redis.spiders import RedisCrawlSpider class Top250Spider(CrawlSpider):     name = &#39;top250&#39;     allowed_domains = [&#39;movie.douban.com&#39;]     # start_urls -- no need     # could get from Redis - set `redis_key=&#39;...&#39;`     # (default setting:`START_URLS_KEY=&#39;%(name)s:start_urls&#39;`)     # start_urls = [&#39;https://movie.douban.com/top250&#39;]     rules = (         Rule(             LinkExtractor(allow=r&#39;\?start=\d+.*&#39;, restrict_xpaths=&#39;//div[@class=&quot;paginator&quot;]&#39;)             , callback=&#39;parse_item&#39;, follow=True),     )     def parse_item(self, response):         #....</code></pre></li><li><p>execute <code>scrapy crawl top250</code> (it will keep running and waiting for the start_urls)</p></li><li>redis cli: <code>lpush key value</code> 插入start_urls<pre><code class="lang-bash"> redis:6379&gt; lpush top:start_urls https://movie.douban.com/top250 (integer) 1 redis:6379&gt; keys * 1) &quot;top:items&quot; 2) &quot;top:dupefilter&quot; 3) &quot;top:requests&quot; redis:6379&gt; keys * 1) &quot;top:items&quot; 2) &quot;top:dupefilter&quot;</code></pre></li></ol><h2 id="header-33">Scrapyd (for Deploy)</h2><ul><li><a href="https://scrapyd.readthedocs.io/en/latest/" target="_blank" rel="noopener">Scrapyd Doc</a></li><li>安装：<code>pip install scrapyd</code>,check: <code>scrapyd -h</code></li><li>启动：<code>scrapyd</code>,then could visit: <code>http://127.0.0.1:6800/</code></li></ul><h3 id="header-34">deploy project</h3><p>deploy tools: <a href="https://github.com/scrapy/scrapyd-client" target="_blank" rel="noopener">scrapyd-client</a></p><ul><li>安装：<code>pip install scrapyd-client</code></li><li>编辑项目的<code>scrapy.cfg</code>文件的<code>[deploy]</code>部分：<pre><code>  [deploy]  url = http://localhost:6800/  project = douban</code></pre></li><li>执行打包上传：<code>scrapyd-deploy</code>，then visit <code>http://localhost:6800/</code> to check<pre><code class="lang-python">  $ scrapyd-deploy  Packing version 1562566994  Deploying to project &quot;douban&quot; in http://localhost:6800/addversion.json  Server response (200):  {&quot;node_name&quot;: &quot;cj-Pro.local&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;project&quot;: &quot;douban&quot;, &quot;version&quot;: &quot;1562566994&quot;, &quot;spiders&quot;: 6}</code></pre></li></ul><h3 id="header-35">调用Scrapyd API（直接使用curl）</h3><ul><li>status<pre><code class="lang-bash">  $ curl http://localhost:6800/daemonstatus.json  {&quot;node_name&quot;: &quot;cj-Pro.local&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;pending&quot;: 0, &quot;running&quot;: 0, &quot;finished&quot;: 0}</code></pre></li><li><p>list &amp; delete</p><pre><code class="lang-bash">  # 1.1 list projects  $ curl http://localhost:6800/listprojects.json  {&quot;node_name&quot;: &quot;cj-Pro.local&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;projects&quot;: [&quot;douban&quot;, &quot;default&quot;]}  # 1.2 list project versions  $ curl http://localhost:6800/listversions.json?project=douban  {&quot;node_name&quot;: &quot;cj-Pro.local&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;versions&quot;: [&quot;1562566994&quot;, &quot;1562567575&quot;]}  # 1.3 list spiders  $ curl http://localhost:6800/listspiders.json?project=douban  {&quot;node_name&quot;: &quot;cj-Pro.local&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;spiders&quot;: [&quot;hotMovie&quot;, &quot;movieCsv&quot;, &quot;sinaRss&quot;, &quot;siteUpdate&quot;, &quot;top&quot;, &quot;top250&quot;]}  # 1.4 list jobs  $ curl http://localhost:6800/listjobs.json?project=douban  {&quot;node_name&quot;: &quot;cj-Pro.local&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;pending&quot;: [], &quot;running&quot;: [], &quot;finished&quot;: []}  $ curl http://localhost:6800/listjobs.json?project=douban | python -m json.tool    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current Dload  Upload   Total   Spent    Left  Speed    0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--   100   106  100   106    0     0  14239      0 --:--:-- --:--:-- --:--:-- 15142  {      &quot;finished&quot;: [],      &quot;node_name&quot;: &quot;cj-Pro.local&quot;,      &quot;pending&quot;: [],      &quot;running&quot;: [],      &quot;status&quot;: &quot;ok&quot;  }  # 2.1 delete version  $ curl http://localhost:6800/delversion.json -d project=douban -d version=1562567575  {&quot;node_name&quot;: &quot;cj-Pro.local&quot;, &quot;status&quot;: &quot;ok&quot;}  $ curl http://localhost:6800/listversions.json?project=douban  {&quot;node_name&quot;: &quot;cj-Pro.local&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;versions&quot;: [&quot;1562566994&quot;]}  # 2.2 delte project  $ curl http://localhost:6800/delproject.json -d project=douban  {&quot;node_name&quot;: &quot;cj-Pro.local&quot;, &quot;status&quot;: &quot;ok&quot;}  $ curl http://localhost:6800/listprojects.json  {&quot;node_name&quot;: &quot;cj-Pro.local&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;projects&quot;: [&quot;default&quot;]}</code></pre></li><li><p>schedule &amp; cancel job (run &amp; stop spider)</p><pre><code class="lang-bash">  # 1. schedule  $ curl http://localhost:6800/schedule.json -d project=douban -d spider=top  {&quot;node_name&quot;: &quot;cj-Pro.local&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;jobid&quot;: &quot;73b3a61ca14d11e98c68f45c898fde83&quot;}  # 2. cancel  $ curl http://localhost:6800/cancel.json -d project=douban -d job=73b3a61ca14d11e98c68f45c898fde83  {&quot;node_name&quot;: &quot;cj-Pro.local&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;prevstate&quot;: &quot;running&quot;}  # 3. list  $ curl http://localhost:6800/listjobs.json?project=douban | python -m json.tool    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current     Dload  Upload   Total   Spent    Left  Speed    0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--   100   398  100   398    0     0  51883      0 --:--:-- --:--:-- --:--:-- 56857  {      &quot;finished&quot;: [          {              &quot;end_time&quot;: &quot;2019-04-08 14:56:05.819583&quot;,              &quot;id&quot;: &quot;73b3a61ca14d11e98c68f45c898fde83&quot;,              &quot;spider&quot;: &quot;top&quot;,              &quot;start_time&quot;: &quot;2019-04-08 14:56:04.831726&quot;          }      ],      &quot;node_name&quot;: &quot;cj-Pro.local&quot;,      &quot;pending&quot;: [],      &quot;running&quot;: [],      &quot;status&quot;: &quot;ok&quot;  }  # 4. log  $ curl http://127.0.0.1:6800/logs/douban/top/73b3a61ca14d11e98c68f45c898fde83.log  ....</code></pre></li></ul><h3 id="header-36">调用Scrapyd API（使用python-scrapyd-api包）</h3><ul><li>Refer <a href="https://github.com/djm/python-scrapyd-api" target="_blank" rel="noopener">Github</a></li><li>安装：<code>pip install python-scrapyd-api</code></li><li><p>使用：</p><pre><code class="lang-python">  &gt;&gt;&gt; from scrapyd_api import ScrapydAPI  &gt;&gt;&gt; scrapyd = ScrapydAPI(&#39;http://localhost:6800&#39;)  &gt;&gt;&gt; scrapyd.endpoints  {&#39;add_version&#39;: &#39;/addversion.json&#39;, &#39;cancel&#39;: &#39;/cancel.json&#39;, &#39;delete_project&#39;: &#39;/delproject.json&#39;, &#39;delete_version&#39;: &#39;/delversion.json&#39;, &#39;list_jobs&#39;:&#39;/listjobs.json&#39;, &#39;list_projects&#39;: &#39;/listprojects.json&#39;, &#39;list_spiders&#39;: &#39;/listspiders.json&#39;, &#39;list_versions&#39;: &#39;/listversions.json&#39;, &#39;schedule&#39;: &#39;/schedule.json&#39;}  # 1. list &amp; delete  &gt;&gt;&gt; scrapyd.list_projects()         # scrapyd.delete_project(&#39;project_name&#39;) -- return True/False  [&#39;douban&#39;, &#39;default&#39;]  &gt;&gt;&gt; scrapyd.list_versions(&#39;douban&#39;) # scrapyd.delete_version(&#39;project_name&#39;) -- return True/False  [&#39;1562569685&#39;]  &gt;&gt;&gt; scrapyd.list_spiders(&#39;douban&#39;)  [&#39;hotMovie&#39;, &#39;movieCsv&#39;, &#39;sinaRss&#39;, &#39;siteUpdate&#39;, &#39;top&#39;, &#39;top250&#39;]  &gt;&gt;&gt; scrapyd.list_jobs(&#39;douban&#39;)  {      &#39;node_name&#39;: &#39;cj-Pro.local&#39;,       &#39;pending&#39;: [],       &#39;running&#39;: [],       &#39;finished&#39;: [          {&#39;id&#39;: &#39;db52e176a14c11e98c68f45c898fde83&#39;, &#39;spider&#39;: &#39;top&#39;, &#39;start_time&#39;: &#39;2019-04-08 14:51:49.828399&#39;, &#39;end_time&#39;: &#39;2019-04-08 14:51:50.803166&#39;},           {&#39;id&#39;: &#39;73b3a61ca14d11e98c68f45c898fde83&#39;, &#39;spider&#39;: &#39;top&#39;, &#39;start_time&#39;: &#39;2019-04-08 14:56:04.831726&#39;, &#39;end_time&#39;: &#39;2019-04-08 14:56:05.819583&#39;},           {&#39;id&#39;: &#39;33402590a14f11e98c68f45c898fde83&#39;, &#39;spider&#39;: &#39;top&#39;, &#39;start_time&#39;: &#39;2019-04-08 15:08:34.827750&#39;, &#39;end_time&#39;: &#39;2019-04-08 15:13:33.890973&#39;}      ]  }  # schedule &amp; cancel  &gt;&gt;&gt; settings = {&#39;DOWNLOAD_DELAY&#39;: 2}  &gt;&gt;&gt; scrapyd.schedule(&#39;douban&#39;, &#39;top&#39;, settings=settings)  &#39;3df6469ca15211e98c68f45c898fde83&#39;  &gt;&gt;&gt; scrapyd.job_status(&#39;douban&#39;,&#39;3df6469ca15211e98c68f45c898fde83&#39;)  &#39;running&#39;  &gt;&gt;&gt; scrapyd.cancel(&#39;douban&#39;,&#39;3df6469ca15211e98c68f45c898fde83&#39;)  &#39;running&#39;  &gt;&gt;&gt; scrapyd.job_status(&#39;douban&#39;,&#39;3df6469ca15211e98c68f45c898fde83&#39;)  &#39;finished&#39;</code></pre></li></ul><h2 id="header-37">Reference</h2><ul><li><a href="https://github.com/sixDegree/python-scrapy-demo" target="_blank" rel="noopener">My Demo</a></li><li><a href="https://github.com/jackfrued/Python-100-Days/tree/master/Day66-75" target="_blank" rel="noopener">Python - 100天从新手到大师:爬虫开发</a></li><li><a href="https://blog.csdn.net/zwq912318834/article/details/79530828" target="_blank" rel="noopener">如何使用scrapy中的ItemLoader提取数据？</a></li><li><a href="https://cuiqingcai.com/4421.html" target="_blank" rel="noopener">小白进阶之Scrapy第四篇（图片下载管道篇）</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Scrapy架构，常用命令，文档解析&lt;/li&gt;
&lt;li&gt;Spider: Spider,CrawlSpider,XMLFeedSpider,CSVFeedSpider,SitemapSpider&lt;/li&gt;
&lt;li&gt;Item,ItemLoader&lt;/li&gt;
&lt;li&gt;Middleware: ItemPipeline,Spider/DownloaderMiddleware,Item Exporters&lt;/li&gt;
&lt;li&gt;应用示例：基于Excel爬取，Login,常见问题&lt;/li&gt;
&lt;li&gt;Scrapy-Redis 分布式架构（共享抓取队列）&lt;/li&gt;
&lt;li&gt;Scrapyd（分布式发布）&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://sixdegree.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Python 爬虫之Requests &amp; AioHttp</title>
    <link href="http://sixdegree.github.io/2019/03/18/Python-Requests-AioHttp.html"/>
    <id>http://sixdegree.github.io/2019/03/18/Python-Requests-AioHttp.html</id>
    <published>2019-03-17T16:00:00.000Z</published>
    <updated>2019-07-08T15:46:37.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>requests</li><li>aiohttp</li><li>demo: download files</li></ul><a id="more"></a><h2 id="header-1">Requests Introduction</h2><ul><li>第三方的HTTP客户端库，<a href="http://www.python-requests.org/en/master/" target="_blank" rel="noopener">官网</a>| <a href="http://docs.python-requests.org/zh_CN/latest/index.html" target="_blank" rel="noopener">Doc</a></li><li><p>支持<code>HTTP</code>连接保持和连接池,支持使用<code>cookie</code>保持会话，文件上传，自动确定响应内容的编码，国际化的URL，POST数据自动编码等</p></li><li><p>vs. <code>urllib</code>:</p><ul><li><code>urllib</code>,<code>urllib2</code>,<code>urllib3</code>是python原生类库</li><li><code>urllib</code> 与 <code>urllib2</code> 是两个相互独立的模块(在python3中<code>urllib2</code>被改为<code>urllib.request</code>)</li><li><code>requests</code>库使用了<code>urllib3</code>，支持连接保持（eg:多次请求重复使用一个<code>socket</code>)，更方便<br>  <img src="/2019/03/18/requests-urllib.png" alt=" urllib vs. requests "></li></ul></li><li><p>安装</p><pre><code class="lang-bash">pip install requests</code></pre></li><li><p>使用</p><pre><code class="lang-python">import requestsresponse=requests.get(&#39;http://www.baidu.com&#39;)print(type(response))print(response.status_code,response.reason)print(response.encoding,response.apparent_encoding)print(response.request.headers)print(response.headers)print(response.content)</code></pre></li><li><p>注：</p><ul><li><code>Requests</code>默认的传输适配器使用<code>阻塞IO</code>，<code>Response.content</code>属性会阻塞，直到整个响应下载完成(数据流功能允许每次接受少量的一部分响应，但依然是阻塞式的)</li><li>非阻塞可以考虑其他异步框架，例如<code>grequests</code>，<code>requests-futures</code></li></ul></li></ul><h2 id="header-2">Requests 基础对象和方法</h2><h3 id="header-3">Request 对象</h3><ul><li><code>requests.request(method,url,**kwargs)</code> 构造一个请求,支撑以下各方法的基础方法(method:对应get/put/post等7种)</li><li><code>requests.get(url,params=None,**kwargs)</code></li><li><code>requests.head(url,**kwargs)</code></li><li><code>requests.post(url,data=None,json=None,**kwargs)</code></li><li><code>requests.put/patch(url,data=None,**kwargs)</code></li><li><code>requests.delete(url,**kwargs)</code></li><li>方法参数：<ul><li><code>url</code></li><li><code>params</code>: 作为参数增加到url中 (字典或字节流格式)</li><li><code>data</code>: 作为Request的内容 (字典、字节序列或文件)</li><li><code>json</code>: 作为Request的内容 (JSON格式的数据)</li><li><code>headers</code>: HTTP定制头 (字典)</li><li><code>cookies</code> : Request中的cookie (字典或CookieJar)</li><li><code>auth</code> : 支持HTTP认证功能 (元组)</li><li><code>files</code> : 传输文件 (字典类型)</li><li><code>timeout</code> : 设定超时时间(秒为单位),默认为None，即一直等待</li><li><code>proxies</code> : 设定访问代理服务器,可以增加登录认证(字典类型)</li><li><code>allow_redirects</code> : 重定向开关 (True/False,默认为True)</li><li><code>stream</code> : 获取内容立即下载开关 (True/False)<ul><li>False(默认): 表示立即开始下载文件并存放到内存当中(若文件过大就会导致内存不足的情况)</li><li>True: 推迟下载响应体直到访问 Response.content 属性（请求连接不会被关闭直到读取所有数据或者调用<code>Response.close</code>，使用<code>with</code> 语句发送请求，这样可以保证请求一定会被关闭）</li></ul></li><li><code>verify</code> : 认证SSL证书开关 (True/False,默认为True)</li><li><code>cert</code> : 本地SSL证书路径</li></ul></li></ul><h3 id="header-4">Response 对象</h3><ul><li>类： <code>&lt;class &#39;requests.models.Response&#39;&gt;</code></li><li>状态<ul><li><code>response.status_code</code></li><li><code>response.reason</code></li></ul></li><li>body<ul><li><code>response.raw</code> (原始响应内容 <code>urllib3.response.HTTPResponse</code>, raw.read(),need set <code>stream=True</code> when request)</li><li><code>response.content</code> (二进制形式)</li><li><code>response.text</code> (字符串形式,根据encoding显示网页内容)</li><li><code>response.json()</code> (JSON格式，字典类型)</li></ul></li><li>header<ul><li><code>response.headers</code></li><li><code>request.headers</code></li></ul></li><li>编码<ul><li><code>response.encoding</code> (从HTTP header中猜测的响应内容编码方式)</li><li><code>resposne.apparent_encoding</code> (encoding的备选,从网页内容中分析出的响应内容编码方式) </li></ul></li><li><code>response.raise_for_status()</code><ul><li>在方法内部判断是否等于<code>200</code>,不是则抛出<code>requests.HTTPError</code>异常</li><li>注：不需要增加额外的<code>if</code>语句,该语句便于利用<code>try except</code>进行异常处理</li></ul></li></ul><h3 id="header-5">Exception 对象</h3><ul><li><code>requests.HTTPError</code> : HTTP错误异常</li><li><code>requests.URLRequired</code> : URL缺失异常</li><li><code>requests.Timeout</code> : 请求URL超时,产生超时异常</li><li><code>requests.ConnectTimeout</code> : 连接远程服务器超时异常</li><li><code>requests.ConnectionError</code> : 网络连接错误异常,如DNS查询失败、拒绝连接等</li><li><code>requests.TooManyRedirects</code> 超过最大重定向次数,产生重定向异常</li></ul><h2 id="header-6">Requests 基础示例</h2><p><strong> Visit <code>http://httpbin.org/</code> </strong></p><h3 id="header-7"><code>requests.get</code></h3><pre><code class="lang-python">&gt;&gt;&gt; r=requests.get(&#39;http://httpbin.org/get&#39;)&gt;&gt;&gt; type(r)&lt;class &#39;requests.models.Response&#39;&gt;&gt;&gt;&gt; r.status_code,r.reason(200,&#39;OK&#39;)&gt;&gt;&gt; r.encoding,r.apparent_encoding(None, &#39;ascii&#39;)&gt;&gt;&gt; r.headers{&#39;Access-Control-Allow-Credentials&#39;: &#39;true&#39;, &#39;Access-Control-Allow-Origin&#39;: &#39;*&#39;, &#39;Content-Encoding&#39;: &#39;gzip&#39;, &#39;Content-Type&#39;: &#39;application/json&#39;, &#39;Date&#39;: &#39;Thu, 21 Mar 2019 16:40:42 GMT&#39;, &#39;Server&#39;: &#39;nginx&#39;, &#39;Content-Length&#39;: &#39;184&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;}&gt;&gt;&gt; r.request.headers{&#39;User-Agent&#39;: &#39;python-requests/2.21.0&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Accept&#39;: &#39;*/*&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;}&gt;&gt;&gt; r.json(){  &quot;args&quot;: {},  &quot;headers&quot;: {    &quot;Accept&quot;: &quot;*/*&quot;,    &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;,    &quot;Host&quot;: &quot;httpbin.org&quot;,    &quot;User-Agent&quot;: &quot;python-requests/2.21.0&quot;  },  ...}</code></pre><h3 id="header-8"><code>requests.head</code></h3><pre><code class="lang-python">&gt;&gt;&gt; r=requests.head(&#39;http://httpbin.org/get&#39;)&gt;&gt;&gt; r.text&#39;&#39;&gt;&gt;&gt; r.headers{&#39;Access-Control-Allow-Credentials&#39;: &#39;true&#39;, &#39;Access-Control-Allow-Origin&#39;: &#39;*&#39;, &#39;Content-Encoding&#39;: &#39;gzip&#39;, &#39;Content-Type&#39;: &#39;application/json&#39;, &#39;Date&#39;: &#39;Tue, 19 Mar 2019 13:16:24 GMT&#39;, &#39;Server&#39;: &#39;nginx&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;}</code></pre><h3 id="header-9"><code>requests.post</code>+<code>data</code>/<code>json</code></h3><pre><code class="lang-python"># `data={...}` # 字典,自动编码为form(表单)# content-type: application/x-www-form-urlencoded# request body： key1=value1&amp;key2=value2&gt;&gt;&gt; record={&#39;key1&#39;:&#39;value1&#39;,&#39;key2&#39;:&#39;value2&#39;}&gt;&gt;&gt; r=requests.post(&#39;http://httpbin.org/post&#39;,data=record)&gt;&gt;&gt; r.request.headers[&#39;content-type&#39;]application/x-www-form-urlencoded&gt;&gt;&gt; r.json(){&#39;args&#39;: {}, &#39;data&#39;: &#39;&#39;, &#39;files&#39;: {}, &#39;form&#39;: {&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;}, &#39;headers&#39;: {&#39;Accept&#39;: &#39;*/*&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Content-Length&#39;: &#39;23&#39;, &#39;Content-Type&#39;: &#39;application/x-www-form-urlencoded&#39;, &#39;Host&#39;: &#39;httpbin.org&#39;, &#39;User-Agent&#39;: &#39;python-requests/2.21.0&#39;}, &#39;json&#39;: None, ...}# `data=&#39;...&#39;`# 字符串,自动编码为data# request body: &#39;ABC123&#39;&gt;&gt;&gt; record=&quot;ABC123&quot;&gt;&gt;&gt; r=requests.post(&#39;http://httpbin.org/post&#39;,data=record)&gt;&gt;&gt; r.request.headers.get(&#39;content-type&#39;,None)None&gt;&gt;&gt; r.json(){&#39;args&#39;: {}, &#39;data&#39;: &#39;ABC123&#39;, &#39;files&#39;: {}, &#39;form&#39;: {}, &#39;headers&#39;: {&#39;Accept&#39;: &#39;*/*&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Content-Length&#39;: &#39;6&#39;, &#39;Host&#39;: &#39;httpbin.org&#39;, &#39;User-Agent&#39;: &#39;python-requests/2.21.0&#39;}, &#39;json&#39;: None, ...}# `json={...}`# 字典# content-type: application/json# request body: {&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;}&gt;&gt;&gt; record={&#39;key1&#39;:&#39;value1&#39;,&#39;key2&#39;:&#39;value2&#39;}&gt;&gt;&gt; r = requests.request(&#39;POST&#39;, &#39;http://httpbin.org/post&#39;, json=record)&gt;&gt;&gt; r.request.headers[&#39;Content-Type&#39;]application/json&gt;&gt;&gt; r.json(){&#39;args&#39;: {}, &#39;data&#39;: &#39;{&quot;key1&quot;: &quot;value1&quot;, &quot;key2&quot;: &quot;value2&quot;}&#39;, &#39;files&#39;: {}, &#39;form&#39;: {}, &#39;headers&#39;: {&#39;Accept&#39;: &#39;*/*&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Content-Length&#39;: &#39;36&#39;, &#39;Content-Type&#39;: &#39;application/json&#39;, &#39;Host&#39;: &#39;httpbin.org&#39;, &#39;User-Agent&#39;: &#39;python-requests/2.21.0&#39;}, &#39;json&#39;: {&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;}, ...}</code></pre><h3 id="header-10">kwargs: <code>params</code></h3><pre><code class="lang-python">&gt;&gt;&gt; kv = {&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;}&gt;&gt;&gt; r = requests.request(&#39;GET&#39;, &#39;http://httpbin.org/get&#39;, params=kv) &gt;&gt;&gt; r.urlhttp://httpbin.org/get?key1=value1&amp;key2=value2&gt;&gt;&gt; r.json(){&#39;args&#39;: {&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;}, &#39;headers&#39;: {&#39;Accept&#39;: &#39;*/*&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Host&#39;: &#39;httpbin.org&#39;, &#39;User-Agent&#39;: &#39;python-requests/2.21.0&#39;}, &#39;origin&#39;: &#39;117.83.222.100, 117.83.222.100&#39;, &#39;url&#39;: &#39;https://httpbin.org/get?key1=value1&amp;key2=value2&#39;}</code></pre><h3 id="header-11">kwargs: <code>auth</code></h3><pre><code class="lang-python">import requestsEndpoint=&quot;http://httpbin.org&quot;# 1. basic authr=requests.request(&#39;GET&#39;,Endpoint+&#39;/basic-auth/Tom/Tom111&#39;)print(r.status_code,r.reason)# 401 UNAUTHORIZEDr=requests.request(&#39;GET&#39;,Endpoint+&#39;/basic-auth/Tom/Tom111&#39;,auth=(&#39;Tom&#39;,&#39;Tom123&#39;))print(r.status_code,r.reason)# 401 UNAUTHORIZEDr=requests.request(&#39;GET&#39;,Endpoint+&#39;/basic-auth/Tom/Tom123&#39;,auth=(&#39;Tom&#39;,&#39;Tom123&#39;))print(r.status_code,r.reason)print(r.request.headers)print(r.text)# 200 OK# {&#39;User-Agent&#39;: &#39;python-requests/2.21.0&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Accept&#39;: &#39;*/*&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;Authorization&#39;: &#39;Basic VG9tOlRvbTEyMw==&#39;}# {#   &quot;authenticated&quot;: true,#   &quot;user&quot;: &quot;Tom&quot;# }print(base64.b64decode(&#39;VG9tOlRvbTEyMw==&#39;))print(&#39;--------------------------&#39;)# 2. oauthr=requests.request(&#39;GET&#39;,Endpoint+&#39;/bearer&#39;)print(r.status_code,r.reason)           # 401 UNAUTHORIZEDprint(r.headers)                        # Note: &#39;WWW-Authenticate&#39;: &#39;Bearer&#39;r=requests.request(&#39;GET&#39;,Endpoint+&#39;/bearer&#39;,headers={&#39;Authorization&#39;:&#39;Bearer 1234567&#39;})print(r.status_code,r.reason)           # 200 OKprint(r.headers)print(&#39;--------------------------&#39;)# 3. advance: 自定义身份验证（继承requests.auth.AuthBase）from requests.auth import AuthBaseclass MyAuth(AuthBase):    def __init__(self,authType,token):        self.authType=authType        self.token=token    def __call__(self,req):        req.headers[&#39;Authorization&#39;]=&#39; &#39;.join([self.authType,self.token])        return reqr=requests.request(&#39;GET&#39;,Endpoint+&#39;/bearer&#39;,auth=MyAuth(&#39;Bearer&#39;,&#39;123456&#39;))print(r.status_code,r.reason)                   # 200 OKprint(&quot;Request Headers:&quot;,r.request.headers)print(&quot;Response Headers:&quot;,r.headers)print(&quot;Response Text:&quot;,r.text)</code></pre><h3 id="header-12">kwargs: <code>cookies</code></h3><pre><code class="lang-python">&gt;&gt;&gt; r=requests.request(&#39;GET&#39;,&#39;http://httpbin.org/cookies/set?freedom=test123&#39;)&gt;&gt;&gt; r.cookies&gt;&gt;&gt; r.request.headers{&#39;User-Agent&#39;: &#39;python-requests/2.21.0&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Accept&#39;: &#39;*/*&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;Cookie&#39;: &#39;freedom=test123&#39;}&gt;&gt;&gt; cookies = dict(cookies_are=&#39;working&#39;)       # {&#39;cookies_are&#39;:&#39;working&#39;}&gt;&gt;&gt; r = requests.get(&#39;http://httpbin.org/cookies&#39;, cookies=cookies)&gt;&gt;&gt; r.text&#39;{&quot;cookies&quot;: {&quot;cookies_are&quot;: &quot;working&quot;}}&#39;&gt;&gt;&gt; jar = requests.cookies.RequestsCookieJar()&gt;&gt;&gt; jar.set(&#39;tasty_cookie&#39;, &#39;yum&#39;, domain=&#39;httpbin.org&#39;, path=&#39;/cookies&#39;)&gt;&gt;&gt; jar.set(&#39;gross_cookie&#39;, &#39;blech&#39;, domain=&#39;httpbin.org&#39;, path=&#39;/get&#39;)&gt;&gt;&gt; r = requests.get(&#39;http://httpbin.org/cookies&#39;, cookies=jar)&gt;&gt;&gt; r.text&#39;{&quot;cookies&quot;: {&quot;tasty_cookie&quot;: &quot;yum&quot;}}&#39;</code></pre><h3 id="header-13">kwargs: <code>timeout</code></h3><pre><code class="lang-python">def timeout_request(url,timeout):    try:        resp=requests.get(url,timeout=timeout)        resp.raise_for_status()    except requests.Timeout or requests.HTTPError as e:        print(e)    except Exception as e:        print(&quot;unknow exception:&quot;,e)    else:        print(resp.text)        print(resp.status_code)timeout_request(&#39;http://httpbin.org/get&#39;,0.1)# HTTPConnectionPool(host=&#39;httpbin.org&#39;, port=80): Max retries exceeded with url: /get (Caused by ConnectTimeoutError(&lt;urllib3.connection.HTTPConnection object at 0x1025d9400&gt;, &#39;Connection to httpbin.org timed out. (connect timeout=0.1)&#39;))</code></pre><h3 id="header-14">kwargs: <code>proxies</code></h3><pre><code class="lang-python">&gt;&gt;&gt; pxs = { &#39;http&#39;: &#39;http://user:pass@10.10.10.1:1234&#39; &#39;https&#39;: &#39;https://10.10.10.1:4321&#39; }&gt;&gt;&gt; r = requests.request(&#39;GET&#39;, &#39;http://www.baidu.com&#39;, proxies=pxs)</code></pre><h3 id="header-15">kwargs: <code>files</code></h3><pre><code class="lang-python">f={&#39;image&#39;: open(&#39;黑洞1.jpg&#39;, &#39;rb&#39;)}r = requests.post(Endpoint+&#39;/post&#39;, files=f)print(r.status_code,r.reason)print(r.headers)print(r.text[100:200])print(&#39;--------------------------&#39;)# POST Multiple Multipart-Encoded Filesmultiple_files = [    (&#39;images&#39;, (&#39;黑洞1.jpg&#39;, open(&#39;黑洞1.jpg&#39;, &#39;rb&#39;), &#39;image/jpg&#39;)),    (&#39;images&#39;, (&#39;极光1.jpg&#39;, open(&#39;极光1.jpg&#39;, &#39;rb&#39;), &#39;image/jpg&#39;))]r = requests.post(Endpoint+&#39;/post&#39;, files=multiple_files)print(r.status_code,r.reason)print(r.headers)print(r.text[100:200])print(&#39;--------------------------&#39;)</code></pre><h3 id="header-16">kwargs: <code>stream</code></h3><pre><code class="lang-python">with requests.get(Endpoint+&quot;/stream/3&quot;,stream=True) as r:    print(r.status_code,r.reason)    contentLength=int(r.headers.get(&#39;content-length&#39;,0))    print(&quot;content-length:&quot;,contentLength)    # 此时仅有响应头被下载下来了，连接保持打开状态，因此允许我们根据条件获取内容    if contentLength&lt;100:        print(r.content)    else:        print(&#39;read line by line&#39;)        lines=r.iter_lines() # iter_content 一块一块的下载遍历内容        for line in lines:              if line:                print(line)                 print(&#39;Done&#39;)print(&#39;--------------------------&#39;)</code></pre><h3 id="header-17">Exception</h3><pre><code class="lang-python">import requestsdef do_request(url):  try:    r=requests.get(url,timeout=0.1)    r.raise_for_status()    r.encoding=r.apparent_encoding  except requests.Timeout or requests.HTTPError as e:    print(e)  except Exception as e:    print(&quot;Request Error:&quot;,e)  else:    print(r.text)    print(r.status_code)    return rif __name__==&#39;__main__&#39;:  do_request(&quot;http://www.baidu.com&quot;)</code></pre><h2 id="header-18">Requests 进阶使用</h2><h3 id="header-19">Event hooks</h3><pre><code class="lang-python">def get_key_info(response,*args,**kwargs):    print(&quot;callback:content-type&quot;,response.headers[&#39;Content-Type&#39;])r=requests.get(Endpoint+&#39;/get&#39;,hooks=dict(response=get_key_info))print(r.status_code,r.reason)# callback:content-type application/json# 200 OK</code></pre><h3 id="header-20">Session</h3><ul><li><p>跨请求保持某些参数</p><pre><code class="lang-python">  # 在同一个 Session 实例发出的所有请求之间保持 cookie， 期间使用 urllib3 的 connection pooling 功能  s = requests.Session()  r=s.get(Endpoint+&#39;/cookies/set/mycookie/123456&#39;)  print(&quot;set cookies&quot;,r.status_code,r.reason)     # set cookies 200 OK  r = s.get(Endpoint+&quot;/cookies&quot;)  print(&quot;get cookies&quot;,r.status_code,r.reason)     # get cookies 200 OK  print(r.text)  # {  #   &quot;cookies&quot;: {  #     &quot;mycookie&quot;: &quot;123456&quot;  #   }  # }</code></pre></li><li><p>为请求方法提供缺省数据</p><pre><code class="lang-python">  # 通过为会话对象的属性提供数据来实现（注：方法层的参数覆盖会会话的参数）  s = requests.Session()  s.auth = (&#39;user&#39;, &#39;pass&#39;)  s.headers.update({&#39;x-test&#39;: &#39;true&#39;})  # both &#39;x-test&#39; and &#39;x-test2&#39; are sent  r=s.get(Endpoint+&#39;/headers&#39;, headers={&#39;x-test2&#39;: &#39;true&#39;})  print(r.request.headers)  # {&#39;User-Agent&#39;: &#39;python-requests/2.21.0&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Accept&#39;: &#39;*/*&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;x-test&#39;: &#39;true&#39;, &#39;x-test2&#39;: &#39;true&#39;, &#39;Authorization&#39;: &#39;Basic dXNlcjpwYXNz&#39;}</code></pre></li><li><p>用作前后文管理器</p><pre><code class="lang-python">  with requests.Session() as s:       # 这样能确保 with 区块退出后会话能被关闭，即使发生了异常也一样      s.get(&#39;http://httpbin.org/cookies/set/mycookie/Test123&#39;)      r = s.get(Endpoint+&quot;/cookies&quot;)      print(&quot;set cookies&quot;,r.status_code,r.reason)      print(r.text)      # {      #   &quot;cookies&quot;: {      #     &quot;mycookie&quot;: &quot;Test123&quot;      #   }      # }  print(&quot;out with:&quot;)  r = s.get(Endpoint+&quot;/cookies&quot;)  print(&quot;get cookies&quot;,r.status_code,r.reason)  print(r.text)  # {  #   &quot;cookies&quot;: {  #     &quot;mycookie&quot;: &quot;Test123&quot;  #   }  # }</code></pre></li></ul><h3 id="header-21">Prepared Request</h3><pre><code class="lang-python"># 可在发送请求前，对body／header等做一些额外处理s=Session()req=Request(&#39;GET&#39;,Endpoint+&#39;/get&#39;,headers={&#39;User-Agent&#39;:&#39;fake1.0.0&#39;})prepared=req.prepare()  # 要获取一个带有状态的 PreparedRequest需使用`s.prepare_request(req)`# could do something with prepared.body/prepared.headers here# ...resp=s.send(prepared,timeout=3)print(resp.status_code,resp.reason)print(&quot;request headers:&quot;,resp.request.headers)# {&#39;User-Agent&#39;: &#39;fake1.0.0&#39;}print(&quot;response headers:&quot;,resp.headers)# {&#39;Access-Control-Allow-Credentials&#39;: &#39;true&#39;, &#39;Access-Control-Allow-Origin&#39;: &#39;*&#39;, &#39;Content-Type&#39;: &#39;application/json&#39;, &#39;Date&#39;: &#39;Thu, 21 Mar 2019 15:47:30 GMT&#39;, &#39;Server&#39;: &#39;nginx&#39;, &#39;Content-Length&#39;: &#39;216&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;}print(resp.text)# {#   &quot;args&quot;: {},#   &quot;headers&quot;: {#     &quot;Accept-Encoding&quot;: &quot;identity&quot;,#     &quot;Host&quot;: &quot;httpbin.org&quot;,#     &quot;User-Agent&quot;: &quot;fake1.0.0&quot;#   },#   &quot;origin&quot;: &quot;117.83.222.100, 117.83.222.100&quot;,#   &quot;url&quot;: &quot;https://httpbin.org/get&quot;# }</code></pre><h3 id="header-22">Chunk-Encoded Requests</h3><pre><code class="lang-python"># 分块传输,使用生成器或任意没有具体长度的迭代器def gen():    yield b&#39;hi &#39;    yield b&#39;there! &#39;    yield b&#39;How are you?&#39;    yield b&#39;This is for test 123567890.....!&#39;    yield b&#39;Test ABCDEFG HIGKLMN OPQ RST UVWXYZ.....!&#39;r=requests.post(Endpoint+&#39;/post&#39;, data=gen())    # stream=Trueprint(r.status_code,r.reason,r.headers[&#39;content-length&#39;])for chunk in r.iter_content(chunk_size=100):        # chunk_size=None    if chunk:        print(chunk)print(&#39;done&#39;)</code></pre><h2 id="header-23">Reqeusts 应用示例</h2><p><strong> Download pic from <code>http://www.nationalgeographic.com.cn</code> </strong></p><h3 id="header-24">一次性下载（小文件，<code>stream=False</code>)</h3><pre><code class="lang-python">import requestsimport osdef download_small_file(url):    try:        r=requests.get(url)        r.raise_for_status()        print(r.status_code,r.reason)        contentType=r.headers[&quot;Content-Type&quot;]        contentLength=int(r.headers.get(&quot;Content-Length&quot;,0))        print(contentType,contentLength)    except Exception as e:        print(e)    else:        filename=r.url.split(&#39;/&#39;)[-1]        print(&#39;filename:&#39;,filename)        target=os.path.join(&#39;.&#39;,filename)        if os.path.exists(target) and os.path.getsize(target):            print(&#39;Exist -- Skip download!&#39;)        else:            with open(target,&#39;wb&#39;) as fd:                fd.write(r.content)    print(&#39;done!&#39;)if __name__ == &#39;__main__&#39;:    import time    print(&#39;start&#39;)    start = time.time()    url=&quot;http://image.nationalgeographic.com.cn/2017/0211/20170211061910157.jpg&quot;    download_small_file(url)    end=time.time()    print(&#39;Runs %0.2f seconds.&#39; % (end - start))    print(&#39;end&#39;)</code></pre><h3 id="header-25">流式分块下载（大文件，<code>stream=True</code>）</h3><pre><code class="lang-python">import requestsimport osdef download_large_file(url):    try:        r=requests.get(url,stream=True)        r.raise_for_status()        print(r.status_code,r.reason)        contentType=r.headers[&quot;Content-Type&quot;]        contentLength=int(r.headers.get(&quot;Content-Length&quot;,0))        print(contentType,contentLength)            except Exception as e:        print(e)    else:        filename=r.url.split(&#39;/&#39;)[-1]        print(&#39;filename:&#39;,filename)        target=os.path.join(&#39;.&#39;,filename)        if os.path.exists(target) and os.path.getsize(target):            print(&#39;Exist -- Skip download!&#39;)        else:            with open(target,&#39;wb&#39;) as fd:                for chunk in r.iter_content(chunk_size=10240):                    if chunk:                        fd.write(chunk)                        print(&#39;download:&#39;,len(chunk))    finally:        r.close()        print(&#39;close&#39;)if __name__ == &#39;__main__&#39;:    import time    print(&#39;start&#39;)    start = time.time()    url=&quot;http://image.nationalgeographic.com.cn/2017/0211/20170211061910157.jpg&quot;    download_large_file(url)    end=time.time()    print(&#39;Runs %0.2f seconds.&#39; % (end - start))    print(&#39;end&#39;)</code></pre><h3 id="header-26">显示进度条</h3><pre><code class="lang-python">import requestsimport osdef download_with_progress(url):    try:        with requests.get(url,stream=True) as r:            r.raise_for_status()            print(r.status_code,r.reason)            contentType=r.headers[&quot;Content-Type&quot;]            contentLength=int(r.headers.get(&quot;Content-Length&quot;,0))            print(contentType,contentLength)            filename=r.url.split(&#39;/&#39;)[-1]            print(&#39;filename:&#39;,filename)            target=os.path.join(&#39;.&#39;,filename)            if os.path.exists(target) and os.path.getsize(target):                print(&#39;Exist -- Skip download!&#39;)            else:                chunk_size=1024                progress =ProgressBar(filename, total=content_length,chunk_size=1024,unit=&quot;KB&quot;)                with open(target,&#39;wb&#39;) as fd:                    for chunk in r.iter_content(chunk_size=chunk_size):                        if chunk:                            fd.write(chunk)                            #print(&#39;download:&#39;,len(chunk))                            progress.refresh(count=len(chunk))    except Exception as e:        print(e)    print(&#39;done&#39;)# ProgressBarclass ProgressBar(object):def __init__(self,title,total,chunk_size=1024,unit=&#39;KB&#39;):    self.title=title    self.total=total    self.chunk_size=chunk_size    self.unit=unit    self.progress=0.0def __info(self):    return &quot;【%s】%s %.2f%s / %.2f%s&quot; % (self.title,self.status,self.progress/self.chunk_size,self.unit,self.total/self.chunk_size,self.unit)def refresh(self,progress):    self.progress += progress    self.status=&quot;......&quot;    end_str=&#39;\r&#39;    if self.total&gt;0 and self.progress&gt;=self.total:        end_str=&#39;\n&#39;        self.status=&#39;completed&#39;    print(self.__info(),end=end_str)if __name__ == &#39;__main__&#39;:    import time    print(&#39;start&#39;)    start = time.time()    url=&quot;http://image.nationalgeographic.com.cn/2017/0211/20170211061910157.jpg&quot;    download_with_progress(url)    end=time.time()    print(&#39;Runs %0.2f seconds.&#39; % (end - start))    print(&#39;end&#39;)</code></pre><h3 id="header-27">多任务下载</h3><ul><li><p>多进程下载：并行（同时）</p><pre><code class="lang-python">  import multiprocessing  from multiprocessing import Pool  # do multiple downloads - multiprocessing  def do_multiple_download_multiprocessing(url_list,targetDir):      cpu_cnt=multiprocessing.cpu_count()      print(&quot;系统进程数: %s, Parent Pid: %s&quot; % (cpu_cnt,os.getpid()))      p = Pool(cpu_cnt)      results=[]      for i,url in enumerate(url_list):          result=p.apply_async(do_download,args=(i,url,targetDir,False,),callback=print_return)          results.append(result)      print(&#39;Waiting for all subprocesses done...&#39;)      p.close()      p.join()      for result in results:          print(os.getpid(),result.get())      print(&#39;All subprocesses done.&#39;)  # callback  def print_return(result):      print(os.getpid(),result)</code></pre></li><li><p>多线程下载：并发（交替）</p><pre><code class="lang-python">  import threading  def do_multiple_downloads_threads(url_list,targetDir):      thread_list=[]      for i,url in enumerate(url_list):          thread=threading.Thread(target=do_download,args=(i,url,targetDir,True,))          thread.start()          thread_list.append(thread)      print(&#39;Waiting for all threads done...&#39;)      for thread in thread_list:          thread.join()      print(&#39;All threads done.&#39;)</code></pre></li><li><p>verify</p><pre><code class="lang-python">  import requests  from bs4 import BeautifulSoup  import os,time  import re  # do download using `requests`  def do_download(i,url,targetDir,isPrint=False):      headers={          &#39;User-Agent&#39;:&#39;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:60.0) Gecko/20100101 Firefox/60.0&#39;      }      try:          response=requests.get(url,headers=headers,stream=True,verify=False)          response.raise_for_status()      except Exception as e:          print(&quot;Occur Exception:&quot;,e)      else:          content_length = int(response.headers.get(&#39;Content-Length&#39;,0))          filename=str(i)+&quot;.&quot;+url.split(&#39;/&#39;)[-1]          print(response.status_code,response.reason,content_length,filename)          progressBar=ProgressBar(filename, total=content_length,chunk_size=1024,unit=&quot;KB&quot;)          with open(os.path.join(targetDir,filename),&#39;wb&#39;) as fd:              for chunk in response.iter_content(chunk_size=1024):                  if chunk:                      fd.write(chunk)                      progressBar.refresh(len(chunk))          if isPrint:              print(os.getpid(),threading.current_thread().name,filename,&quot;Done!&quot;)          return &#39;%s %s %s Done&#39; % (os.getpid(),threading.current_thread().name,filename)  # prepare download urls  def url_list_crawler():      url=&quot;http://m.ngchina.com.cn/travel/photo_galleries/5793.html&quot;      response=requests.get(url)      print(response.status_code,response.reason,response.encoding,response.apparent_encoding)      response.encoding=response.apparent_encoding      soup=BeautifulSoup(response.text,&#39;html.parser&#39;)      #results=soup.select(&#39;div#slideBox ul a img&#39;)      #results=soup.find_all(&#39;img&#39;)      results=soup.select(&quot;div.sub_center img[src^=&#39;http&#39;]&quot;)      url_list=[ r[&quot;src&quot;] for r in results]      print(&quot;url_list:&quot;,len(url_list))      print(url_list)      return url_list  # main  if __name__==&#39;__main__&#39;:      print(&#39;start&#39;)      targetDir=&quot;/Users/cj/space/python/download&quot;      url=&quot;http://image.ngchina.com.cn/2019/0325/20190325110244384.jpg&quot;      url_list=url_list_crawler()      start=time.time()      # 0 download one file using `requests`      do_download(&quot;A&quot;,url,targetDir)      end = time.time()      print(&#39;Total cost %0.2f seconds.&#39; %  (end-start))      start=end      # 1 using multiple processings      do_multiple_download_multiprocessing(url_list,targetDir)      end = time.time()      print(&#39;Total cost %0.2f seconds.&#39; %  (end-start))      start=end      # 2 using multiple threads      do_multiple_downloads_threads(url_list,targetDir)      end = time.time()      print(&#39;Total cost %0.2f seconds.&#39; %  (end-start))      start=end      print(&#39;end&#39;)</code></pre></li></ul><h2 id="header-28">aiohttp</h2><p><a href="https://aiohttp.readthedocs.io/en/stable/" target="_blank" rel="noopener">官网</a></p><blockquote><p>Asynchronous HTTP Client/Server for asyncio and Python.</p></blockquote><ul><li>支持客户端和HTTP服务器</li><li>提供异步web服务的库 ( <code>requests</code>是同步阻塞的)</li><li>无需使用Callback Hell即可支持Server/Client WebSockets</li><li>install: <code>pip install aiohttp</code></li></ul><h3 id="header-29">Client Sample</h3><p>Refer to <a href="https://aiohttp.readthedocs.io/en/stable/client_quickstart.html#" target="_blank" rel="noopener">Client Quickstart</a></p><pre><code class="lang-python">import aiohttpimport asyncioasync def fetch(session, url):    async with session.get(url) as response:        return await response.text()async def main():    async with aiohttp.ClientSession() as session:        html = await fetch(session, &#39;http://httpbin.org/headers&#39;)        print(html)loop = asyncio.get_event_loop()loop.run_until_complete(main())</code></pre><h3 id="header-30">Server Sample</h3><p>Refer to <a href="https://aiohttp.readthedocs.io/en/stable/web_quickstart.html" target="_blank" rel="noopener">Web Server Quickstart</a></p><pre><code class="lang-python">from aiohttp import webasync def handle(request):    name = request.match_info.get(&#39;name&#39;, &quot;Anonymous&quot;)    text = &quot;Hello, &quot; + name    return web.Response(text=text)app = web.Application()app.add_routes([web.get(&#39;/&#39;, handle),                web.get(&#39;/{name}&#39;, handle)])web.run_app(app)</code></pre><h3 id="header-31">应用：协程并发下载文件</h3><p>单线程 &amp; 异步 &amp; 非阻塞</p><ul><li><p>do download using <code>aiohttp</code></p><pre><code class="lang-python">  async def do_aiohttp_download(session,i,url,targetDir):      async with session.get(url) as response:          content_length = int(response.headers.get(&#39;Content-Length&#39;,0))          filename=str(i)+&quot;.&quot;+url.split(&#39;/&#39;)[-1]          print(response.status,response.reason,content_length,filename)          progressBar=ProgressBar(filename, total=content_length,chunk_size=1024,unit=&quot;KB&quot;)          with open(os.path.join(targetDir,filename),&#39;wb&#39;) as fd:              while True:                  chunk=await response.content.read(1024)                  if not chunk:                      break;                  fd.write(chunk)                  progressBar.refresh(len(chunk))          await response.release()      # print(filename,&quot;Done!&quot;)      return filename  # callback  def print_async_return(task):      print(task.result(),&quot;Done&quot;)  def print_async_return2(i,task):      print(i,&quot;:&quot;,task.result(),&quot;Done&quot;)</code></pre></li><li><p>case1: do one download</p><pre><code class="lang-python">  async def do_async_download(i,url,targetDir):      async with aiohttp.ClientSession() as session:          return await do_aiohttp_download(session,i,url,targetDir)</code></pre></li><li><p>case2: do multiple download</p><pre><code class="lang-python">  # do multiple downloads - asyncio  async def do_multiple_downloads_async(url_list,targetDir):       async with aiohttp.ClientSession() as session:          # tasks=[do_aiohttp_download(session,url,targetDir) for url in url_list]          # await asyncio.gather(*tasks)                    tasks=[]          for i,url in enumerate(url_list):              task=asyncio.create_task(do_aiohttp_download(session,i,url,targetDir))              # task.add_done_callback(print_async_return)              task.add_done_callback(functools.partial(print_async_return2,i))              tasks.append(task)              await asyncio.gather(*tasks)</code></pre></li><li><p>verify</p><pre><code class="lang-python">  import os,time  import asyncio  import aiohttp  import functools  import re  # prepare download urls  def url_list_crawler():      url=&quot;http://m.ngchina.com.cn/travel/photo_galleries/5793.html&quot;      response=requests.get(url)      print(response.status_code,response.reason,response.encoding,response.apparent_encoding)      response.encoding=response.apparent_encoding      soup=BeautifulSoup(response.text,&#39;html.parser&#39;)      #results=soup.select(&#39;div#slideBox ul a img&#39;)      #results=soup.find_all(&#39;img&#39;)      results=soup.select(&quot;div.sub_center img[src^=&#39;http&#39;]&quot;)      url_list=[ r[&quot;src&quot;] for r in results]      print(&quot;url_list:&quot;,len(url_list))      print(url_list)      return url_list  # main            if __name__==&#39;__main__&#39;:      print(&#39;start&#39;)      targetDir=&quot;/Users/cj/space/python/download&quot;      url=&quot;http://image.ngchina.com.cn/2019/0325/20190325110244384.jpg&quot;      url_list=url_list_crawler()      start=time.time()      # 1. download one file using `aiohttp`      loop=asyncio.get_event_loop()      loop.run_until_complete(do_async_download(&quot;A&quot;,url,targetDir))      loop.close()      end = time.time()      print(&#39;Total cost %0.2f seconds.&#39; %  (end-start))      start=end      # 2. download many files using `aiohttp`      loop=asyncio.get_event_loop()      loop.run_until_complete(do_multiple_downloads_async(url_list,targetDir))      loop.close()      end = time.time()      print(&#39;Total cost %0.2f seconds.&#39; %  (end-start))      start=end      print(&#39;end&#39;)</code></pre></li></ul><h2 id="header-32">Reference</h2><ul><li><a href="https://github.com/sixDegree/python-basic-demo" target="_blank" rel="noopener">My Demo</a></li><li><a href="https://zhuanlan.zhihu.com/p/37824910" target="_blank" rel="noopener">python下载文件—-requests</a></li><li><a href="https://www.cnblogs.com/ssyfj/p/9222342.html" target="_blank" rel="noopener">python—aiohttp的使用</a></li><li><a href="https://blog.fudenglong.site/2017/06/04/Python%E5%B9%B6%E5%8F%91%E4%B8%8B%E8%BD%BD%E7%9A%84%E4%BE%8B%E5%AD%90%E5%92%8C%E6%AF%94%E8%BE%83/" target="_blank" rel="noopener">Python并发下载的例子和比较</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;requests&lt;/li&gt;
&lt;li&gt;aiohttp&lt;/li&gt;
&lt;li&gt;demo: download files&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://sixdegree.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Python 爬虫基础</title>
    <link href="http://sixdegree.github.io/2019/03/15/Python-Crawler-Basic.html"/>
    <id>http://sixdegree.github.io/2019/03/15/Python-Crawler-Basic.html</id>
    <published>2019-03-14T16:00:00.000Z</published>
    <updated>2019-07-08T17:20:04.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>爬虫基础</li><li>Re</li><li>BeautifulSoup(include CSS Selector)</li><li>XPath</li><li>JSONPath</li></ul><a id="more"></a><h2 id="header-1">基础概念</h2><h3 id="header-2">HTTP协议</h3><blockquote><p>wikipedia:<br>The Hypertext transfer Protocal(HTTP) is a stateless(无状态) application-level protocol for distributed(分布式),collaborative(协作式),hypertext information systems.</p></blockquote><p>HyperText Transfer Protocol 超文本传输协议</p><ul><li><p>是一个基于“请求与响应”模式的、无状态的应用层协议</p></li><li><p>采用URL作为定位网络资源的标识,格式如下: <code>http://host[:port][path]</code></p><ul><li><code>host</code>: 合法的Internet主机域名或IP地址 </li><li><code>port</code>: 端口号,缺省端口为80</li><li><code>path</code>: 请求资源的路径</li></ul></li><li><p>对资源的操作：</p><ul><li><code>GET</code> 请求获取URL位置的资源</li><li><code>HEAD</code> 请求获取URL位置资源的响应消息报告,即获得该资源的头部信息 </li><li><code>POST</code> 请求向URL位置的资源后附加新的数据</li><li><code>PUT</code> 请求向URL位置存储一个资源,覆盖原URL位置的资源</li><li><code>PATCH</code> 请求局部更新URL位置的资源,即改变该处资源的部分内容</li><li><code>DELETE</code> 请求删除URL位置存储的资源</li><li><code>PATCH</code> vs. <code>PUT</code>: <ul><li>假设URL位置有一组数据UserInfo,包括UserID、UserName等20个字段, 需求:用户修改了UserName,其他不变</li><li><code>PATCH</code>: 仅向URL提交UserName的局部更新请求</li><li><code>PUT</code> 须将所有20个字段一并提交到URL,未提交字段被删除 </li><li><code>PATCH</code>的最主要好处:节省网络带宽</li></ul></li></ul></li><li><p>响应状态码</p><ul><li><code>2xx</code> 成功</li><li><code>3xx</code> 跳转<ul><li>300 Multiple Choices 存在多个可用资源，可处理可丢弃</li><li>301 Moved Permanetly 重定向</li><li>302 Found 重定向</li><li>304 Not Modified 请求资源未更新，丢弃</li><li>注：一些python库（urllib2,requests,…）已经对重定向做了处理，会自动跳转</li></ul></li><li><code>4xx</code> 客户端错误<ul><li>400 Bad Request 客户端请求有语法错误，不能被服务器所理解（请求参数或者路径错误）</li><li>401 Unauthorized 请求未经授权，这个状态吗需和www-Authenticate报头域一起使用（无权限访问）</li><li>403 Forbidden 服务器收到请求，但拒绝提供服务（未登录/IP被封/…）</li><li>404 Not Found 请求资源部存在</li></ul></li><li><code>5xx</code> 服务端错误<ul><li>500 Internal Server Error 服务器发生了不可预期的错误</li><li>503 Server Unavailable 服务器当前不能处理客户端请求，一段时间后可能恢复正常</li></ul></li></ul></li><li><p>Http Header</p><ul><li>Request Http Header<pre><code>  Accept: text/plain  Accept-Charset: utf-8  Accept-Encoding: gzip,deflate  Accept-Language: en-US  Connection: keep-alive  Content-Length: 348  Content-Type: application/x-www-form-urlencoded  User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:60.0) Gecko/20100101 Firefox/60.0  Cookie: $version=1; Skin=new;  Date: ...  Host: ...  ....</code></pre></li><li>Response Http Header<pre><code>  Status: 200 OK  Accept: text/plain;charset=utf-8  Content-Eoncoding: gzip  Content_Language: en-US  Content-Length: 348  Set-Cookie: UserID=xxx,Max-Age=3600;Version=1;...  Location: ...  Last-Modified: ...  ...</code></pre></li></ul></li></ul><h3 id="header-3">深度抓取与广度抓取</h3><pre><code>        A     /     \    B       C    /       \ D,E,F,G     X,Y,Z|H,I,J,K</code></pre><ul><li>深度抓取(垂直)<ul><li>堆栈（递归，先进后出）</li><li>A -&gt; B -&gt; D -&gt; H -&gt; I,J,K -&gt; E,F,G -&gt; C -&gt; X,Y,Z</li></ul></li><li>广度抓取(水平)<ul><li>队列（先进先出）</li><li>A -&gt; B,C -&gt; D,E,F,G ; X,Y,Z -&gt; H,I,J,K</li></ul></li><li>策略：<ul><li>重要的网页距离种子站点比较近</li><li>一个网页可能有很多路径可以到达（图）</li><li>广度优先有利于多爬虫并行抓取</li><li>深度与广度结合</li></ul></li></ul><h3 id="header-4">不重复抓取策略</h3><ul><li>记录抓取历史（URL）<ul><li>保存到数据库（效率低）</li><li>使用HashSet(内存限制)</li></ul></li><li>尽量压缩URL<ul><li>MD5／SHA-1编码成一段统一长度的数字／字符串，太长，一般会编码后再取模</li><li>BitMap方法：建立BitSet,将URL（可在MD5基础上）经过Hash函数映射到一个或多个Bit位来记录</li><li>BloomFilter: 在BitMap基础上，使用多个Hash函数</li><li>注：存在一定碰撞</li></ul></li><li>操作：<ul><li>评估网站的网页数量</li><li>选择合适的Hash算法和空间阈值，降低碰撞几率</li><li>选择合适的存储结构和算法</li></ul></li><li>注：<ul><li>网页数量少的情况下，不需要进行压缩（多数情况）</li><li>网页数量大的情况下，可使用BloomFilter压缩URL，重点是计算碰撞概率，以此确定存储空间的阈值</li><li>分布式系统，可将散列映射到多台主机</li></ul></li></ul><h3 id="header-5">网络爬虫的限制</h3><ul><li>来源审查: 检查来访HTTP协议头的<code>User‐Agent</code>域,只响应浏览器或友好爬虫的访问</li><li>发布公告: <code>Robots</code>协议,网站告知网络爬虫哪些页面可以抓取,哪些不行<ul><li>在网站根目录下的<code>robots.txt</code>文件 (Robots Exclusion Standard 网络爬虫排除标准)</li><li><code>Robots</code>协议是建议但非约束性,网络爬虫可以不遵守,但存在法律风险</li><li>基本语法<pre><code>  # `*`代表所有, `/`代表根目录   User‐agent: *  Disallow: /</code></pre></li><li>eg: <code>https://www.jd.com/robots.txt</code><pre><code>  User‐agent: *  Disallow: /?*  Disallow: /pop/*.html  Disallow: /pinpai/*.html?*  User‐agent: EtaoSpider  Disallow: /  User‐agent: HuihuiSpider  Disallow: /  User‐agent: GwdangSpider  Disallow: /  User‐agent: WochachaSpider  Disallow: /</code></pre></li></ul></li></ul><h3 id="header-6">网站结构分析</h3><ul><li>利用sitemap里的信息</li><li>对网站目录结构进行分析</li><li>网页解析器：<ul><li>模糊匹配：<ul><li>正则表达式</li></ul></li><li>结构化解析：<ul><li>htmp.parser</li><li>BeautifulSoup</li><li>lxml</li><li>。。。</li></ul></li></ul></li></ul><h2 id="header-7">文档解析之Re</h2><h3 id="header-8">正则表达式</h3><p>Regular Expression(regex)</p><ul><li>一种通用的字符串表达框架,简洁表达一组字符串</li><li>特点: 简洁,一行胜千言（一行就是特征，即模式)</li><li>用途(主要应用在字符串匹配中)：<ul><li>表达文本类型的特征(病毒、入侵等)</li><li>匹配字符串的全部或部分</li><li>查找或替换一组字符串</li><li>…</li></ul></li><li><p>语法由字符和操作符构成, 常用操作符:</p><table class="table"><thead><tr><th style="text-align:left">操作符</th><th style="text-align:left">说明</th><th style="text-align:left">实例</th></tr></thead><tbody><tr><td style="text-align:left"><code>.</code></td><td style="text-align:left">表示任何单个字符 |</td></tr><tr><td style="text-align:left"><code>[]</code></td><td style="text-align:left">字符集,对单个字符给出取值范围</td><td style="text-align:left"><code>[abc]</code>表示a、b、c,<code>[a‐z]</code>表示a到z单个字符</td></tr><tr><td style="text-align:left"><code>[^ ]</code></td><td style="text-align:left">非字符集,对单个字符给出排除范围</td><td style="text-align:left"><code>[^abc]</code>表示非a或b或c的单个字符</td></tr><tr><td style="text-align:left"><code>*</code></td><td style="text-align:left">前一个字符0次或无限次扩展</td><td style="text-align:left"><code>abc*</code> 表示 ab、abc、abcc、abccc等</td></tr><tr><td style="text-align:left"><code>+</code></td><td style="text-align:left">前一个字符1次或无限次扩展</td><td style="text-align:left"><code>abc+</code> 表示 abc、abcc、abccc等</td></tr><tr><td style="text-align:left"><code>?</code></td><td style="text-align:left">前一个字符0次或1次扩展</td><td style="text-align:left"><code>abc?</code> 表示 ab、abc</td></tr><tr><td style="text-align:left"><code>|</code></td><td style="text-align:left">左右表达式任意一个</td><td style="text-align:left"><code>abc|def</code> 表示 abc、def</td></tr><tr><td style="text-align:left"><code>{m}</code></td><td style="text-align:left">扩展前一个字符m次</td><td style="text-align:left"><code>ab{2}c</code>表示abbc</td></tr><tr><td style="text-align:left"><code>{m,n}</code></td><td style="text-align:left">扩展前一个字符m至n次(含n)</td><td style="text-align:left"><code>ab{1,2}c</code>表示abc、abbc</td></tr><tr><td style="text-align:left"><code>^</code></td><td style="text-align:left">匹配字符串开头</td><td style="text-align:left"><code>^abc</code>表示abc且在一个字符串的开头</td></tr><tr><td style="text-align:left"><code>$</code></td><td style="text-align:left">匹配字符串结尾</td><td style="text-align:left"><code>abc$</code>表示abc且在一个字符串的结尾</td></tr><tr><td style="text-align:left"><code>( )</code></td><td style="text-align:left">分组标记,内部只能使用</td><td style="text-align:left"><code>(abc)</code>表示abc,<code>(abc|def)</code>表示abc、def</td></tr><tr><td style="text-align:left"><code>\d</code></td><td style="text-align:left">数字,等价于<code>[0‐9]</code></td><td style="text-align:left">/</td></tr><tr><td style="text-align:left"><code>\w</code></td><td style="text-align:left">单词字符,等价于<code>[A‐Za‐z0‐9]</code></td><td style="text-align:left">/</td></tr></tbody></table></li><li><p>eg1:</p><ul><li>一组字符串(无穷个): ‘PY’, ‘PYY’, ‘PYYY’, ‘PYYYY’, ……, ‘PYYYY……’</li><li>正则表达式(无穷字符串组的简洁表达): <code>PY+</code></li></ul></li><li>eg2: <ul><li>一组字符串:  ‘PN’, ‘PYN’, ‘PYTN’, ‘PYTHN’, ‘PYTHON’</li><li>正则表达式(简洁表达):<code>P(Y|YT|YTH|YTHO)?N</code></li></ul></li><li>eg3:<ul><li>表示一组’PY’开头，后续存在不多于10个字符，且不能是’P’或’Y’的字符串(如：’PYABC’ 正确;’PYKXYZ’ 不正确)</li><li>正则表达式（特征字符串组的简洁表达）：<code>PY[^PY]{0,10}</code></li></ul></li><li><p>eg4:</p><ul><li>‘PN’、’PYN’、’PYYN’、’PYYYN’…</li><li><code>PY{:3}N</code></li></ul></li><li><p>经典正则表达式实例：</p><ul><li><code>^[A‐Za‐z]+$</code> 由26个字母组成的字符串</li><li><code>^[A‐Za‐z0‐9]+$</code> 由26个字母和数字组成的字符串</li><li><code>[1‐9]\d{5}</code> 中国境内6位邮政编码</li><li><code>[\u4e00‐\u9fa5]</code> 匹配中文字符</li><li><code>\d{3}‐\d{8}|\d{4}‐\d{7}</code> 国内电话号码(eg: 010‐68913536)</li><li><code>(([1‐9]?\d|1\d{2}|2[0‐4]\d|25[0‐5]).){3}([1‐9]?\d|1\d{2}|2[0‐4]\d|25[0‐5])</code> IP地址(4段)<ul><li>0‐99: <code>[1‐9]?\d</code></li><li>100‐199: <code>1\d{2}</code></li><li>200‐249: <code>2[0‐4]\d</code> </li><li>250‐255: <code>25[0‐5]</code></li><li>简化表达：<code>\d+.\d+.\d+.\d+</code> 或 <code>\d{1,3}.\d{1,3}.\d{1,3}.\d{1,3}</code></li></ul></li></ul></li></ul><h3 id="header-9">Re库</h3><ul><li>Python的标准库(用于字符串匹配)</li><li>导入<code>import re</code></li><li>正则表达式的表示类型：<ul><li>raw string(原生字符串类型)：<code>r&#39;text&#39;</code>, eg: <code>r&#39;[1‐9]\d{5}&#39;</code>, <code>r&#39;\d{3}‐\d{8}|\d{4}‐\d{7}&#39;</code></li><li>string(更繁琐), eg: <code>&#39;[1‐9]\\d{5}&#39;</code>,<code>&#39;\\d{3}‐\\d{8}|\\d{4}‐\\d{7}&#39;</code></li><li>注：raw string不包含对转义符再次转义的字符串,所以建议当正则表达式包含转义符时,使用raw string</li></ul></li><li><p>函数式用法: 一次性操作</p><ul><li><code>re.search(pattern, string, flags=0)</code>: 搜索第一个匹配的,返回<code>match</code>对象<ul><li><code>pattern</code> : 正则表达式(string/raw string)</li><li><code>string</code> : 待匹配字符串</li><li><code>flags</code>: 控制标记<ul><li><code>re.I</code>,<code>re.IGNORECASE</code> : 忽略大小写</li><li><code>re.M</code>,<code>re.MULTILINE</code> : <code>^</code>操作符能将给定字符串的每行当作匹配开始</li><li><code>re.S</code>,<code>re.DOTALL</code> : <code>.</code>操作符能匹配所有字符(默认是匹配除换行外的所有字符）</li></ul></li><li>eg: <pre><code class="lang-python">  import re  match=re.search(r&#39;[1-9]\d{5}&#39;,&#39;BIT100081 TSU100084&#39;)  if match:      print(match.group(0))   # 100081</code></pre></li></ul></li><li><p><code>re.match(pattern, string, flags=0)</code>: 从头开始匹配，返回<code>match</code>对象</p><ul><li>参数同上</li><li><p>eg:</p><pre><code class="lang-python">  match=re.match(r&#39;[1-9]\d{5}&#39;,&#39;BIT100081 TSU100084&#39;)  if match:      print(match.group(0))   # AttributeError: &#39;NoneType&#39; object has no attribute &#39;group&#39;  match=re.match(r&#39;[1-9]\d{5}&#39;,&#39;100081BIT TSU100084&#39;)  if match:      print(match.group(0))   # 100081</code></pre></li></ul></li><li><code>re.findall(pattern, string, flags=0)</code>: 搜索,返回匹配字串列表<ul><li>参数同上</li><li>eg:<pre><code class="lang-python">  ls=re.findall(r&#39;[1-9]\d{5}&#39;,&#39;BIT100081 TSU100084&#39;) # [&#39;100081&#39;,&#39;100084&#39;]</code></pre></li></ul></li><li><code>re.finditer(pattern, string, flags=0)</code>: 搜索,返回匹配结果的迭代类型,每个迭代元素是<code>match</code>对象<ul><li>参数同上</li><li>eg:<pre><code class="lang-python">  for match in re.finditer(r&#39;[1-9]\d{5}&#39;,&#39;BIT100081 TSU100084&#39;):      if match:          print(match.group(0))  # 100081  # 100084</code></pre></li></ul></li><li><code>re.split(pattern, string, maxsplit=0, flags=0)</code>: 分割, 返回列表<ul><li><code>maxsplit</code>: 最大分割数,剩余部分作为最后一个元素输出</li><li>eg:<pre><code class="lang-python">  re.split(r&#39;[1-9]\d{5}&#39;,&#39;BIT100081 TSU100084&#39;)   # [&#39;BIT&#39;,&#39; TSU&#39;,&#39;&#39;]   re.split(r&#39;[1-9]\d{3}&#39;,&#39;BIT100081 TSU100084&#39;)   # [&#39;BIT&#39;, &#39;81 TSU&#39;, &#39;84&#39;]  re.split(r&#39;[1-9]\d{5}&#39;,&#39;BIT100081 TSU100084&#39;,maxsplit=1) # [&#39;BIT&#39;,&#39; TSU100084&#39;]</code></pre></li></ul></li><li><code>re.sub(pattern, repl, string, count=0, flags=0)</code>: 替换所有匹配的子串,返回替换后的字符串<ul><li><code>repl</code> 替换字符串</li><li><code>string</code> : 待匹配字符串</li><li><code>count</code> 最大替换次数</li><li>eg: <pre><code class="lang-python">  re.sub(r&#39;[1-9]\d{5}&#39;,&#39;:zipcode&#39;,&#39;BIT100081 TSU100084&#39;) # &#39;BIT:zipcode TSU:zipcode&#39;</code></pre></li></ul></li></ul></li><li><p>面向对象用法:编译后的多次操作</p><ul><li>Step1: <code>regex = re.compile(pattern, flags=0)</code> 将正则表达式的字符串形式<code>编译</code>成正则表达式对象</li><li>Step2:<ul><li><code>regex.search(string, flags=0)</code></li><li><code>regex.match(string, flags=0)</code></li><li><code>regex.findall(string, flags=0)</code></li><li><code>regex.finditer(string, flags=0)</code></li><li><code>regex.split(string, maxsplit=0, flags=0)</code></li><li><code>regex.sub(repl, string, count=0, flags=0)</code></li></ul></li></ul></li><li><p>Match对象：</p><ul><li>一次匹配的结果,包含匹配的很多信息</li><li>属性：<ul><li><code>.string</code>: 待匹配的文本</li><li><code>.re</code>: 匹配时使用的pattern对象(正则表达式)</li><li><code>.pos</code>: 搜索文本的开始位置</li><li><code>.endpos</code>: 搜索文本的结束位置</li></ul></li><li>方法：<ul><li><code>.group(0)</code>: 获得匹配后的字符串 </li><li><code>.start()</code>: 匹配字符串在原始字符串的开始位置 </li><li><code>.end()</code>: 匹配字符串在原始字符串的结束位置 </li><li><code>.span()</code>: 返回<code>(.start(), .end())</code></li></ul></li><li>eg:<pre><code class="lang-python">  match=re.search(r&#39;[1-9]\d{5}&#39;,&#39;BIT100081 TSU100084&#39;)  # 属性：  match.string    # &#39;BIT100081 TSU100084&#39;  match.re        # re.compile(&#39;[1-9]\\d{5}&#39;)  match.pos       # 0  match.endpos    # 19  # 方法：  match.group(0)  # 100081  match.start()   # 3  match.end()     # 9  match.span()    # (3,9)</code></pre></li></ul></li><li><p>贪婪匹配（默认）：输出匹配最长的子串</p><pre><code class="lang-python">  match = re.search(r&#39;PY.*N&#39;, &#39;PYANBNCNDN&#39;)  match.group(0)  # &#39;PYANBNCNDN&#39;</code></pre></li><li><p>最小匹配: 输出最短匹配子串，操作符后增加<code>?</code>操作符</p><pre><code class="lang-python">  &#39;&#39;&#39;  只要长度输出可能不同的,都可以通过在操作符后增加?变成最小匹配  `*?`: 前一个字符0次或无限次扩展,最小匹配  `+?`: 前一个字符1次或无限次扩展,最小匹配  `??`: 前一个字符0次或1次扩展,最小匹配  `{m,n}?`: 扩展前一个字符m至n次(含n),最小匹配  &#39;&#39;&#39;  match = re.search(r&#39;PY.*?N&#39;, &#39;PYANBNCNDN&#39;)  match.group(0)  # &#39;PYAN&#39;</code></pre></li></ul><h2 id="header-10">文档解析之BeautifulSoup</h2><blockquote><p>一个网页解析库，处理高效，目前可支持<code>html</code>, <code>xml</code>,<code>html5</code>文档解析，可配置使用不同解析器</p></blockquote><p>常见解析器：</p><table class="table"><thead><tr><th style="text-align:left">解析器</th><th style="text-align:left">使用</th><th style="text-align:left">说明</th></tr></thead><tbody><tr><td style="text-align:left">html.parser</td><td style="text-align:left">BeautifulSoup(content,’html.parser’)</td><td style="text-align:left">Python内置标准库,速度适中，容错能力适中，不依赖扩展</td></tr><tr><td style="text-align:left">lxml</td><td style="text-align:left">BeautifulSoup(content,’lxml’),BeautifulSoup(content,’xml’)</td><td style="text-align:left">第三方库（<code>pip install lxml</code>），速度快(局部遍历），支持XML解析，容错能力强，依赖C扩展</td></tr><tr><td style="text-align:left">html5hib</td><td style="text-align:left">BeautifulSoup(content,’html5hib’)</td><td style="text-align:left">第三方库（<code>pip install html5hib</code>）,速度慢，以浏览器的方式解析生成HTML5格式的文档，容错能力最好，不依赖外部扩展</td></tr></tbody></table><ol><li><p>安装 （BeautifulSoup 包含在一个名为 bs4 的文件包中，需要另外安装）</p><pre><code class="lang-bash"> pip install bs4</code></pre></li><li><p>创建BeautifulSoup对象，结构化解析Dom树（<code>HTML/XML</code> &lt;=&gt; <code>文档树</code> &lt;=&gt; <code>BeautifulSoup对象</code>）</p><pre><code class="lang-python"> from bs4 import BeautifulSoup # soup = BeautifulSoup(&quot;&lt;html&gt;&lt;body&gt;&lt;p&gt;data&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&quot;,&#39;html.parser&#39;) soup = BeautifulSoup(&quot;&lt;html&gt;&lt;body&gt;&lt;p&gt;data&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&quot;) print(soup.p) # 格式化输出（为HTML文本及其内容增加添加`\n`），也可用于标签:`&lt;tag&gt;.prettify()` print(soup.prettify())</code></pre></li><li><p>访问节点</p><table class="table"><thead><tr><th style="text-align:left">BeautifulSoup基本元素</th><th style="text-align:left">说明</th><th style="text-align:left">使用</th><th style="text-align:left">示例</th></tr></thead><tbody><tr><td style="text-align:left">Tag</td><td style="text-align:left">标签</td><td style="text-align:left"><code>&lt;tag&gt;</code></td><td style="text-align:left"><code>soup.p</code></td></tr><tr><td style="text-align:left">Name</td><td style="text-align:left">标签名字，字符串类型</td><td style="text-align:left"><code>&lt;tag&gt;.name</code></td><td style="text-align:left"><code>soup.p.name</code></td></tr><tr><td style="text-align:left">Attributes</td><td style="text-align:left">标签的属性，字典形式组织</td><td style="text-align:left"><code>&lt;tag&gt;.attrs</code></td><td style="text-align:left"><code>soup.p.attrs</code>,<code>soup.p[&#39;attrname&#39;]</code></td></tr><tr><td style="text-align:left">NavigableString</td><td style="text-align:left">标签内非属性字符串(<code>&lt;&gt;⋯&lt;/&gt;</code>中字符串)</td><td style="text-align:left"><code>&lt;tag&gt;.string</code></td><td style="text-align:left"><code>soup.p.string</code></td></tr><tr><td style="text-align:left">Comment</td><td style="text-align:left">标签内字符串的注释部分, 特殊类型的 NavigableString 对象</td><td style="text-align:left">/</td><td style="text-align:left">/</td></tr></tbody></table><ul><li>是否有设置属性<ul><li><code>has_attr(&quot;attrname&quot;)</code></li></ul></li><li>获取属性<ul><li><code>.attrs[&quot;attrname&quot;]</code></li><li><code>[&quot;attrname&quot;]</code></li></ul></li><li>获取内容<ul><li><code>.text</code></li><li><code>.get_text()</code></li></ul></li><li><p><code>.string</code> vs <code>.text</code></p><ul><li><code>.string</code> on a Tag type object returns a <code>NavigableString</code> type object.</li><li><code>.text</code> gets all the child strings and return concatenated using the given separator. </li><li><p>sample:</p><table class="table"><thead><tr><th style="text-align:left">Html</th><th style="text-align:left">string</th><th style="text-align:left">text</th></tr></thead><tbody><tr><td style="text-align:left"><code>&lt;td&gt;some text&lt;/td&gt;</code></td><td style="text-align:left"><code>some text</code></td><td style="text-align:left"><code>some text</code></td></tr><tr><td style="text-align:left"><code>&lt;td&gt;&lt;/td&gt;</code></td><td style="text-align:left"><code>None</code></td><td style="text-align:left">/</td></tr><tr><td style="text-align:left"><code>&lt;td&gt;&lt;p&gt;more text&lt;/p&gt;&lt;/td&gt;</code></td><td style="text-align:left"><code>more text</code></td><td style="text-align:left"><code>more text</code></td></tr><tr><td style="text-align:left"><code>&lt;td&gt;even &lt;p&gt;more text&lt;/p&gt;&lt;/td&gt;</code></td><td style="text-align:left"><code>None</code> (因为文本数<code>&gt;=2</code>，<code>.string</code>不知道获取哪一个)</td><td style="text-align:left"><code>even more text</code> (<code>.text</code>返回的是，两段文本的拼接)</td></tr><tr><td style="text-align:left"><code>&lt;td&gt;&lt;!--This is comment--&gt;&lt;/td&gt;</code></td><td style="text-align:left"><code>This is comment</code></td><td style="text-align:left">/</td></tr><tr><td style="text-align:left"><code>&lt;td&gt;even &lt;!--This is comment--&gt;&lt;/td&gt;</code></td><td style="text-align:left"><code>None</code></td><td style="text-align:left"><code>even</code></td></tr></tbody></table></li></ul></li></ul></li><li><p>Navigating the Tree</p><ul><li>Going Down: 下行遍历(子节点和子孙节点)<ul><li><code>.contents</code> 返回儿子节点列表<code>list</code></li><li><code>.children</code> 返回儿子节点迭代类型<code>list_iterator</code></li><li><code>.descendants</code> 返回子孙节点迭代类型<code>generator</code> (包含所有子孙节点)</li></ul></li><li>Going up: 上行遍历(父节点和祖先节点)<ul><li><code>.parent</code> 返回节点的父亲节点</li><li><code>.parents</code> 返回所有先辈节点的迭代类型<code>generator</code> (包括soup本身)</li></ul></li><li>Going sideways: 平行遍历(兄弟节点)<ul><li><code>.next_sibling</code> / <code>.previous_sibling</code> 返回HTML文本顺序的下/上一个平行节点</li><li><code>.next_siblings</code> / <code>.previous_siblings</code> 返回HTML文本顺序的后/前续所有平行节点的迭代类型<code>generator</code></li></ul></li><li>Going back and forth: 前后遍历（不分层次） <ul><li><code>.next_element</code> / <code>.next_elements</code></li><li><code>.previous_element</code> / <code>.previous_elements</code></li></ul></li></ul></li><li><p>Searching the Tree</p><ul><li>Searching Down: 下行搜索（子孙）<ul><li><code>find</code>/<code>find_all</code></li></ul></li><li>Searching up: 上行搜索（父祖）<ul><li><code>find_parent</code> / <code>find_parents</code> </li></ul></li><li>Searching sideway: 平行搜索（兄弟）：<ul><li><code>find_next_sibling</code> / <code>find_previous_sibling</code></li><li><code>find_next_siblings</code> / <code>find_previous_siblings</code></li></ul></li><li>Searching back and forth: 前后搜索（不分层次的前/后节点）：<ul><li><code>find_next</code> / <code>find_all_next</code></li><li><code>find_previous</code> / <code>find_all_previous</code></li></ul></li><li>注： <ul><li>方法参数<code>(name=None,attrs={},recursive=True,text=None,limit=None,**kwargs)</code>，可以使用正则表达式<ul><li><code>name</code>: 标签名称</li><li><code>attrs</code>: 标签属性</li><li><code>recursive</code>: 是否对子孙全部检索,默认True</li><li><code>text</code>: 内容字符串</li><li><code>limit</code>: 限制条数</li></ul></li><li><code>&lt;tag&gt;(..)</code> 等价于 <code>&lt;tag&gt;.find_all(..)</code> </li><li><code>soup(..)</code> 等价于 <code>soup.find_all(..)</code></li><li>每个元素是一个 <code>bs4.element.Tag</code> 对象</li></ul></li></ul></li><li><p>可使用CSS Selectors选择节点: <code>.select(&#39;...&#39;)</code> </p><ul><li>基础选择：<ul><li><code>#id</code></li><li><code>tagName</code></li><li><code>.styleClass</code></li></ul></li><li>属性过滤:<ul><li><code>[attribute]</code></li><li><code>[attribute=value]</code></li><li><code>[attribute!=value]</code></li><li><code>[attribute^=value]</code></li><li><code>[attribute$=value]</code></li><li><code>[attribute*=value]</code></li></ul></li><li>层级选择:<ul><li><code>ancestor descendent</code></li><li><code>parent &gt; child</code></li><li><code>prev + next</code> (next sibling tag)</li><li><code>prev ~ siblings</code> (next all sibling tags)</li></ul></li><li>元素过滤:<ul><li><code>:not(selector)</code></li><li><code>:nth-of-type(index)</code></li><li><code>:nth-child(index)</code></li><li><code>:first-child</code></li><li><code>:last-child</code></li><li><code>:only-child</code></li></ul></li><li>内容过滤:<ul><li><code>:contains(text)</code></li><li><code>:empty</code></li><li><code>:has(selector)</code></li></ul></li><li>表单属性过滤:<ul><li><code>:enabled</code></li><li><code>:checked</code></li><li><code>:disabled</code></li></ul></li><li>混合:<ul><li><code>selector1, selector2, selectorN</code>：获取多个选择符的合集</li><li><code>[selector1][selector2][selectorN]</code>：匹配同时符合多个属性选择符的对象</li></ul></li></ul></li><li><p>注： </p><ul><li>BeautifulSoup用编码自动检测子库来识别当前文档编码并转换成<code>Unicode</code>编码，输出使用<code>utf-8</code>编码</li><li>获取属性值<code>.attrs</code>,<code>.attrs[&#39;xxx&#39;]</code></li><li>获取内容<code>.text</code>,<code>.get_text()</code>,<code>.string</code>,<code>.strings</code></li></ul></li></ol><h3 id="header-11">Demo: 访问节点</h3><pre><code class="lang-python">from bs4 import BeautifulSoupcontent=&#39;&#39;&#39;&lt;b&gt;Chat with sb&lt;/b&gt;&lt;a&gt; This is title  &lt;!-- Guess --&gt; &lt;/a&gt;&lt;i&gt;&lt;!--This is comment--&gt;&lt;/i&gt;&lt;div id=&quot;div1&quot;&gt;    &lt;div id=&quot;div2&quot;&gt;        &lt;p id=&quot;test&quot; class=&quot;highlight&quot;&gt;            Hello &lt;a&gt;Tom&lt;/a&gt;            Nice to meet you &lt;!-- This is a comment --&gt;        &lt;/p&gt;    &lt;/div&gt;&lt;/div&gt;&#39;&#39;&#39;soup=BeautifulSoup(content,&#39;html.parser&#39;)</code></pre><ol><li><p>Tag <code>name</code>,<code>attrs</code></p><pre><code class="lang-python"> print(&quot;soup.p:&quot;,soup.p) # &lt;p class=&quot;highlight&quot; id=&quot;test1&quot;&gt; #                 Hello &lt;a&gt;Tom&lt;/a&gt; #                 Nice to meet you &lt;!-- This is a comment --&gt; # &lt;/p&gt; print(&quot;soup.p.name:&quot;,soup.p.name) # p print(&quot;soup.p.attrs:&quot;,soup.p.attrs) # {&#39;id&#39;: &#39;test1&#39;, &#39;class&#39;: [&#39;highlight&#39;]} print(&quot;soup.p.attr[&#39;class&#39;]:&quot;,soup.p.attrs[&quot;class&quot;]) # [&#39;highlight&#39;] print(&quot;soup.p.attrs[&#39;id&#39;]:&quot;,soup.p.attrs[&quot;id&quot;]) #  test1 print(&quot;soup.p[&#39;class&#39;]:&quot;,soup.p[&quot;class&quot;]) #[&#39;highlight&#39;]</code></pre></li><li><p>Tag <code>text</code>/<code>string</code></p><pre><code class="lang-python"> print(&quot;soup.p.text:&quot;,soup.p.text) # #                Hello Tom #                Nice to meet you # print(&quot;soup.p.get_text():&quot;,soup.p.get_text()) # #                Hello Tom #                Nice to meet you # print(&quot;type(soup.p.get_text()):&quot;,type(soup.p.get_text()))   # &lt;class &#39;str&#39;&gt; print(&quot;-----------------------------------&quot;) print(&#39;--- Demo: Tag &lt;p&gt; string ---&#39;) print(&quot;soup.p.string:&quot;,soup.p.string)               # None print(&quot;type(soup.p.string)&quot;,type(soup.p.string))    # &lt;class &#39;NoneType&#39;&gt; print(&quot;soup.p.strings:&quot;,soup.p.strings)             # &lt;generator object Tag._all_strings at 0x00000000028FDD68&gt; for i,s in enumerate(soup.p.strings):     print(i,&quot;:&quot;,s) print(&quot;-----------------------------------&quot;)     # 0 : #                 Hello # 1 : Tom # 2 : #                 Nice to meet you # 3 : print(&#39;--- Demo: Tag &lt;a&gt; text/string ---&#39;) print(&quot;soup.a.text:&quot;,soup.a.text)                       # Chat with sb print(&quot;soup.a.string:&quot;,soup.a.string)                   # Chat with sb print(&quot;type(soup.a.string):&quot;,type(soup.a.string))       # &lt;class &#39;bs4.element.NavigableString&#39;&gt; print(&quot;-----------------------------------&quot;) print(&#39;--- Demo: Tag &lt;b&gt; text/string ---&#39;) print(&quot;soup.b.text:&quot;,soup.b.text)                       # This is title print(&quot;soup.b.string:&quot;,soup.b.string)                   # None print(&quot;type(soup.b.string):&quot;,type(soup.b.string))       # &lt;class &#39;NoneType&#39;&gt; print(&quot;-----------------------------------&quot;) print(&#39;--- Demo: Tag &lt;i&gt; text/string ---&#39;) print(&quot;soup.i.text:&quot;,soup.i.text)                       # print(&quot;soup.i.string:&quot;,soup.i.string)                   # This is comment print(&quot;type(soup.i.string):&quot;,type(soup.i.string))       # &lt;class &#39;bs4.element.Comment&#39;&gt;</code></pre></li></ol><h3 id="header-12">Demo: Navigating the Tree</h3><pre><code class="lang-python">from bs4 import BeautifulSoupcontent=&#39;&#39;&#39;&lt;b&gt;Chat with sb&lt;/b&gt;&lt;a&gt; This is title  &lt;!-- Guess --&gt; &lt;/a&gt;&lt;i&gt;&lt;!--This is comment--&gt;&lt;/i&gt;&lt;div id=&quot;div1&quot;&gt;    &lt;div id=&quot;div2&quot;&gt;        &lt;p id=&quot;test&quot; class=&quot;highlight&quot;&gt;            Hello &lt;a&gt;Tom&lt;/a&gt;            Nice to meet you &lt;!-- This is a comment --&gt;        &lt;/p&gt;    &lt;/div&gt;&lt;/div&gt;&#39;&#39;&#39;soup=BeautifulSoup(content,&#39;html.parser&#39;)def print_result(result):    if type(result)==element.Tag or (type(result)== list and len(result)==0):        print(result)        return    for i,r in enumerate(result):        print(i,&quot;:&quot;,r)    print(&#39;-------------------------&#39;)def print_result_name(result):    if type(result)==element.Tag or type(result)==element.NavigableString or (type(result)== list and len(result)==0):        print(result)        return    for i,r in enumerate(result):        print(i,&quot;:&quot;,r.name)    print(&#39;-------------------------&#39;)</code></pre><ol><li><p>Going down:</p><ul><li><p><code>.contents</code></p><pre><code class="lang-python">  print(soup.p.contents)  # &lt;class &#39;list&#39;&gt;  print_result(soup.p.contents)  # 0 :  #                 Hello  # 1 : &lt;a&gt;Tom&lt;/a&gt;  # 2 :  #                 Nice to meet you  # 3 :  This is a comment  # 4 :</code></pre></li><li><p><code>.children</code>   </p><pre><code class="lang-python">  print(soup.p.children)            # &lt;list_iterator object at 0x0000000001E742E8&gt;  print_result(soup.p.children)  # 0 :  #                 Hello  # 1 : &lt;a&gt;Tom&lt;/a&gt;  # 2 :  #                 Nice to meet you  # 3 :  This is a comment  # 4 :</code></pre></li><li><p><code>.descendants</code></p><pre><code class="lang-python">  print(&#39;--- Demo: Tag &lt;p&gt; descendants ---&#39;)  print(soup.p.descendants)  # &lt;generator object Tag.descendants at 0x00000000028ADD68&gt;  print_result(soup.p.descendants)  # 0 :  #                 Hello  # 1 : &lt;a&gt;Tom&lt;/a&gt;  # 2 : Tom  # 3 :  #                 Nice to meet you  # 4 :  This is a comment  # 5 :</code></pre></li></ul></li><li><p>Going up:</p><ul><li><p><code>.parent</code></p><pre><code class="lang-python">  print(type(soup.p.parent))        # &lt;class &#39;bs4.element.Tag&#39;&gt;  print_result(soup.p.parent)  # &lt;div id=&quot;div2&quot;&gt;  # &lt;p class=&quot;highlight&quot; id=&quot;test1&quot;&gt;  #                 Hello &lt;a&gt;Tom&lt;/a&gt;  #                 Nice to meet you &lt;!-- This is a comment --&gt;  # &lt;/p&gt;  # &lt;p class=&quot;story&quot; id=&quot;test2&quot;&gt;Story1&lt;/p&gt;  # &lt;p class=&quot;story&quot; id=&quot;test3&quot;&gt;Story2&lt;/p&gt;  # &lt;/div&gt;</code></pre></li><li><p><code>.parents</code></p><pre><code class="lang-python">  print(soup.p.parents)             # &lt;generator object PageElement.parents at 0x00000000028FDD68&gt;  print_result_name(soup.p.parents)  # 0 : div  # 1 : div  # 2 : [document]</code></pre></li></ul></li><li><p>Going sideway:</p><ul><li><code>next_sibling</code><pre><code class="lang-python">  print_result(soup.p.next_sibling)  # 0 :</code></pre></li><li><p><code>next_siblings</code></p><pre><code class="lang-python">  print(soup.p.next_siblings)       # &lt;generator object PageElement.next_siblings at 0x00000000028FDD68&gt;  print_result(soup.p.next_siblings)  # 0 :  #   # 1 : &lt;p class=&quot;story&quot; id=&quot;test2&quot;&gt;Story1&lt;/p&gt;  # 2 :  #   # 3 : &lt;p class=&quot;story&quot; id=&quot;test3&quot;&gt;Story2&lt;/p&gt;  # 4 :</code></pre></li><li>vs. <code>find_next_silbings</code><pre><code class="lang-python">  print(&#39;--- Demo: `find_next_siblings()` ---&#39;)  result=soup.p.find_next_siblings()  print_result(result)  # 0 : &lt;p class=&quot;story&quot; id=&quot;test2&quot;&gt;Story1&lt;/p&gt;  # 1 : &lt;p class=&quot;story&quot; id=&quot;test3&quot;&gt;Story2&lt;/p&gt;</code></pre></li></ul></li><li><p>Going forth and back:</p><ul><li><code>next_element</code><pre><code class="lang-python">  print(soup.p.next_element)  #  # Hello  print(type(soup.p.next_element))  # &lt;class &#39;bs4.element.NavigableString&#39;&gt;</code></pre></li><li><p><code>next_elements</code></p><pre><code class="lang-python">  print(soup.p.next_elements)       # &lt;generator object PageElement.next_elements at 0x00000000028FDD68&gt;  print_result(soup.p.next_elements)  # 0 :  #                 Hello  # 1 : &lt;a&gt;Tom&lt;/a&gt;  # 2 : Tom  # 3 :  #                 Nice to meet you  # 4 :  This is a comment  # 5 :  #   # 6 :  #   # 7 : &lt;p class=&quot;story&quot; id=&quot;test2&quot;&gt;Story1&lt;/p&gt;  # 8 : Story1  # 9 :  #   # 10 : &lt;p class=&quot;story&quot; id=&quot;test3&quot;&gt;Story2&lt;/p&gt;  # 11 : Story2  # 12 :  #   # 13 :  #   # 14 :</code></pre></li><li>vs. <code>find_all_next()</code><pre><code class="lang-python">  result=soup.p.find_all_next()  print_result(result)  # 0 : &lt;a&gt;Tom&lt;/a&gt;  # 1 : &lt;p class=&quot;story&quot; id=&quot;test2&quot;&gt;Story1&lt;/p&gt;  # 2 : &lt;p class=&quot;story&quot; id=&quot;test3&quot;&gt;Story2&lt;/p&gt;</code></pre></li></ul></li></ol><h3 id="header-13">Demo: Searching the Tree</h3><pre><code class="lang-python">from bs4 import BeautifulSoupfrom bs4 import elementimport recontent=&#39;&#39;&#39;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;&lt;/head&gt; &lt;body&gt;&lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;&#39;&#39;&#39;soup=BeautifulSoup(content,&#39;html.parser&#39;)print(soup.prettify())def print_result(result):    if type(result)==element.Tag or (type(result)== list and len(result)==0):        print(result)        return    for i,r in enumerate(result):        print(i,&quot;:&quot;,r)    print(&#39;-------------------------&#39;)def print_result_name(result):    if type(result)==element.Tag or (type(result)== list and len(result)==0):        print(result)        return    for i,r in enumerate(result):        print(i,&quot;:&quot;,r.name)    print(&#39;-------------------------&#39;)</code></pre><ol><li><p>Searching down</p><ul><li><p>by <code>name</code></p><pre><code class="lang-python">  print(&#39;--- Demo: `find_all(&quot;a&quot;)` ---&#39;)  result=soup.find_all(&#39;a&#39;)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;  # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;  # 2 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;  print(&#39;--- Demo: `find_all([&quot;a&quot;,&quot;title&quot;])` ---&#39;)  result=soup.find_all([&#39;a&#39;,&#39;title&#39;])  print_result(result)  # 0 : &lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;  # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;  # 2 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;  # 3 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;  print(&#39;--- Demo: `find_all(True)` ---&#39;)  result=soup.find_all(True)  print_result(result)  # 0 : &lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;&lt;/head&gt; &lt;body&gt;  # &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;  # &lt;p class=&quot;story&quot;&gt;  # Once upon a time there were three little sisters; and their names were  # &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,  # &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well.  # &lt;/p&gt;  # &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;  # &lt;/body&gt;  # &lt;/html&gt;  # 1 : &lt;head&gt;&lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;&lt;/head&gt;  # 2 : &lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;  # 3 : &lt;body&gt;  # &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;  # &lt;p class=&quot;story&quot;&gt;  # Once upon a time there were three little sisters; and their names were  # &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,  # &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well.  # &lt;/p&gt;  # &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;  # &lt;/body&gt;  # 4 : &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;  # 5 : &lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;  # 6 : &lt;p class=&quot;story&quot;&gt;  # Once upon a time there were three little sisters; and their names were  # &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,  # &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well.  # &lt;/p&gt;  # 7 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;  # 8 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;  # 9 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;  # 10 : &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;  print(&#39;--- Demo: `find_all(re.compile(&quot;b&quot;)` ---&#39;)  result=soup.find_all(re.compile(&#39;b&#39;))  print_result(result)  # 0 : &lt;body&gt;  # &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;  # &lt;p class=&quot;story&quot;&gt;  # Once upon a time there were three little sisters; and their names were  # &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,  # &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well.  # &lt;/p&gt;  # &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;  # &lt;/body&gt;  # 1 : &lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;</code></pre></li><li><p>by <code>attrs</code></p><pre><code class="lang-python">  print(&#39;--- Demo: find_all(&quot;p&quot;,&quot;story&quot;) ---&#39;)  result=soup.find_all(&#39;p&#39;,&#39;story&#39;)  print_result(result)  # 0 : &lt;p class=&quot;story&quot;&gt;  # Once upon a time there were three little sisters; and their names were  # &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,  # &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well.  # &lt;/p&gt;  # 1 : &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;  print(&#39;--- Demo: find_all(id=&quot;link1&quot;) ---&#39;)  result=soup.find_all(id=&#39;link1&#39;)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;  print(&#39;--- Demo: find_all(class_=&quot;sister&quot;) ---&#39;)  result=soup.find_all(class_=&#39;sister&#39;)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;  # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;  # 2 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;  print(&#39;--- Demo: find_all(re.compile(&quot;link&quot;)) ---&#39;)  result=soup.find_all(id=re.compile(&#39;link&#39;))  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;  # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;  # 2 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;  print(&#39;--- Demo: find_all(attrs={&quot;class&quot;:&quot;story&quot;}) ---&#39;)  result=soup.find_all(attrs={&#39;class&#39;:&#39;story&#39;})  print_result(result)  # 0 : &lt;p class=&quot;story&quot;&gt;  # Once upon a time there were three little sisters; and their names were  # &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,  # &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well.  # &lt;/p&gt;  # 1 : &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;</code></pre></li><li><p>by <code>recursive</code></p><pre><code class="lang-python">  print(&#39;--- Demo: find_all(&quot;a&quot;) ---&#39;)  result=soup.find_all(&#39;a&#39;)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;  # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;  # 2 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;  print(&#39;--- Demo: find_all(&quot;a&quot;,recursive=False) ---&#39;)  result=soup.find_all(&#39;a&#39;,recursive=False)  print_result(result)  # []</code></pre></li><li><p>by <code>string/text</code></p><pre><code class="lang-python">  print(&#39;--- Demo: find_all(string=&quot;three&quot;) ---&#39;)  result=soup.find_all(string=&#39;three&#39;)  print_result(result)  # []  print(&#39;--- Demo: find_all(string=re.compile(&quot;e&quot;)) ---&#39;)  result=soup.find_all(string=re.compile(&#39;e&#39;))  print_result(result)  # 0 : The Dormouse&#39;s story  # 1 : The Dormouse&#39;s story  # 2 :  # Once upon a time there were three little sisters; and their names were  #  # 3 : Elsie  # 4 : Lacie  # 5 : Tillie  # 6 : ; and they lived at the bottom of a well.</code></pre></li><li>by <code>limit</code> : <code>find()</code>也就是当<code>limit=1</code>时的<code>find_all()</code><pre><code class="lang-python">  print(&#39;--- Demo: find_all(&quot;a&quot;,limit-2) ---&#39;)  result=soup.find_all(&#39;a&#39;,limit=2)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;  # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;</code></pre></li><li><p>by <code>self def function</code></p><pre><code class="lang-python">  print(&#39;--- Demo: using `self def function` ---&#39;)  def my_filter(tag):      return tag.has_attr(&#39;id&#39;) and re.match(&#39;link&#39;,tag.get(&quot;id&quot;))  result=soup.find_all(my_filter)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;  # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;  # 2 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;</code></pre></li></ul></li><li><p>Searching up: <code>find_parents</code></p><pre><code class="lang-python"> print(&#39;--- Demo: link2.`find_parents()` ---&#39;) result=soup.find(id=&quot;link2&quot;).find_parents() print_result_name(result) # 0 : p # 1 : body # 2 : html # 3 : [document] print(&#39;--- Demo: link2.`find_parents(&quot;p&quot;)` ---&#39;) result=soup.find(id=&quot;link2&quot;).find_parents(&#39;p&#39;) print_result(result) # 0 : &lt;p class=&quot;story&quot;&gt; # Once upon a time there were three little sisters; and their names were # &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;, # &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well. # &lt;/p&gt;</code></pre></li><li><p>Searching sideway: <code>find_next_siblings</code></p><pre><code class="lang-python"> print(&#39;--- Demo: `find_next_siblings()` ---&#39;) result=soup.find(id=&quot;link1&quot;).find_next_siblings() print_result(result) # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;</code></pre></li><li><p>Searching forth and back: <code>find_all_next</code></p><pre><code class="lang-python"> print(&#39;--- Demo: `find_all_next()` ---&#39;) result=soup.find(id=&quot;link1&quot;).find_all_next() print_result(result) # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; # 2 : &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;</code></pre></li></ol><h3 id="header-14">Demo: CSS Selectors</h3><pre><code class="lang-html">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;&lt;/head&gt; &lt;body&gt;&lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&lt;input type=&quot;text&quot; disabled value=&quot;input something&quot;&gt;&lt;/input&gt;&lt;/body&gt;&lt;/html&gt;</code></pre><ol><li><p>基础选择</p><ul><li><p><code>#id</code></p><pre><code class="lang-python">  print(&#39;--- Demo: `select(&quot;#link1&quot;)` ---&#39;)  result=soup.select(&quot;#link1&quot;)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;  print(&#39;--- Demo: `select(&quot;a#link1&quot;)` ---&#39;)  result=soup.select(&quot;a#link2&quot;)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;</code></pre></li><li><code>tagName</code><pre><code class="lang-python">  print(&#39;--- Demo: `select(&quot;title&quot;)` ---&#39;)  result=soup.select(&quot;title&quot;)  print_result(result)  # 0 : &lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;</code></pre></li><li><code>.styleClass</code><pre><code class="lang-python">  print(&#39;--- Demo: `select(&quot;.sister&quot;)` ---&#39;)  result=soup.select(&quot;.sister&quot;)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;  # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;  # 2 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;</code></pre></li></ul></li><li><p>属性过滤</p><ul><li><p><code>[attribute]</code></p><pre><code class="lang-python">  print(&#39;--- Demo: `select(&quot;a[href]&quot;)` ---&#39;)  result=soup.select(&#39;a[href]&#39;)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;  # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;  # 2 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;</code></pre></li><li><p><code>[attribute=value]</code></p><pre><code class="lang-python">  print(&#39;--- Demo: `select(&quot;[class=sister]&quot;)` ---&#39;)  result=soup.select(&quot;[class=sister]&quot;)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;  # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;  # 2 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;</code></pre></li><li><code>[attribute^=value]</code><pre><code class="lang-python">  print(&#39;--- Demo: `select(&quot;a[href^=&quot;http://example.com/&quot;]&quot;)` ---&#39;)  result=soup.select(&#39;a[href^=&quot;http://example.com/&quot;]&#39;)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;  # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;  # 2 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;</code></pre></li><li><code>[attribute$=value]</code><pre><code class="lang-python">  print(&#39;--- Demo: `select(&quot;a[href$=&quot;tillie&quot;])` ---&#39;)  result=soup.select(&#39;a[href$=&quot;tillie&quot;]&#39;)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;</code></pre>-<code>[attribute*=value]</code><pre><code class="lang-python">  print(&#39;--- Demo: `select(&quot;a[href*=&quot;.com/el&quot;]&quot;)` ---&#39;)  result=soup.select(&#39;a[href*=&quot;.com/el&quot;]&#39;)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;</code></pre></li><li><code>[selector1][selector2][selectorN]</code><pre><code class="lang-python">  print(&quot;--- Demo: `[class=&#39;sister&#39;][id=link2]` --- &quot;)  print_result(soup.select(&quot;[class=sister][id=link2]&quot;))  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;</code></pre></li></ul></li><li><p>层级选择</p><ul><li><code>ancestor descendent</code><pre><code class="lang-python">  print(&#39;--- Demo: `select(&quot;body a&quot;)` ---&#39;)  result=soup.select(&quot;body a&quot;)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;  # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;  # 2 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;</code></pre></li><li><p><code>parent &gt; child</code></p><pre><code class="lang-python">  print(&#39;--- Demo: `select(&quot;body &gt; a&quot;) ---&#39;)  result=soup.select(&quot;body &gt; a&quot;)  print_result(result)  # []  print(&#39;--- Demo: `select(&quot;p &gt; a&quot;) ---&#39;)  result=soup.select(&quot;p &gt; a&quot;)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;  # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;  # 2 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;  print(&#39;--- Demo: `select(&quot;p &gt; a:nth-of-type(2)&quot;)` ---&#39;)  result=soup.select(&quot;p &gt; a:nth-of-type(2)&quot;)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;  print(&#39;--- Demo: `select(&quot;p &gt; #link1&quot;)` ---&#39;)  result=soup.select(&quot;p &gt; #link1&quot;)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;</code></pre></li><li><code>prev + next</code> ：next sibling tag<pre><code class="lang-python">  print(&#39;--- Demo: `select(&quot;#link1 ~ .sister&quot;)` ---&#39;)  result=soup.select(&quot;#link1 ~ .sister&quot;)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;  # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;</code></pre></li><li><code>prev ~ siblings</code> ：next all sibling tags<pre><code class="lang-python">  print(&#39;--- Demo: `select(&quot;#link1 + .sister&quot;)` ---&#39;)  result=soup.select(&quot;#link1 + .sister&quot;)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;</code></pre></li></ul></li><li><p>元素过滤</p><ul><li><code>:not(selector)</code><pre><code class="lang-python">  print(&quot;--- Demo: `:not(.story)` --- &quot;)  print_result(soup.select(&quot;p:not(.story)&quot;))  # 0 : &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;</code></pre></li><li><code>:nth-of-type(index)</code><pre><code class="lang-python">  print(&#39;--- Demo: `select(&quot;p:nth-of-type(3)&quot;)` ---&#39;)  result=soup.select(&quot;p:nth-of-type(3)&quot;)  print_result(result)  # 0 : &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;</code></pre></li><li><code>:nth-child(index)</code><pre><code class="lang-python">  print(&quot;--- Demo: `p &gt; :nth-child(1)` --- &quot;)  print_result(soup.select(&quot;p &gt; :nth-child(1)&quot;))  # 0 : &lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;  # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;</code></pre></li><li><code>:first-child</code><pre><code class="lang-python">  print(&quot;--- Demo: `p &gt; :first-child` --- &quot;)  print_result(soup.select(&quot;p &gt; :first-child&quot;))  # 0 : &lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;  # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;</code></pre></li><li><code>:last-child</code><pre><code class="lang-python">  print(&quot;--- Demo: `p &gt; :last-child` --- &quot;)  print_result(soup.select(&quot;p &gt; :last-child&quot;))  # 0 : &lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;  # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;</code></pre></li><li><code>:only-child</code><pre><code class="lang-python">  print(&quot;--- Demo: `p &gt; :only-child` --- &quot;)  print_result(soup.select(&quot;p &gt; :only-child&quot;))  # 0 : &lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;</code></pre></li></ul></li><li><p>内容过滤</p><ul><li><code>:contains(text)</code><pre><code class="lang-python">  print(&quot;--- Demo: `p:contains(story)` --- &quot;)  print_result(soup.select(&quot;p:contains(story)&quot;))  # 0 : &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;</code></pre></li><li><code>:empty</code><pre><code class="lang-python">  print(&quot;--- Demo: `p:empty` --- &quot;)  print_result(soup.select(&quot;p:empty&quot;))  # []</code></pre></li><li><code>:has(selector)</code><pre><code class="lang-python">  print(&quot;--- Demo: `p:has(b)` --- &quot;)  print_result(soup.select(&quot;p:has(b)&quot;))  # 0 : &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;</code></pre></li></ul></li><li><p>表单属性过滤</p><ul><li><code>:enabled</code>,<code>:disabled</code>,<code>:checked</code><pre><code class="lang-python">  print(&quot;--- Demo: `:disabled`` --- &quot;)  print_result(soup.select(&quot;:disabled&quot;))  # 0 : &lt;input disabled=&quot;&quot; type=&quot;text&quot; value=&quot;input something&quot;/&gt;</code></pre></li></ul></li><li><p>其他：</p><ul><li><code>selector1, selector2, selectorN</code><pre><code class="lang-python">  print(&#39;--- Demo: `select(&quot;#link1,#link2&quot;)` ---&#39;)  result=soup.select(&quot;#link1,#link2&quot;)  print_result(result)  # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;  # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;</code></pre></li><li><code>select_one()</code><pre><code class="lang-python">  print(&#39;--- Demo: `select_one(&quot;.sister&quot;)` ---&#39;)  result=soup.select_one(&quot;.sister&quot;)  print_result(result)  # &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;</code></pre></li></ul></li><li><p>get attribute value:</p><pre><code class="lang-python"> print(&#39;--- Demo: `get attribute value` ---&#39;) result=soup.select(&quot;.sister&quot;) print_result(result) # 0 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt; # 1 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; # 2 : &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt; print(result[0].get_text()) #Elsie print(result[0].attrs) #{&#39;href&#39;: &#39;http://example.com/elsie&#39;, &#39;class&#39;: [&#39;sister&#39;], &#39;id&#39;: &#39;link1&#39;} print(result[0].attrs[&#39;id&#39;]) #link1</code></pre></li></ol><h2 id="header-15">文档解析之XPath</h2><ul><li>使用路径表达式来选取<code>XML/HTML</code>文档中的节点或节点集</li><li>安装: <code>pip install lxml</code></li><li>导入：<code>from lxml import etree</code></li><li>注意：<code>lxml</code>和正则一样用<code>C</code>实现，是一款高性能的Python<code>HTML/XML</code>解析器，可以利用<code>XPath</code>语法来快速的定位特定元素以及节点信息</li><li>Tools: Chrome的插件<code>XPath Helper</code>，快速得到页面元素的匹配规则</li></ul><h3 id="header-16">路径表达式</h3><ul><li><p><code>//</code> : 选取所有的当前节点，不考虑他们的位置</p><ul><li><code>//p</code> (<code>.//p</code>)</li><li><code>/p//a</code></li><li><code>//p/a</code></li></ul></li><li><p><code>/</code> : 从根节点选取</p><ul><li><code>/p</code> (<code>./p</code>)</li><li><code>/p/a</code></li></ul></li><li><p><code>.</code> 当前节点,<code>..</code> : 当前节点的父节点</p><ul><li><code>./p</code> </li><li><code>../p</code></li><li><code>//p/b/../a</code></li><li><code>root.xpath(&#39;//p/b&#39;).xpath(&#39;./a&#39;)</code></li><li><code>root.xpath(&#39;//p/b&#39;).xpath(&#39;../text()&#39;)</code></li><li><code>root.xpath(&#39;//p/b/..//a&#39;)[0].text</code></li></ul></li><li><p><code>@</code> : 选取属性</p><ul><li><code>//@class</code></li><li><code>//p/@class</code></li><li><code>//p//@class</code></li><li><code>//p[@class]</code></li><li><code>//p[@class=&#39;s1&#39;]</code></li><li><code>//p[@class=&#39;s1&#39;]/@class</code></li></ul></li><li><p><code>/text()</code>,<code>string(.)</code> 选择内容</p><ul><li><code>&quot;//b/text()&quot;</code></li><li><code>//b//text()</code></li><li><code>string(.)</code></li><li><code>string(./description)</code></li></ul></li><li><p><code>[]</code>: Predicates</p><ul><li><code>//p[1]</code>,<code>//p[last()]</code>,<code>//p[last()-1]</code></li><li><code>//p[position()&lt;=2]</code></li><li><code>//p[@class]</code>,<code>//p[@class=&#39;s1&#39;]</code></li><li><code>//p[b]</code>,<code>//p[b/@class]</code>,<code>//p[b[@class=&#39;s1&#39;]]</code></li></ul></li><li><p><code>*</code> : 通配符，匹配任何 </p><ul><li><code>//p/*</code></li><li><code>//p//*</code></li><li><code>//p/*/a</code></li><li><code>//p[@*]</code></li><li><code>//*[@class=&#39;s1&#39;]</code></li></ul></li><li><p><code>|</code> : 选取多个路径</p><ul><li><code>/p | //b</code></li><li><code>//p/a | //p/b[@class]</code></li></ul></li><li><p><code>and</code>,<code>or</code>,<code>not</code>:</p><ul><li><code>//a[@class=&#39;sister&#39; and @id=&#39;link2&#39;]</code>,<code>//a[@class=&#39;sister&#39;][@id=&#39;link2&#39;]</code></li><li><code>//a[@id=&#39;link1&#39; or @class=&#39;outAstyle&#39;]</code></li><li><code>//a[not(@class=&#39;sister&#39;)]</code></li><li><code>//a[not(@class=&#39;sister&#39;) and @class or @id=&#39;link1&#39;]</code></li></ul></li><li><p><code>xxx()</code>: </p><ul><li><code>starts-with()</code>: <code>//a[starts-with(@href,&#39;http://example.com/&#39;)]</code></li><li><code>contains()</code>: <code>//a[contains(text(),&#39;ie&#39;) and contains(@id,&#39;link&#39;)]</code></li><li><code>text()</code>: <code>//b/text()</code>,<code>//b//text()</code></li><li><code>string(.)</code>: <code>data.xpath(&#39;//div[@class=&quot;name&quot;]&#39;)[0].xpath(&#39;string(.)&#39;)</code></li></ul></li><li><p><code>::</code></p><ul><li>go self: <code>self::</code>,eg: <code>//self::b</code></li><li>go up: <code>ancestor::</code> , <code>ancestor-or-self::</code>,<code>parent::</code>,eg: <code>//a/ancestor::p</code></li><li>go down: <code>descendant::</code>,<code>child::</code>,eg: <code>//p/descendant::a[not(@class)]</code></li><li>go forward: <code>following::</code>,<code>following-sibling::</code>, eg: <code>p[last()-1]/following::*</code></li><li>go back: <code>preceding::</code>,<code>preceding-sibling::</code> , eg: <code>p[2]/preceding::*</code></li><li>get attributes: <code>attribute::</code>,eg: <code>//a/attribute::*</code>,<code>//a/attribute::class</code></li></ul></li><li><p><code>lxml.etree._Element</code>:</p><ul><li><code>tag</code></li><li><code>attrib</code></li><li><code>text</code></li><li><code>.xpath(&#39;string(.)&#39;)</code></li><li><code>.get(&#39;attribute&#39;)</code></li></ul></li></ul><h3 id="header-17">Demo: 解析HTML</h3><pre><code class="lang-python">#!/usr/bin/env python# -*- coding:utf-8 -*-from lxml import etreecontent=&#39;&#39;&#39;&lt;div&gt;    &lt;p class=&quot;title&quot;&gt;&lt;b class=&#39;bstyle&#39;&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;    &lt;p class=&quot;story&quot;&gt;        Once upon a time there were three little sisters; and their names were        &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,        &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;         and &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;        ; and they lived at the bottom of a well.        &lt;p&gt; hello ...&lt;b&gt;&lt;a&gt; World &lt;/a&gt;&lt;/b&gt; &lt;/p&gt;    &lt;/p&gt;    &lt;p class=&quot;story&quot;&gt;...&lt;a class=&quot;outAstyle&quot;&gt;Miss&lt;/a&gt; &lt;/p&gt;&lt;/div&gt;&#39;&#39;&#39;# html = etree.parse(&#39;./test.html&#39;,etree.HTMLParser())html = etree.HTML(content)print(html)# &lt;Element html at 0x1019312c8&gt;# result = etree.tostring(html)     # 会补全缺胳膊少腿的标签# print(result.decode(&quot;utf-8&quot;))print(etree.tounicode(html))        # 会补全缺胳膊少腿的标签# &lt;html&gt;&lt;body&gt;&lt;div&gt;#   &lt;p class=&quot;title&quot;&gt;&lt;b class=&quot;bstyle&quot;&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;#   &lt;p class=&quot;story&quot;&gt;#       Once upon a time there were three little sisters; and their names were#       &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,#       &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;#       and &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;#       ; and they lived at the bottom of a well.#       &lt;/p&gt;&lt;p&gt; hello ...&lt;b&gt;&lt;a&gt; World &lt;/a&gt;&lt;/b&gt; &lt;/p&gt;#   &lt;p class=&quot;story&quot;&gt;...&lt;a class=&quot;outAstyle&quot;&gt;Miss&lt;/a&gt; &lt;/p&gt;# &lt;/div&gt;# &lt;/body&gt;&lt;/html&gt;result=html.xpath(&quot;//p/b&quot;)for i,r in enumerate(result):    print(i,type(r),&quot;:&quot;,r.tag,r.attrib,r.get(&#39;class&#39;),r.text,r.xpath(&#39;string(.)&#39;))# 0 &lt;class &#39;lxml.etree._Element&#39;&gt; : b {&#39;class&#39;: &#39;bstyle&#39;} bstyle The Dormouse&#39;s story The Dormouse&#39;s story# 1 &lt;class &#39;lxml.etree._Element&#39;&gt; : b {} None None  World############################ More test:test_path_any(html)test_path_attr(html)test_path_predicates(html)def test_path_any(root):    print(&quot;--- `//` ----&quot;)    do_xpath(root,&#39;p&#39;)    # []    do_xpath(root,&#39;//p&#39;)    # [&lt;Element p at 0x109f34148&gt;, &lt;Element p at 0x109f34188&gt;, &lt;Element p at 0x109f34248&gt;, &lt;Element p at 0x109f34288&gt;]    do_xpath(root,&#39;//p/a/text()&#39;)    # [&#39;Elsie&#39;, &#39;Lacie&#39;, &#39;Tillie&#39;, &#39;Miss&#39;]    do_xpath(root,&#39;//p//a/text()&#39;)    # [&#39;Elsie&#39;, &#39;Lacie&#39;, &#39;Tillie&#39;, &#39; World &#39;, &#39;Miss&#39;]    do_xpath(root,&#39;.//a/text()&#39;)    # [&#39;Elsie&#39;, &#39;Lacie&#39;, &#39;Tillie&#39;, &#39; World &#39;, &#39;Miss&#39;]    print(&#39;--- `xpath` ---&#39;)    print(root.xpath(&quot;//p/b//a&quot;))    # [&lt;Element a at 0x10b555f08&gt;]    print(root.xpath(&quot;//p/b&quot;)[1].xpath(&quot;//a&quot;))    # [&lt;Element a at 0x10b555f08&gt;, &lt;Element a at 0x10b5770c8&gt;, &lt;Element a at 0x10b577108&gt;, &lt;Element a at 0x10b577048&gt;, &lt;Element a at 0x10b577088&gt;]    print(root.xpath(&quot;//p/b&quot;)[1].xpath(&quot;./a&quot;))    # [&lt;Element a at 0x10c719f48&gt;]    print(root.xpath(&quot;//p/b&quot;)[1].xpath(&quot;../text()&quot;))    # [&#39; hello ...&#39;, &#39; &#39;]    print(root.xpath(&#39;//p/b/..//a&#39;)[0].text)    # World    print(&#39;------------------------&#39;)def test_path_attr(root):    print(&quot;--- `@` ----&quot;)    do_xpath(root,&#39;/@class&#39;)    # []    do_xpath(root,&#39;//@class&#39;)    # [&#39;title&#39;, &#39;bstyle&#39;, &#39;story&#39;, &#39;sister&#39;, &#39;sister&#39;, &#39;sister&#39;, &#39;story&#39;, &#39;outAstyle&#39;]    do_xpath(root,&#39;//p[@class]&#39;)    # [&lt;Element p at 0x10e4c3888&gt;, &lt;Element p at 0x10e4c36c8&gt;, &lt;Element p at 0x10e4c3708&gt;]    do_xpath(root,&quot;//p[@class=&#39;story&#39;]&quot;)    # [&lt;Element p at 0x110ba8708&gt;, &lt;Element p at 0x110ba8548&gt;]    do_xpath(root,&quot;//p/@class&quot;)    # [&#39;title&#39;, &#39;story&#39;, &#39;story&#39;]    do_xpath(root,&quot;//p[@class=&#39;story&#39;]/@class&quot;)    # [&#39;story&#39;, &#39;story&#39;]    do_xpath(root,&quot;//p[@class=&#39;story&#39;]//@class&quot;)    # [&#39;story&#39;, &#39;sister&#39;, &#39;sister&#39;, &#39;sister&#39;, &#39;story&#39;, &#39;outAstyle&#39;]    print(&#39;------------------------&#39;)def test_path_predicates(root):    print(&quot;--- `[]` ----&quot;)    do_xpath_detail(root,&#39;//p[1]&#39;)    # 0 : &lt;p class=&quot;title&quot;&gt;&lt;b class=&quot;bstyle&quot;&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;    do_xpath_detail(root,&#39;//p[last()]&#39;)    # 0 : &lt;p class=&quot;story&quot;&gt;...&lt;a class=&quot;outAstyle&quot;&gt;Miss&lt;/a&gt; &lt;/p&gt;    do_xpath_detail(root,&#39;//p[last()-1]&#39;)    # 0 : &lt;p&gt; hello ...&lt;b&gt;&lt;a&gt; World &lt;/a&gt;&lt;/b&gt; &lt;/p&gt;    do_xpath_detail(root,&#39;//a[1]&#39;)    # 0 : &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,    # 1 : &lt;a&gt; World &lt;/a&gt;    # 2 : &lt;a class=&quot;outAstyle&quot;&gt;Miss&lt;/a&gt;    do_xpath_detail(root,&#39;//p/a[1]&#39;)    # 0 : &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,    # 1 : &lt;a class=&quot;outAstyle&quot;&gt;Miss&lt;/a&gt;    do_xpath_detail(root,&#39;//a[position()&lt;=2]&#39;)    # 0 : &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,    # 1 : &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;and    # 2 : &lt;a&gt; World &lt;/a&gt;    # 3 : &lt;a class=&quot;outAstyle&quot;&gt;Miss&lt;/a&gt;    do_xpath_detail(root,&#39;//a[@class]&#39;)    # 0 : &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,    # 1 : &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;and    # 2 : &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well.    # 3 : &lt;a class=&quot;outAstyle&quot;&gt;Miss&lt;/a&gt;    do_xpath_detail(root,&#39;//a[@class=&quot;outAstyle&quot;]&#39;)    # 0 : &lt;a class=&quot;outAstyle&quot;&gt;Miss&lt;/a&gt;    do_xpath_detail(root,&#39;//p[b]&#39;)    # 0 : &lt;p class=&quot;title&quot;&gt;&lt;b class=&quot;bstyle&quot;&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;    # 1 : &lt;p&gt; hello ...&lt;b&gt;&lt;a&gt; World &lt;/a&gt;&lt;/b&gt; &lt;/p&gt;    do_xpath_detail(root,&quot;//p[b/@class]&quot;)    # 0 : &lt;p class=&quot;title&quot;&gt;&lt;b class=&quot;bstyle&quot;&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;    do_xpath_detail(root,&quot;//p[b[@class=&#39;bstyle&#39;]]&quot;)    # 0 : &lt;p class=&quot;title&quot;&gt;&lt;b class=&quot;bstyle&quot;&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt;    print(&#39;------------------------&#39;)def do_xpath(root,path):    result=root.xpath(path)    print(&quot;%s : \n%s&quot; % (path,result))    return resultdef do_xpath_detail(root,path):    result=root.xpath(path)    print(path,&quot;:&quot;)    if type(result)==list and len(result)&gt;0:        for i,r in enumerate(result):            if type(r)==etree._Element:                print(i,&quot;:&quot;,etree.tounicode(r))            else:                print(i,&quot;:&quot;,r)    else:        print(result)    return result</code></pre><h3 id="header-18">Demo: 解析XML</h3><pre><code class="lang-python">from lxml import etreecontent=&#39;&#39;&#39;&lt;collection shelf=&quot;New Arrivals&quot;&gt;    &lt;movie title=&quot;Enemy Behind&quot;&gt;       &lt;type&gt;War, Thriller&lt;/type&gt;       &lt;format&gt;DVD&lt;/format&gt;       &lt;year&gt;2003&lt;/year&gt;       &lt;rating&gt;PG&lt;/rating&gt;       &lt;stars&gt;10&lt;/stars&gt;       &lt;description&gt;Talk about a US-Japan war&lt;/description&gt;    &lt;/movie&gt;    &lt;movie title=&quot;Transformers&quot;&gt;       &lt;type&gt;Anime, Science Fiction&lt;/type&gt;       &lt;format&gt;DVD&lt;/format&gt;       &lt;year&gt;1989&lt;/year&gt;       &lt;rating&gt;R&lt;/rating&gt;       &lt;stars&gt;8&lt;/stars&gt;       &lt;description&gt;A schientific fiction&lt;/description&gt;    &lt;/movie&gt;&lt;/collection&gt;&#39;&#39;&#39;root=etree.XML(content)print(root)print(etree.tounicode(root))result=root.xpath(&#39;//movie&#39;)for i,r in enumerate(result):    print(i,r,&quot;:&quot;,r.tag,r.attrib,r.get(&#39;title&#39;))    print(&quot;text:&quot;,r.text)    print(&quot;string:&quot;,r.xpath(&#39;string(./description)&#39;))    print(&#39;rating:&#39;,r.xpath(&#39;./rating/text()&#39;))</code></pre><h2 id="header-19">文档解析之JSonPath</h2><ul><li>是一种信息抽取类库, 是从<code>JSON</code>文档中抽取指定信息的工具，提供多种语言实现版本，包括：Javascript,Python,PHP，Java</li><li>JsonPath对于<code>JSON</code>来说，相当于<code>XPATH</code> 对于<code>XML</code>, Refer <a href="https://goessner.net/articles/JsonPath/" target="_blank" rel="noopener">JSONPath - XPath for JSON</a></li><li>python中有2个类库可使用<ul><li><code>pip install jsonpath</code>,<code>import jsonpath</code></li><li><code>pip install jsonpath-rw</code>, <code>from jsonpath import jsonpath,parse</code> , Refer <a href="https://pypi.org/project/jsonpath-rw/" target="_blank" rel="noopener">Github</a></li></ul></li></ul><h3 id="header-20">Jsonpath 操作符</h3><ul><li><code>$</code>: 根节点</li><li><code>@</code>: 当前节点</li><li><code>*</code>: 通配符，匹配所有</li><li><code>..</code>: 递归搜索</li><li><code>.</code> : 子节点</li><li><code>[]</code>: 取子节点,迭代器标示(可在里面做简单的迭代操作，如数组下标，根据内容选值等) <ul><li><code>[start:end]</code>,<code>[start:end:step]</code></li><li><code>[,]</code> 支持迭代器中做多选</li></ul></li><li><code>()</code>: 支持表达式计算<ul><li><code>?()</code>: 过滤操作，表达式结果必须是boolean类型</li></ul></li></ul><h3 id="header-21">Json转换</h3><ul><li><code>import json</code></li><li>function:<ul><li><code>loads</code>,<code>load</code>: jsonString -&gt; pythonObj</li><li><code>dumps</code>,<code>dump</code>: pythonObj -&gt; jsonString</li></ul></li><li>转换：<table class="table"><thead><tr><th style="text-align:left">Json</th><th style="text-align:left">Python</th></tr></thead><tbody><tr><td style="text-align:left">object</td><td style="text-align:left">dict</td></tr><tr><td style="text-align:left">array</td><td style="text-align:left">list</td></tr><tr><td style="text-align:left">string</td><td style="text-align:left">unicode</td></tr><tr><td style="text-align:left">number(int)</td><td style="text-align:left">int,long</td></tr><tr><td style="text-align:left">number(real)</td><td style="text-align:left">float</td></tr><tr><td style="text-align:left">true</td><td style="text-align:left">True</td></tr><tr><td style="text-align:left">false</td><td style="text-align:left">False</td></tr><tr><td style="text-align:left">null</td><td style="text-align:left">None</td></tr></tbody></table></li></ul><p><strong> 示例：</strong></p><pre><code class="lang-python">import jsoncontent=&#39;&#39;&#39;{&quot;subjects&quot;:[    {&quot;rate&quot;:&quot;6.5&quot;,&quot;cover_x&quot;:1000,&quot;title&quot;:&quot;硬核&quot;,&quot;url&quot;:&quot;https://movie.douban.com/subject/27109879/&quot;,&quot;playable&quot;:false,&quot;cover&quot;:&quot;https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2532653002.webp&quot;,&quot;id&quot;:&quot;27109879&quot;,&quot;cover_y&quot;:1414,&quot;is_new&quot;:false}    ,{&quot;rate&quot;:&quot;7.1&quot;,&quot;cover_x&quot;:2000,&quot;title&quot;:&quot;奎迪：英雄再起&quot;,&quot;url&quot;:&quot;https://movie.douban.com/subject/26707088/&quot;,&quot;playable&quot;:false,&quot;cover&quot;:&quot;https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2544510053.webp&quot;,&quot;id&quot;:&quot;26707088&quot;,&quot;cover_y&quot;:2800,&quot;is_new&quot;:false}    ,{&quot;rate&quot;:&quot;6.1&quot;,&quot;cover_x&quot;:800,&quot;title&quot;:&quot;芳龄十六&quot;,&quot;url&quot;:&quot;https://movie.douban.com/subject/30334122/&quot;,&quot;playable&quot;:false,&quot;cover&quot;:&quot;https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2549923514.webp&quot;,&quot;id&quot;:&quot;30334122&quot;,&quot;cover_y&quot;:1185,&quot;is_new&quot;:false}    ,{&quot;rate&quot;:&quot;7.7&quot;,&quot;cover_x&quot;:1500,&quot;title&quot;:&quot;污垢&quot;,&quot;url&quot;:&quot;https://movie.douban.com/subject/1945750/&quot;,&quot;playable&quot;:false,&quot;cover&quot;:&quot;https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2548709468.webp&quot;,&quot;id&quot;:&quot;1945750&quot;,&quot;cover_y&quot;:2222,&quot;is_new&quot;:false}    ,{&quot;rate&quot;:&quot;6.8&quot;,&quot;cover_x&quot;:1179,&quot;title&quot;:&quot;欢乐满人间2&quot;,&quot;url&quot;:&quot;https://movie.douban.com/subject/26611891/&quot;,&quot;playable&quot;:false,&quot;cover&quot;:&quot;https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2515404175.webp&quot;,&quot;id&quot;:&quot;26611891&quot;,&quot;cover_y&quot;:1746,&quot;is_new&quot;:false}]}&#39;&#39;&#39;# 1. loads: string -&gt; python objprint(&#39;---- loads: --------------&#39;)result=json.loads(content)print(type(result))                             # &lt;class &#39;dict&#39;&gt;print(result)# 2. dumps: python obj -&gt; stringprint(&#39;---- dumps: --------------&#39;)subjects=result.get(&#39;subjects&#39;)result=json.dumps(subjects,ensure_ascii=False)  # 禁用ascii编码，按utf-8编码    print(type(result))                             # &lt;class &#39;str&#39;&gt;print(result)# 3. dump: python obj -&gt; string -&gt; fileprint(&#39;---- dump: --------------&#39;)json.dump(subjects,open(&#39;test.json&#39;,&#39;w&#39;),ensure_ascii=False)with open(&#39;test.json&#39;,&#39;r&#39;) as f:    print(f.read())# 4. load: file -&gt; string -&gt; python objprint(&#39;---- load: --------------&#39;)result=json.load(open(&#39;test.json&#39;,&#39;r&#39;))print(type(result))                             # &lt;class &#39;list&#39;&gt;print(result)print(&#39;-------------------------&#39;)</code></pre><h3 id="header-22">Demo：使用Jsonpath解析JSON</h3><pre><code class="lang-python">import jsonimport jsonpathcontent=&#39;&#39;&#39;{&quot;subjects&quot;:[    {&quot;rate&quot;:&quot;6.5&quot;,&quot;cover_x&quot;:1000,&quot;title&quot;:&quot;硬核&quot;,&quot;url&quot;:&quot;https://movie.douban.com/subject/27109879/&quot;,&quot;playable&quot;:false,&quot;cover&quot;:&quot;https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2532653002.webp&quot;,&quot;id&quot;:&quot;27109879&quot;,&quot;cover_y&quot;:1414,&quot;is_new&quot;:false}    ,{&quot;rate&quot;:&quot;7.1&quot;,&quot;cover_x&quot;:2000,&quot;title&quot;:&quot;奎迪：英雄再起&quot;,&quot;url&quot;:&quot;https://movie.douban.com/subject/26707088/&quot;,&quot;playable&quot;:false,&quot;cover&quot;:&quot;https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2544510053.webp&quot;,&quot;id&quot;:&quot;26707088&quot;,&quot;cover_y&quot;:2800,&quot;is_new&quot;:false}    ,{&quot;rate&quot;:&quot;6.1&quot;,&quot;cover_x&quot;:800,&quot;title&quot;:&quot;芳龄十六&quot;,&quot;url&quot;:&quot;https://movie.douban.com/subject/30334122/&quot;,&quot;playable&quot;:false,&quot;cover&quot;:&quot;https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2549923514.webp&quot;,&quot;id&quot;:&quot;30334122&quot;,&quot;cover_y&quot;:1185,&quot;is_new&quot;:false}    ,{&quot;rate&quot;:&quot;7.7&quot;,&quot;cover_x&quot;:1500,&quot;title&quot;:&quot;污垢&quot;,&quot;url&quot;:&quot;https://movie.douban.com/subject/1945750/&quot;,&quot;playable&quot;:false,&quot;cover&quot;:&quot;https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2548709468.webp&quot;,&quot;id&quot;:&quot;1945750&quot;,&quot;cover_y&quot;:2222,&quot;is_new&quot;:false}    ,{&quot;rate&quot;:&quot;6.8&quot;,&quot;cover_x&quot;:1179,&quot;title&quot;:&quot;欢乐满人间2&quot;,&quot;url&quot;:&quot;https://movie.douban.com/subject/26611891/&quot;,&quot;playable&quot;:false,&quot;cover&quot;:&quot;https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2515404175.webp&quot;,&quot;id&quot;:&quot;26611891&quot;,&quot;cover_y&quot;:1746,&quot;is_new&quot;:false}]}&#39;&#39;&#39;# 0. 加载obj=json.loads(content)# 1. `[?()]`results=jsonpath.jsonpath(obj,&#39;$.subjects[?(float(@.rate)&gt;=7)]&#39;)print(type(results))# &lt;class &#39;list&#39;&gt;    print(results)#[{&#39;rate&#39;: &#39;7.1&#39;, &#39;cover_x&#39;: 2000, &#39;title&#39;: &#39;奎迪：英雄再起&#39;, &#39;url&#39;: &#39;https://movie.douban.com/subject/26707088/&#39;, &#39;playable&#39;: False, &#39;cover&#39;: &#39;https://img3.doubanio.com/view/photo/s_ratio_poster/public/p2544510053.webp&#39;, &#39;id&#39;: &#39;26707088&#39;, &#39;cover_y&#39;: 2800, &#39;is_new&#39;: False}# , {&#39;rate&#39;: &#39;7.7&#39;, &#39;cover_x&#39;: 1500, &#39;title&#39;: &#39;污垢&#39;, &#39;url&#39;: &#39;https://movie.douban.com/subject/1945750/&#39;, &#39;playable&#39;: False, &#39;cover&#39;: &#39;https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2548709468.webp&#39;, &#39;id&#39;: &#39;1945750&#39;, &#39;cover_y&#39;: 2222, &#39;is_new&#39;: False}# ]# 2. `.xxx`results=jsonpath.jsonpath(obj,&#39;$.subjects[?(float(@.rate)&gt;=7)].title&#39;)print(results)# [&#39;奎迪：英雄再起&#39;, &#39;污垢&#39;]# 3. `[index1,index2]`results=jsonpath.jsonpath(obj,&#39;$.subjects[0,2,3].cover_x&#39;)print(results)# [1000, 800, 1500]# 4. `[start:end]`results=jsonpath.jsonpath(obj,&#39;$.subjects[0:3].cover_x&#39;)print(results)# [1000, 2000, 800]# 5. `[start:end:step]`results=jsonpath.jsonpath(obj,&#39;$.subjects[0:3:2].cover_x&#39;)print(results)# [1000, 800]# 6. `?( &amp;&amp; )`,`?(,)`# cover_x   cover_y# 1000      1414# 2000      2800# 800       1185# 1500      2222# 1179      1746results=jsonpath.jsonpath(obj,&#39;$.subjects[?(@.cover_x&gt;=1000 &amp;&amp; @.cover_y&lt;1500)]&#39;)print(len(results))# 1results=jsonpath.jsonpath(obj,&#39;$.subjects[?(@.cover_x&gt;=1000,@.cover_y&lt;1500)]&#39;)print(len(results))# 5print(&#39;-------------------------&#39;)</code></pre><h2 id="header-23">Reference</h2><ul><li><a href="https://github.com/sixDegree/python-basic-demo" target="_blank" rel="noopener">My Demo</a></li><li><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.html" target="_blank" rel="noopener">Beautiful Soup Documentation</a></li><li><a href="http://www.cnblogs.com/descusr/archive/2012/06/20/2557075.html" target="_blank" rel="noopener">用lxml解析HTML</a></li><li><a href="https://www.cnblogs.com/MUMO/p/5732836.html" target="_blank" rel="noopener">Python爬虫：Xpath语法笔记</a></li><li><a href="https://www.cnblogs.com/lei0213/p/7506130.html" target="_blank" rel="noopener">python爬虫之xpath的基本使用</a></li><li><a href="https://cloud.tencent.com/developer/news/374969" target="_blank" rel="noopener">Python使用JsonPath</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;爬虫基础&lt;/li&gt;
&lt;li&gt;Re&lt;/li&gt;
&lt;li&gt;BeautifulSoup(include CSS Selector)&lt;/li&gt;
&lt;li&gt;XPath&lt;/li&gt;
&lt;li&gt;JSONPath&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://sixdegree.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>SpringBoot Redis</title>
    <link href="http://sixdegree.github.io/2019/01/06/SpringBoot-Redis.html"/>
    <id>http://sixdegree.github.io/2019/01/06/SpringBoot-Redis.html</id>
    <published>2019-01-05T16:00:00.000Z</published>
    <updated>2019-03-09T08:44:28.000Z</updated>
    
    <content type="html"><![CDATA[<ol><li>SpringBoot Redis + Demo</li><li>应用：Session持久化：NoSession方案,SpringSession方案</li><li>方案：NoSession示例</li><li>方案：SpringSession示例</li></ol><a id="more"></a><h2 id="header-1">SpringBoot Redis</h2><ol><li><p>Redis 特性：高性能的key-value数据库</p><ul><li>支持数据的持久化，可将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用</li><li>支持String，Hash,list,Set,ZSet等数据结构的存储</li><li>支持数据的备份（master-slave）</li><li>支持分布式集群，横向扩展</li></ul></li><li><p>Spring: 封装了RedisTemplate对象来支持对redis的各种操作</p><ul><li><code>RedisTemplate&lt;K,V&gt;</code>: 默认采用JDK的序列化策略</li><li><code>StringRedisTemplate extends RedisTemplate&lt;String, String&gt;</code> : 默认采用String的序列化策略</li><li>RedisTemplate中定义了对5种数据结构操作:<ul><li>redisTemplate.opsForValue() 操作字符串</li><li>redisTemplate.opsForHash() 操作hash</li><li>redisTemplate.opsForList() 操作list</li><li>redisTemplate.opsForSet() 操作set</li><li>redisTemplate.opsForZSet() 操作有序set</li></ul></li></ul></li><li><p>SpringBoot:自动化装配</p><ul><li><p><code>org.springframework.boot.autoconfigure.data.redis.RedisProperties</code></p><pre><code class="lang-java">  @ConfigurationProperties(prefix = &quot;spring.redis&quot;)  public class RedisProperties {      private int database = 0;      private String url;      private String host = &quot;localhost&quot;;      private String password;      private int port = 6379;      private boolean ssl;      private Duration timeout;      private Sentinel sentinel;      private Cluster cluster;      private final Jedis jedis = new Jedis();      private final Lettuce lettuce = new Lettuce();      // getter &amp; setter ...      public static class Pool {          private int maxIdle = 8;          private int minIdle = 0;          private int maxActive = 8;          private Duration maxWait = Duration.ofMillis(-1);          //getter &amp; setter ...      }      public static class Cluster {          private List&lt;String&gt; nodes;            //Comma-separated list of &quot;host:port&quot; pairs          private Integer maxRedirects;          //getter &amp; setter ...      }      public static class Sentinel {          private String master;          private List&lt;String&gt; nodes;          //getter &amp; setter ...      }      public static class Jedis {          private Pool pool;          //getter &amp; setter ...      }      public static class Lettuce {          private Duration shutdownTimeout = Duration.ofMillis(100);          private Pool pool;          //getter &amp; setter ...      }  }</code></pre></li><li><code>org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration</code><pre><code class="lang-java">  @Configuration  @ConditionalOnClass(RedisOperations.class)  @EnableConfigurationProperties(RedisProperties.class)  @Import({ LettuceConnectionConfiguration.class, JedisConnectionConfiguration.class })  public class RedisAutoConfiguration {      @Bean      @ConditionalOnMissingBean(name = &quot;redisTemplate&quot;)      public RedisTemplate&lt;Object, Object&gt; redisTemplate(              RedisConnectionFactory redisConnectionFactory) throws UnknownHostException {          RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate&lt;&gt;();          template.setConnectionFactory(redisConnectionFactory);          return template;      }      @Bean      @ConditionalOnMissingBean      public StringRedisTemplate stringRedisTemplate(              RedisConnectionFactory redisConnectionFactory) throws UnknownHostException {          StringRedisTemplate template = new StringRedisTemplate();          template.setConnectionFactory(redisConnectionFactory);          return template;      }  }</code></pre></li></ul></li><li><p>数据同步,eg: redis和mysql间的数据的同步</p><ul><li>Read: <ul><li>Read from redis -&gt; Exist -&gt; get Data</li><li>Read from redis -&gt; None -&gt; Read mysql -&gt; write to redis</li></ul></li><li>Write: <ul><li>Write to mysql -&gt; Success -&gt; Write to Redis</li></ul></li></ul></li></ol><h3 id="header-2">Demo</h3><ol><li><p>pom.xml</p><pre><code class="lang-xml"> &lt;!-- SpringBoot --&gt; &lt;dependency&gt;     &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;     &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt;     &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;     &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Springboot redis --&gt; &lt;dependency&gt;     &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;     &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt;</code></pre></li><li><p>resources/application.yml</p><pre><code> spring:   redis:     host: localhost     port: 6379     password: 123456     timeout: 30000     jedis:       pool:         max-active: 8         max-wait: 1         max-idle: 8         min-idle: 0</code></pre></li><li><p>RedisConfig</p><pre><code class="lang-java"> @Configuration public class RedisConfig {     @Bean     @ConditionalOnMissingBean(name = &quot;redisTemplate&quot;)        // create and inject when no bean which named `redisTemplate`     public RedisTemplate&lt;Object, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory){         RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate&lt;&gt;();         template.setConnectionFactory(redisConnectionFactory);         // set serializer for key: use `RedisSerializer&lt;String&gt;`         RedisSerializer&lt;String&gt; stringSerializer = new StringRedisSerializer();         template.setKeySerializer(stringSerializer);         template.setHashKeySerializer(stringSerializer);         // set serializer for value: use `Jackson2JsonRedisSerializer&lt;Object&gt;`         Jackson2JsonRedisSerializer&lt;Object&gt;    jsonSerializer = new Jackson2JsonRedisSerializer&lt;Object&gt;(Object.class);         ObjectMapper objectMapper = initObjectMapper();         jsonSerializer.setObjectMapper(objectMapper);         template.setValueSerializer(jsonSerializer);         template.setHashValueSerializer(jsonSerializer);         return template;     }     private ObjectMapper initObjectMapper(){         ObjectMapper objectMapper = new ObjectMapper();         //去除掉对getter和setter的依赖,ObjectMapper将通过反射机制直接操作Java对象上的字段         objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY);         // DefaultTyping:          // 指定什么样的类型输入会被使用 ( eg: `NON_FINAL`表示对所有非final类型或者非final类型元素对象持久化)         // 这样Json序列化/反序列化不需要知道具体子类的类型，只需要根据父类以及类别标识就能准确判断子类类型         // 注：会存储类型信息（为了能准确的反序列多态类型的数据）         objectMapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL);         // disable Feature:         // `DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES` (Json -&gt; Object): 忽略json字符串中不识别的属性         // `SerializationFeature.FAIL_ON_EMPTY_BEANS` (Object -&gt; Json) 忽略无法转换的对象 “No serializer found for class com.xxx.xxx”         objectMapper.disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES);          objectMapper.disable(SerializationFeature.FAIL_ON_EMPTY_BEANS);         // Serialization (Object -&gt; Json):          // `NON_EMPTY`只序列化非空属性         //objectMapper.setSerializationInclusion(JsonInclude.Include.NON_EMPTY);         return objectMapper;     } }</code></pre></li><li><p>RedisService</p><pre><code class="lang-java"> @Service public class RedisService {     // @Autowired     // private StringRedisTemplate redisTemplate;     //  inject base on bean name     // @Resource             // private RedisTemplate&lt;String, Object&gt; redisTemplate;     // inject base on the bean type     @Autowired     private RedisTemplate&lt;String, Object&gt; redisTemplate;     public Object get(String key) {         return redisTemplate.opsForValue().get(key);     }     public void set(String key, Object value) {         redisTemplate.opsForValue().set(key, value);     }     public void set(String key,Object value,int timeout){         redisTemplate.opsForValue().set(key, value,timeout,TimeUnit.SECONDS);     }     public Boolean delete(String key){         return redisTemplate.delete(key);     }     public void expire(String key,int timeout){         redisTemplate.expire(key, timeout, TimeUnit.SECONDS);     } }</code></pre></li></ol><h2 id="header-3">应用：Session 持久化</h2><ul><li>Session在服务端保存用户会话状态（如：用户登录信息等） </li><li>在程序重启、多进程运行、负载均衡、跨域等情况时，会出现Session丢失或多进程、多个负载站点间状态不能共享的情况</li><li>解决方案：将Session持久化存储以共享 （ Redis 是一个高性能的<code>key-value</code>数据库，可用来存储<code>Session</code>），eg:<ul><li><code>NoSession</code>(服务端自己生成一个序列码代替Session作为标识)</li><li><code>SpringSession</code>(对每一个请求request进行封装，后续取到的不是HttpSession而是可持久化的SpringSession)</li></ul></li></ul><h3 id="header-4">方案：NoSession</h3><p>Scenarios:</p><ol><li><p>One App, Multiple Clients Login,only latest login valid:</p><ul><li>Login Process:<ul><li>Client1: login -&gt; success</li><li>Client2: login -&gt; success &amp; expire Client1 login</li></ul></li><li>Visit controlled resources:<ul><li>Client1: 401 Unauthorized -&gt; please login</li><li>Client2: success</li></ul></li><li>Logout Process:<ul><li>Client1: logout -&gt; invalid token-&gt; success</li><li>Client2: logout -&gt; valid token -&gt; success</li></ul></li><li>Implement:<ul><li>使用Redis维护保存用户登陆信息:<ul><li><code>&lt;prefix&gt;:&lt;token&gt;</code>:<code>&lt;user&gt;</code> &amp; expireTime</li><li><code>&lt;prefix&gt;:&lt;user.id&gt;</code>:<code>&lt;token&gt;</code> &amp; expireTime</li></ul></li><li>HttpHeader中带有token:<ul><li><code>&lt;token&gt;</code></li></ul></li><li>login:<ul><li>delete the two redis key-values (for invaliding other clients login)</li><li>generate new <code>&lt;token&gt;</code></li><li>set the two redis key-values</li><li>set <code>&lt;token&gt;</code> to Http Response</li></ul></li><li>logout:<ul><li>get <code>token1</code> from http header</li><li>get <code>user.id</code> from redis key-value <code>&lt;prefix&gt;:&lt;token1&gt;</code>:<code>&lt;user&gt;</code></li><li>get <code>token2</code> from redis key-value <code>&lt;prefix&gt;:&lt;user.id&gt;</code>:<code>&lt;token2&gt;</code></li><li>can’t get <code>user.id</code> || can’t get <code>token2</code> -&gt; valid -&gt; success</li><li>compare <code>token1</code> &amp; <code>token2</code><ul><li>match -&gt; valid -&gt; delete the two redis key-values -&gt; success</li><li>unmatch -&gt; invalid -&gt; fail</li></ul></li></ul></li><li>getAuthentication<ul><li>get <code>user</code> from redis key-value <code>&lt;prefix&gt;:&lt;token&gt;</code>:<code>&lt;user&gt;</code> (get <code>token</code> from http header) </li></ul></li></ul></li></ul></li><li><p>One AuthService,Multiple other Services call AuthService(seperate auth process)</p><ul><li>AuthService API:<ul><li>login</li><li>logout</li><li>getAuthentication</li></ul></li><li>Service1 -&gt; AuthService</li><li>Service2 -&gt; AuthService</li><li>Implement:<ul><li>使用Redis维护保存用户登陆信息: (note: different services use different <code>&lt;prefix&gt;</code>)<ul><li><code>&lt;prefix&gt;:&lt;token&gt;</code>:<code>&lt;user&gt;</code> &amp; expireTime </li></ul></li><li>HttpHeader中带有token:<ul><li><code>&lt;token&gt;</code></li></ul></li><li>login: <ul><li>generate new <code>token</code></li><li>set redis key-value <code>&lt;prefix&gt;:&lt;token&gt;</code>:<code>&lt;user&gt;</code></li><li>set <code>&lt;token&gt;</code> to HttpResponse</li></ul></li><li>logout: <ul><li>delete redis key-value <code>&lt;prefix&gt;:&lt;token&gt;</code>:<code>&lt;user&gt;</code> (get <code>token</code> from http header)</li></ul></li><li>getAuthentication: <ul><li>get <code>user</code> from redis key-value <code>&lt;prefix&gt;:&lt;token&gt;</code>:<code>&lt;user&gt;</code> (get <code>token</code> from http header)</li></ul></li></ul></li></ul></li></ol><h3 id="header-5">方案：SpringSession</h3><p>Refer to <a href="https://docs.spring.io/spring-session/docs/current/reference/html5/guides/boot-redis.html" target="_blank" rel="noopener">Spring Session - Spring Boot</a></p><p>spring-session</p><ul><li>spring旗下的一个项目, 把servlet容器实现的HttpSession替换为spring-session的HttpSession</li><li>核心组件：<code>SessionRepositoryFilter</code> 拦截Web请求,确保随后调用<code>javax.servlet.http.HttpServletRequest</code>的<code>getSession()</code>，会返回Spring Session的HttpSession实例，而不是应用服务器默认的<code>HttpSession</code><ul><li>HttpSessionIdResolver: resolveSessionIds，expireSession</li><li>SessionRepository: createSession,save,findById,deleteById</li><li>@Override doFilterInternal(HttpServletRequest request,HttpServletResponse response, FilterChain filterChain)<ul><li>SessionRepositoryRequestWrapper (getSession -&gt; return HttpSessionWrapper)</li><li>SessionRepositoryResponseWrapper    </li></ul></li></ul></li><li>使用Redis存储的SpringSession(默认使用前缀：<code>spring:session</code>),对于每一个session都会创建3组数据,eg:<ul><li><code>spring:session:sessions:[sessionId]</code>: hash结构,存储springsession的主要内容:<ul><li>sessionAttr:[sessionId]  存储session信息（eg：实体类的序列化数据）</li><li>creationTime</li><li>maxInactiveInterval</li><li>lastAccessedTime</li></ul></li><li><code>spring:session:sessions:expires:[sessionId]</code>：string结构，value为空,ttl倒计时过期</li><li><code>spring:session:expirations:[expireTime]</code>：set结构<ul><li>expires:[sessionId] 一个会话一条</li><li>redis的ttl删除key是一个异步行为且是一个低优先级的行为，可能会导致session不被清除，于是引入了expirations这个key，来主动进行session的过期行为判断</li></ul></li></ul></li><li>Process：<ul><li>通过request的<code>getSession(boolean create)</code> 方法获取<code>session</code></li><li>根据sessionId 读取 <code>spring:session:sessions:[sessionId]</code> 的值</li></ul></li><li>Scenarios:<ul><li>One App,Multiple Clients Login =&gt; seperated,all success</li><li>One AuthService,Multiple other Services call AuthService =&gt; seperated,all successs</li><li>One App,Multiple ports (Nginx/Apache+Tomcat) =&gt; depends on session async strategy</li></ul></li></ul><h2 id="header-6">方案：NoSession 示例</h2><h3 id="header-7">Dependency</h3><p>pom.xml</p><pre><code class="lang-xml">&lt;!-- SpringBoot --&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;    &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;!-- Redis --&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- for StringUtils --&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;    &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- for MD5 --&gt;&lt;dependency&gt;    &lt;groupId&gt;commons-codec&lt;/groupId&gt;    &lt;artifactId&gt;commons-codec&lt;/artifactId&gt;&lt;/dependency&gt;</code></pre><h3 id="header-8">Config</h3><ol><li><p>resources/application.yml</p><pre><code class="lang-yml"> server:       port: 8080       servlet:         context-path: /micro-auth spring:   redis:     host: localhost     port: 6379     password: 123456     timeout: 30000     jedis:       pool:         max-active: 8         max-wait: 1         max-idle: 8         min-idle: 0 # for authController         auth:   usersessionHeader: usersession   principalHeader: micro-auth   expireTime: 180</code></pre></li><li><p>RedisConfig</p><pre><code class="lang-java"> @Configuration public class RedisConfig {     @Bean     public RedisTemplate&lt;Object,Object&gt; jsonRedisTemplate(RedisConnectionFactory redisConnectionFactory){         RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate&lt;Object,Object&gt;();         template.setConnectionFactory(redisConnectionFactory);         RedisSerializer&lt;String&gt; stringSerializer = new StringRedisSerializer();         template.setKeySerializer(stringSerializer);         template.setHashKeySerializer(stringSerializer);         Jackson2JsonRedisSerializer&lt;Object&gt;    jsonSerializer = initJsonSerializer();         template.setValueSerializer(jsonSerializer);         template.setHashValueSerializer(jsonSerializer);         template.afterPropertiesSet();         System.out.println(&quot;Create Customer JsonRedisTemplate-----&quot;);         return template;     }     private Jackson2JsonRedisSerializer&lt;Object&gt; initJsonSerializer(){         Jackson2JsonRedisSerializer&lt;Object&gt;    jsonSerializer = new Jackson2JsonRedisSerializer&lt;Object&gt;(Object.class);         ObjectMapper objectMapper = initObjectMapper();         jsonSerializer.setObjectMapper(objectMapper);         return jsonSerializer;     }     private ObjectMapper initObjectMapper(){         ObjectMapper objectMapper = new ObjectMapper();         objectMapper.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY);         objectMapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL);         objectMapper.disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES);          objectMapper.disable(SerializationFeature.FAIL_ON_EMPTY_BEANS);         //objectMapper.setSerializationInclusion(JsonInclude.Include.NON_EMPTY);         return objectMapper;     } }</code></pre></li></ol><h3 id="header-9">RedisService</h3><pre><code class="lang-java">@Servicepublic class RedisService {            @Autowired    private RedisTemplate&lt;Object, Object&gt; jsonRedisTemplate;    public Object get(String key) {        return jsonRedisTemplate.opsForValue().get(key);    }        public void set(String key, Object value) {        jsonRedisTemplate.opsForValue().set(key, value);    }    public void set(String key,Object value,int timeout){        jsonRedisTemplate.opsForValue().set(key, value,timeout,TimeUnit.SECONDS);    }    public Boolean delete(String key){        return jsonRedisTemplate.delete(key);    }    public void expire(String key,int timeout){        jsonRedisTemplate.expire(key, timeout, TimeUnit.SECONDS);    }}</code></pre><h3 id="header-10">AuthController</h3><pre><code class="lang-java">@RestControllerpublic class AuthController {    @Value(&quot;${auth.principalHeader}&quot;)    private String principalHeader;    @Value(&quot;${auth.expireTime}&quot;)    private int expireTime;    @Autowired    private RedisService redisService;    @Autowired    private UserService userService;    @GetMapping(&quot;/&quot;)    public Object index(){        return ResponseEntity.ok(&quot;This is micro-authService!&quot;);    }    @PostMapping(&quot;/login&quot;)    public Object login(@RequestBody User loginUser,            @RequestHeader(name=&quot;${auth.usersessionHeader}&quot;) String sessionKey,             @RequestHeader(name=&quot;${auth.principalHeader}&quot;,required=false) String principle,            HttpServletResponse response){        // db: verify         if(loginUser==null || StringUtils.isAnyBlank(loginUser.getName(),loginUser.getPassword()))            return MicroResponse.InvalidRequest;        // check principle        if(principle!=null){            User user=(User)this.redisService.get(sessionKey+&quot;:&quot;+principle);            if(user!=null                     &amp;&amp; loginUser.getName().equals(user.getName())                     &amp;&amp; MD5Utils.getMD5Str(loginUser.getPassword()).equals(user.getPassword())){                this.redisService.expire(sessionKey+&quot;:&quot;+principle, expireTime);                this.redisService.expire(sessionKey+&quot;:&quot;+user.getId(), expireTime);                response.setHeader(principalHeader, principle);                return MicroResponse.success(&quot;login success&quot;);            }        }        // check name &amp; password        User user=userService.findByNameAndPassword(loginUser);        if(user==null)            return MicroResponse.AuthenticationFail;        // delete other clients login        String oldToken = (String)this.redisService.get(sessionKey+&quot;:&quot;+user.getId());        if(user!=null)            this.redisService.delete(sessionKey+&quot;:&quot;+oldToken);        this.redisService.delete(sessionKey+&quot;:&quot;+user.getId());        // set new        String token = UUID.randomUUID().toString();        this.redisService.set(sessionKey+&quot;:&quot;+token,user,expireTime);        this.redisService.set(sessionKey+&quot;:&quot;+user.getId(),token,expireTime);        response.setHeader(principalHeader, token);        return MicroResponse.success(&quot;login success&quot;);    }    @GetMapping(&quot;/logout&quot;)    public Object logout(@RequestHeader(name=&quot;${auth.principalHeader}&quot;) String principle,@RequestHeader(name=&quot;${auth.usersessionHeader}&quot;) String sessionKey){        User user=(User)this.redisService.get(sessionKey+&quot;:&quot;+principle);        if(user==null)            return MicroResponse.success(&quot;logout success&quot;);        String token=(String)this.redisService.get(sessionKey+&quot;:&quot;+user.getId());        if(token==null)            return MicroResponse.success(&quot;logout success&quot;);        if(principle.equals(token)){            this.redisService.delete(sessionKey+&quot;:&quot;+user.getId());            this.redisService.delete(sessionKey+&quot;:&quot;+token);            return MicroResponse.success(&quot;logout success&quot;);        }        return MicroResponse.fail(&quot;logout fail&quot;);    }    @GetMapping(&quot;/authentication&quot;)    public Object getAuthentication(@RequestHeader(name=&quot;${auth.principalHeader}&quot;) String principle,@RequestHeader(name=&quot;${auth.usersessionHeader}&quot;) String sessionKey){        return MicroResponse.success(redisService.get(sessionKey+&quot;:&quot;+principle));    }    @PostMapping(&quot;/regist&quot;)    public Object regist(@RequestBody User user){        if(user==null || StringUtils.isAnyBlank(user.getName(),user.getPassword()))            return MicroResponse.InvalidRequest;        boolean result=userService.save(user);        return new MicroResponse(result,result?1:0,user);    }}// catch error!@RestControllerpublic class AuthErrorController implements ErrorController{    @Override    public String getErrorPath() {        return &quot;/error&quot;;    }    @RequestMapping(value=&quot;/error&quot;)    public Object onError(HttpServletResponse rs,Exception ex){       HttpStatus status=HttpStatus.resolve(rs.getStatus());       if(status!=null)           return new MicroResponse(false,rs.getStatus(),status.getReasonPhrase());       else           return MicroResponse.fail(ex.getMessage());    }}</code></pre><h3 id="header-11">Service &amp; Repository &amp; Entity</h3><p>Service: UserService</p><pre><code class="lang-java">@Servicepublic class UserService {    @Autowired    private UserRepository userRepository;    public User findByNameAndPassword (User user){        Optional&lt;User&gt; result= this.userRepository.findByNameAndPassword(user.getName(),user.getPassword());        if(result.isPresent())            return result.get();        return null;    }    public boolean save(User user){        return this.userRepository.save(user);    }}</code></pre><p>Repository: UserRepository</p><pre><code class="lang-java">@Repositorypublic class UserRepository {    //private final ConcurrentMap&lt;Integer,User&gt; users = new ConcurrentHashMap&lt;Integer,User&gt;();    private final ConcurrentMap&lt;String,User&gt; users=new ConcurrentHashMap&lt;String,User&gt;();    private final static AtomicInteger idGenerator = new AtomicInteger();    @PostConstruct    public void init(){        users.put(&quot;admin&quot;, new User(idGenerator.incrementAndGet(),&quot;admin&quot;,MD5Utils.getMD5Str(&quot;admin123&quot;)));    }    public boolean save(User user){        Integer id = idGenerator.incrementAndGet();        user.setId(id);        user.setPassword(MD5Utils.getMD5Str(user.getPassword()));        return users.put(user.getName(),user)==null;    }    public Collection&lt;User&gt; list(){        return users.values();    }    public Optional&lt;User&gt; findByNameAndPassword(String name,String password){        User user=users.get(name);        if(user!=null &amp;&amp; user.getPassword().equals(MD5Utils.getMD5Str(password)))            return Optional.of(user);        return Optional.empty();    }    public boolean existsByName(String name){        return users.containsKey(name);    }}</code></pre><p>Entity: User</p><pre><code class="lang-java">public class User implements Serializable{    private static final long serialVersionUID = -4198480470411674996L;    private Integer id;    private String name;    private String password;    public User(){}    public User(Integer id,String name,String password){        this.id=id;        this.name=name;        this.password=password;    }    @JsonIgnore    public String getPassword() {        return password;    }    @JsonProperty    public void setPassword(String password) {        this.password = password;    }    /* other getter &amp; setter ... */}</code></pre><h3 id="header-12">Utils: MD5Utils &amp; MicroResponse</h3><pre><code class="lang-java">public class MD5Utils {     public static String getMD5Str(String strValue) {        try {            MessageDigest md5 = MessageDigest.getInstance(&quot;MD5&quot;);            String newstr = Base64.encodeBase64String(md5.digest(strValue.getBytes()));            return newstr;        } catch (NoSuchAlgorithmException e) {            //e.printStackTrace();            System.out.println(e.getMessage());        }        return strValue;    }}</code></pre><pre><code class="lang-java">public class MicroResponse {    public static final MicroResponse OK=new MicroResponse(true,1,null);    public static final MicroResponse AuthenticationFail=new MicroResponse(false,2, &quot;Authentication Fail&quot;);    public static final MicroResponse UnAuthorized=new MicroResponse(false,3, &quot;Not Authorized&quot;);    public static final MicroResponse InvalidRequest=new MicroResponse(false,4,&quot;Invalid Request&quot;);    public static final MicroResponse Existed=new MicroResponse(false,5,&quot;Already Existed&quot;);    public static final MicroResponse NotExist=new MicroResponse(false,6,&quot;Not Exist&quot;);    public static MicroResponse success(Object data){        return new MicroResponse(true,1,data);    }    public static MicroResponse fail(Object data){        return new MicroResponse(false,0,data);    }    private boolean success;    private Integer code;    private Object data;    public MicroResponse(boolean success, Integer code, Object data) {        super();        this.success = success;        this.code = code;        this.data = data;    }    /* getter &amp; setter ...  */}</code></pre><h3 id="header-13">Run and Visit</h3><ol><li><p>main</p><pre><code class="lang-java"> @SpringBootApplication public class AuthServiceApplication {     public static void main(String[] args) {         SpringApplication.run(AuthServiceApplication.class, args);     } }</code></pre></li><li><p>Visit: <code>http://localhost:8080/micro-auth</code></p><ul><li>POST <code>/regist</code><ul><li>body: <code>{&quot;name&quot;:&quot;Tom&quot;,&quot;password&quot;:&quot;123123&quot;}</code></li></ul></li><li>POST <code>/login</code><ul><li>header: <code>usersession:xx</code></li><li>body: <code>{&quot;name&quot;:&quot;Tom&quot;,&quot;password&quot;:&quot;123123&quot;}</code></li></ul></li><li>GET     <code>/logout</code><ul><li>header: <code>usersession:xx</code>,<code>micro-auth:xxxxxxxxxxxxxx</code></li></ul></li><li>GET <code>/authentication</code><ul><li>header: <code>usersession:xx</code>,<code>micro-auth:xxxxxxxxxxxxxx</code></li></ul></li></ul></li></ol><h3 id="header-14">Verify</h3><ol><li><p>POST <code>/regist</code></p><pre><code class="lang-bash"> &gt; curl -i -H &quot;Content-Type: application/json&quot; -H &quot;usersession:s1&quot; -X POST -d &#39;{&quot;name&quot;:&quot;Tom&quot;,&quot;password&quot;:&quot;123123&quot;}&#39; http://localhost:8080/micro-auth/regist HTTP/1.1 200 Content-Type: application/json;charset=UTF-8 Transfer-Encoding: chunked Date: Thu, 07 Feb 2019 05:38:22 GMT {&quot;success&quot;:true,&quot;code&quot;:1,&quot;data&quot;:{&quot;id&quot;:2,&quot;name&quot;:&quot;Tom&quot;}}</code></pre></li><li><p>POST <code>/login</code></p><pre><code class="lang-bash"> &gt; curl -i -H &quot;Content-Type: application/json&quot; -H &quot;usersession:s1&quot; -X POST -d &#39;{&quot;name&quot;:&quot;Tom&quot;,&quot;password&quot;:&quot;123123&quot;}&#39; http://localhost:8080/micro-auth/login HTTP/1.1 200 micro-auth: 1d0fa647-9dbe-410a-8d9c-0e1c973a98e2 Content-Type: application/json;charset=UTF-8 Transfer-Encoding: chunked Date: Thu, 07 Feb 2019 05:39:33 GMT {&quot;success&quot;:true,&quot;code&quot;:1,&quot;data&quot;:&quot;login success&quot;} # check redis: redis:6379&gt; keys * 1) &quot;s1:2&quot; 2) &quot;s1:1d0fa647-9dbe-410a-8d9c-0e1c973a98e2&quot; redis:6379&gt; get s1:2 &quot;\&quot;1d0fa647-9dbe-410a-8d9c-0e1c973a98e2\&quot;&quot; redis:6379&gt; get s1:1d0fa647-9dbe-410a-8d9c-0e1c973a98e2 &quot;[\&quot;com.cj.auth.entity.User\&quot;,{\&quot;id\&quot;:2,\&quot;name\&quot;:\&quot;Tom\&quot;,\&quot;password\&quot;:\&quot;Qpf0SxOVUjUkWySXOZ16kw==\&quot;}]&quot; redis:6379&gt; ttl s1:2 (integer) 100 redis:6379&gt; ttl s1:1d0fa647-9dbe-410a-8d9c-0e1c973a98e2 (integer) 99</code></pre></li><li><p>GET <code>/authentication</code></p><pre><code class="lang-bash"> &gt; curl -i -H &quot;micro-auth:1d0fa647-9dbe-410a-8d9c-0e1c973a98e2&quot; -H &quot;usersession:s1&quot; -X GET http://localhost:8080/micro-auth/authentication HTTP/1.1 200 Content-Type: application/json;charset=UTF-8 Transfer-Encoding: chunked Date: Thu, 07 Feb 2019 05:40:49 GMT {&quot;success&quot;:true,&quot;code&quot;:1,&quot;data&quot;:{&quot;id&quot;:2,&quot;name&quot;:&quot;Tom&quot;}}</code></pre></li><li><p>GET <code>/logout</code></p><pre><code class="lang-bash"> &gt; curl -i -H &quot;micro-auth: 1d0fa647-9dbe-410a-8d9c-0e1c973a98e2&quot; -H &quot;usersession:s1&quot; -X GET http://localhost:8080/micro-auth/logout HTTP/1.1 200 Content-Type: application/json;charset=UTF-8 Transfer-Encoding: chunked Date: Thu, 07 Feb 2019 05:04:30 GMT {&quot;success&quot;:true,&quot;code&quot;:1,&quot;data&quot;:&quot;logout success&quot;} # check redis: redis:6379&gt; keys * (empty list or set)</code></pre></li></ol><h3 id="header-15">Client: call AuthService</h3><p>使用RestTemplate方式call发布的AuthService ( 注：也可考虑使用Dobbo等其他RPC方式，AuthService发布方式也需要改变)：</p><ul><li><p>resources/application.yml</p><pre><code>  server:    port: 9080    servlet:      context-path: /micro-service1  auth:      url: http://localhost:8080/micro-auth      principalHeader: micro-auth      usersessionHeader: usersession      usersessionKey: s1</code></pre></li><li><p>Controller</p><pre><code class="lang-java">  @RestController  @RequestMapping(&quot;/auth&quot;)  public class Service1Controller {      @Autowired      private AuthCallService authCallService;      @PostMapping(&quot;/login&quot;)      public Object login(@RequestBody User loginUser){          // ...      }      @PostMapping(&quot;/logout&quot;)      public Object logout(){          // ...      }      @GetMapping(&quot;/authentication&quot;)      public Object getAuthentication(@RequestHeader(name=&quot;${auth.principalHeader}&quot;,required=false) String principle){          return AuthCallService.getAuthentication(principle);      }  }</code></pre></li><li><p>AuthCallService</p><pre><code class="lang-java">  @Service  public class AuthCallService {      @Value(&quot;${auth.url}&quot;)      private String authURL;      @Value(&quot;${auth.principalHeader}&quot;)      private String principalHeader;      @Value(&quot;${auth.usersessionHeader}&quot;)      private String usersessionHeader;      @Value(&quot;${auth.usersessionKey}&quot;)      private String usersessionKey;      @Autowired      private RestTemplate restTemplate;      public Object login(){          //...      }      public Object logout(){          //...      }      public Object getAuthentication(String principle){          HttpHeaders headers = new HttpHeaders();           headers.set(principalHeader,principle);           headers.set(usersessionHeader,usersessionKey);           headers.setContentType(MediaType.APPLICATION_JSON);          HttpEntity&lt;String&gt; entity=new HttpEntity&lt;String&gt;(null,headers);          HttpEntity&lt;Map&gt; response=restTemplate.exchange(authURL+&quot;/authentication&quot;,HttpMethod.GET,entity,Map.class);          return response.getBody();      }  }</code></pre></li><li><p>main </p><pre><code class="lang-java">  @SpringBootApplication  public class ServiceApplication {      public static void main(String[] args) {          SpringApplication.run(ServiceApplication.class, args);      }  }</code></pre></li><li><p>Visit: <code>http://localhost:9080/micro-service1</code></p><ul><li><code>/login</code></li><li><code>/logout</code></li><li><code>/getAuthentication</code></li></ul></li></ul><h3 id="header-16">Client：简化测试版</h3><pre><code class="lang-java">@RunWith(SpringRunner.class)@SpringBootTest(webEnvironment=SpringBootTest.WebEnvironment.RANDOM_PORT)public class ServiceCallAuthNoSessionTest {    @Autowired    private TestRestTemplate restTemplate;    private String authURL=&quot;http://localhost:8080/micro-auth&quot;;    private String principalHeader=&quot;micro-auth&quot;;    private String usersessionHeader=&quot;usersession&quot;;    private String usersessionKey=&quot;s1&quot;;    String principle=&quot;a37377ec-dc92-41f8-95af-4d0a494f3eb8&quot;;    @Test    public void callTest(){        //getAuthentication        callGetAuthentication();        // login        callLoginTest();        // callGetAuthentication        callGetAuthentication();        // logout        callLogoutTest();        // getAuthentication        callGetAuthentication();    }    @Test    public void callLoginTest(){        System.out.println(&quot;call login...&quot;);        HttpHeaders headers = new HttpHeaders();         headers.set(usersessionHeader,usersessionKey);         headers.setContentType(MediaType.APPLICATION_JSON);        Map&lt;String,String&gt; userMap=new HashMap&lt;String,String&gt;();        userMap.put(&quot;name&quot;, &quot;Tom&quot;);        userMap.put(&quot;password&quot;, &quot;123123&quot;);        HttpEntity&lt;Map&gt; entity=new HttpEntity&lt;Map&gt;(userMap,headers);        HttpEntity&lt;Map&gt; response=restTemplate.exchange(authURL+&quot;/login&quot;,HttpMethod.POST,entity,Map.class);        System.out.println(response);        principle=(String)response.getHeaders().getFirst(principalHeader);        System.out.println(&quot;token:&quot;+principle);    }    @Test    public void callLogoutTest(){        System.out.println(&quot;call logout...&quot;);        HttpHeaders headers = new HttpHeaders();         headers.set(principalHeader,principle);         headers.set(usersessionHeader,usersessionKey);         headers.setContentType(MediaType.APPLICATION_JSON);        HttpEntity&lt;String&gt; entity=new HttpEntity&lt;String&gt;(null,headers);        HttpEntity&lt;String&gt; response=restTemplate.exchange(authURL+&quot;/logout&quot;,HttpMethod.GET,entity,String.class);        System.out.println(response);    }    @Test    public void callGetAuthentication(){        System.out.println(&quot;call getAuthentication...&quot;);        HttpHeaders headers = new HttpHeaders();         headers.set(principalHeader,principle);         headers.set(usersessionHeader,usersessionKey);         headers.setContentType(MediaType.APPLICATION_JSON);        HttpEntity&lt;String&gt; entity=new HttpEntity&lt;String&gt;(null,headers);        HttpEntity&lt;Map&gt; response=restTemplate.exchange(authURL+&quot;/authentication&quot;,HttpMethod.GET,entity,Map.class);        System.out.println(response);    }}</code></pre><p>Run Junit Test: callTest</p><pre><code>call getAuthentication...&lt;200,{success=true, code=1, data=null},{Content-Type=[application/json;charset=UTF-8], Transfer-Encoding=[chunked], Date=[Thu, 07 Feb 2019 06:37:22 GMT]}&gt;call login...&lt;200,{success=true, code=1, data=login success},{micro-auth=[80d89cb8-f8e3-4b6c-8344-06d9e3a88d41], Content-Type=[application/json;charset=UTF-8], Transfer-Encoding=[chunked], Date=[Thu, 07 Feb 2019 06:37:22 GMT]}&gt;token:80d89cb8-f8e3-4b6c-8344-06d9e3a88d41call getAuthentication...&lt;200,{success=true, code=1, data={id=2, name=Tom}},{Content-Type=[application/json;charset=UTF-8], Transfer-Encoding=[chunked], Date=[Thu, 07 Feb 2019 06:37:22 GMT]}&gt;call logout...&lt;200,{&quot;success&quot;:true,&quot;code&quot;:1,&quot;data&quot;:&quot;logout success&quot;},{Content-Type=[application/json;charset=UTF-8], Transfer-Encoding=[chunked], Date=[Thu, 07 Feb 2019 06:37:22 GMT]}&gt;call getAuthentication...&lt;200,{success=true, code=1, data=null},{Content-Type=[application/json;charset=UTF-8], Transfer-Encoding=[chunked], Date=[Thu, 07 Feb 2019 06:37:22 GMT]}&gt;</code></pre><h2 id="header-17">方案：SpringSession 示例</h2><h3 id="header-18">Dependency</h3><p>pom.xml</p><pre><code class="lang-xml">&lt;!-- SpringBoot --&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- Springboot redis --&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- Spring Session --&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework.session&lt;/groupId&gt;    &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;</code></pre><h3 id="header-19">Config</h3><ol><li><p>resources/application.yml</p><pre><code> server:   port: 8080   servlet:     context-path: /micro-auth       spring:   redis:     host: localhost     port: 6379     password: 123456     timeout: 30000     jedis:       pool:         max-active: 8         max-wait: 1         max-idle: 8         min-idle: 0       #  session: #    store-type: redis   #     timeout: 180   #    redis: #      namespace: sps        # default prefix is `spring:session` #      flush-mode: on-save</code></pre></li><li><p>RedisConfig (使用<code>@EnableRedisHttpSession</code>或在application.yml中配置<code>spring.session</code>)</p><pre><code class="lang-java"> @Configuration @EnableRedisHttpSession(maxInactiveIntervalInSeconds = 180,     redisFlushMode=RedisFlushMode.ON_SAVE,     redisNamespace=&quot;sps&quot;) public class RedisConfig { }</code></pre></li></ol><h3 id="header-20">Controller</h3><p>AuthSessionController</p><pre><code class="lang-java">@RestController@RequestMapping(&quot;/session&quot;)public class AuthSessionController {    @Autowired    private UserService userService;    @GetMapping(&quot;/&quot;)    public Object index(){        return ResponseEntity.ok(&quot;This is micro-authService using springsession!&quot;);    }    @PostMapping(&quot;/login&quot;)    public Object login(@RequestBody User loginUser,HttpServletRequest request){        if(loginUser==null || StringUtils.isAnyBlank(loginUser.getName(),loginUser.getPassword()))            return MicroResponse.InvalidRequest;        // check name &amp; password        User user=userService.findByNameAndPassword(loginUser);        if(user==null)            return MicroResponse.AuthenticationFail;        HttpSession session=request.getSession();        session.setAttribute(session.getId(), user);        System.out.println(session.getId());        return MicroResponse.success(user);    }    @GetMapping(&quot;/logout&quot;)    public Object logout(HttpServletRequest request){        HttpSession session=request.getSession(false);        if(session!=null){            session.removeAttribute(session.getId());            System.out.println(session.getId());        }else            System.out.println(&quot;logout: session is null&quot;);        return MicroResponse.OK;    }    @GetMapping(&quot;/authentication&quot;)    public Object getAuthentication(HttpServletRequest request /*HttpSession session*/){        HttpSession session=request.getSession(false);        if(session!=null){            System.out.println(session.getId());            return MicroResponse.success(session.getAttribute(session.getId()));        }        System.out.println(&quot;getAuthentication: session is null&quot;);        return MicroResponse.success(null);    }}</code></pre><h3 id="header-21">Service &amp; Repository &amp; Entity</h3><p>UserService &amp; UserRepository &amp; User &amp; MicroResponse 均同上</p><h3 id="header-22">Run</h3><ol><li><p>main</p><pre><code class="lang-java"> @SpringBootApplication public class AuthServiceApplication {     public static void main(String[] args) {         SpringApplication.run(AuthServiceApplication.class, args);     } }</code></pre></li><li><p>Visit: <code>http://localhost:8080/micro-auth/session</code></p><ul><li>POST <code>/login</code><ul><li>body: <code>{&quot;name&quot;:&quot;Tom&quot;,&quot;password&quot;:&quot;123123&quot;}</code></li></ul></li><li>GET <code>/logout</code></li><li>GET <code>/authentication</code></li></ul></li></ol><h3 id="header-23">Verify</h3><ol><li><p>clear redis records</p><pre><code class="lang-bash"> redis:6379&gt; FLUSHALL OK redis:6379&gt; keys * (empty list or set)</code></pre></li><li><p>POST <code>/login</code></p><pre><code class="lang-bash"> &gt; curl -c cookie.txt -i -H &quot;Content-Type:application/json&quot; -X POST -d &#39;{&quot;name&quot;: &quot;admin&quot;, &quot;password&quot;:&quot;admin123&quot;}&#39; http://localhost:8080/micro-auth/session/login HTTP/1.1 200 Set-Cookie: SESSION=ODY1NDBhZDUtYzNmNy00NTg4LTg4ZjYtMDMxZWVlYzE2YTBm; Path=/micro-auth/; HttpOnly Content-Type: application/json;charset=UTF-8 Transfer-Encoding: chunked Date: Thu, 07 Feb 2019 15:17:57 GMT {&quot;success&quot;:true,&quot;code&quot;:1,&quot;data&quot;:{&quot;id&quot;:1,&quot;name&quot;:&quot;admin&quot;}} # check redis: redis:6379&gt; keys * 1) &quot;sps:sessions:86540ad5-c3f7-4588-88f6-031eeec16a0f&quot; 2) &quot;sps:sessions:expires:86540ad5-c3f7-4588-88f6-031eeec16a0f&quot; 3) &quot;sps:expirations:1549552860000&quot; redis:6379&gt; hgetall sps:sessions:36a41b20-a02e-4685-b359-a2b9b0aec2b1 1) &quot;creationTime&quot; 2) &quot;\xac\xed\x00\x05sr\x00\x0ejava.lang.Long;\x8b\xe4\x90\xcc\x8f#\xdf\x02\x00\x01J\x00\x05valuexr\x00\x10java.lang.Number\x86\xac\x95\x1d\x0b\x94\xe0\x8b\x02\x00\x00xp\x00\x00\x01h\xc8ef &quot; 3) &quot;maxInactiveInterval&quot; 4) &quot;\xac\xed\x00\x05sr\x00\x11java.lang.Integer\x12\xe2\xa0\xa4\xf7\x81\x878\x02\x00\x01I\x00\x05valuexr\x00\x10java.lang.Number\x86\xac\x95\x1d\x0b\x94\xe0\x8b\x02\x00\x00xp\x00\x00\x00\xb4&quot; 5) &quot;sessionAttr:36a41b20-a02e-4685-b359-a2b9b0aec2b1&quot; 6) &quot;\xac\xed\x00\x05sr\x00\x17com.cj.auth.entity.User\xc5\xbc\x00A\xb4\xb2z\x8c\x02\x00\x03L\x00\x02idt\x00\x13Ljava/lang/Integer;L\x00\x04namet\x00\x12Ljava/lang/String;L\x00\bpasswordq\x00~\x00\x02xpsr\x00\x11java.lang.Integer\x12\xe2\xa0\xa4\xf7\x81\x878\x02\x00\x01I\x00\x05valuexr\x00\x10java.lang.Number\x86\xac\x95\x1d\x0b\x94\xe0\x8b\x02\x00\x00xp\x00\x00\x00\x01t\x00\x05admint\x00\x18AZICOnu9cyUFFvBp3xi1AA==&quot; 7) &quot;lastAccessedTime&quot; 8) &quot;\xac\xed\x00\x05sr\x00\x0ejava.lang.Long;\x8b\xe4\x90\xcc\x8f#\xdf\x02\x00\x01J\x00\x05valuexr\x00\x10java.lang.Number\x86\xac\x95\x1d\x0b\x94\xe0\x8b\x02\x00\x00xp\x00\x00\x01h\xc8ef &quot;</code></pre></li><li><p>GET <code>/authentication</code></p><pre><code class="lang-bash"> &gt; curl -b cookie.txt -i -H &quot;Content-Type:application/json&quot; -X GET http://localhost:8080/micro-auth/session/authentication</code></pre></li><li><p>GET <code>/logout</code></p><pre><code class="lang-bash"> &gt; curl -b cookie.txt -i -H &quot;Content-Type:application/json&quot; -X GET http://localhost:8080/micro-auth/session/logout</code></pre></li></ol><h3 id="header-24">扩展：使用json方式序列化对象到Redis</h3><p>使用Jackson2JsonRedisSerializer解析Redis Value值</p><pre><code class="lang-java">@Configuration@EnableRedisHttpSession(maxInactiveIntervalInSeconds = 180,    redisFlushMode=RedisFlushMode.ON_SAVE,    redisNamespace=&quot;sps&quot;)public class RedisConfig {    /* @Bean RedisTemplate&lt;Object,Object&gt; jsonRedisTemplate : 同上面NoSession示例 */    /* SessionRepository:      * 定义了创建、保存、删除以及检索session的方法     * (将Session实例真正保存到数据存储的逻辑是在这个接口的实现中编码完成的)     *      * RedisOperationsSessionRepository implements SessionRepository:     * 会在Redis中创建、存储和删除session     *      * {@link RedisHttpSessionConfiguration}     */    /* Method1: */    //    @SuppressWarnings(&quot;unchecked&quot;)    //    @Bean    //    public SessionRepository&lt;?&gt; sessionRepository( @Qualifier(&quot;jsonRedisTemplate&quot;) RedisOperations&lt;Object, Object&gt; redisTemplate){    //        RedisOperationsSessionRepository sessionRepository =  new RedisOperationsSessionRepository(redisTemplate);    //        // sessionRepository.setDefaultSerializer(initJsonSerializer());    //        sessionRepository.setDefaultSerializer((RedisSerializer&lt;Object&gt;) redisTemplate.getValueSerializer());    //        sessionRepository.setDefaultMaxInactiveInterval(180);    //        sessionRepository.setRedisKeyNamespace(&quot;sps&quot;);    //        sessionRepository.setRedisFlushMode(RedisFlushMode.ON_SAVE);    //        System.out.println(&quot;Create Customer RedisOperationsSessionRepository --- &quot;);    //        return sessionRepository;    //    }    /* Method2 - Recomend */    @Bean    public RedisSerializer&lt;Object&gt; springSessionDefaultRedisSerializer(@Qualifier(&quot;jsonRedisTemplate&quot;) RedisOperations&lt;Object, Object&gt; redisTemplate){        return (RedisSerializer&lt;Object&gt;)redisTemplate.getValueSerializer();    }}</code></pre><h3 id="header-25">Verify again</h3><ol><li><p>POST <code>/login</code></p><pre><code class="lang-bash"> &gt; curl -c cookie.txt -i -H &quot;Content-Type:application/json&quot; -X POST -d &#39;{&quot;name&quot;: &quot;admin&quot;, &quot;password&quot;:&quot;admin123&quot;}&#39; http://localhost:8080/micro-auth/session/login HTTP/1.1 200 Set-Cookie: SESSION=MzAxZWJkNjMtOWYzMy00NWJiLWFhZjMtMGM0ZjJlMzIyYWM1; Path=/micro-auth/; HttpOnly Content-Type: application/json;charset=UTF-8 Transfer-Encoding: chunked Date: Thu, 07 Feb 2019 16:27:43 GMT {&quot;success&quot;:true,&quot;code&quot;:1,&quot;data&quot;:{&quot;id&quot;:1,&quot;name&quot;:&quot;admin&quot;}} # check redis: redis:6379&gt; keys * 1) &quot;sps:sessions:293d00d7-359f-4b82-87be-bbbe1fc64c8d&quot; 2) &quot;sps:sessions:expires:293d00d7-359f-4b82-87be-bbbe1fc64c8d&quot; 3) &quot;sps:expirations:1549558500000&quot; redis:6379&gt; hgetall sps:sessions:293d00d7-359f-4b82-87be-bbbe1fc64c8d 1) &quot;creationTime&quot; 2) &quot;1549558319123&quot; 3) &quot;maxInactiveInterval&quot; 4) &quot;180&quot; 5) &quot;sessionAttr:293d00d7-359f-4b82-87be-bbbe1fc64c8d&quot; 6) &quot;[\&quot;com.cj.auth.entity.User\&quot;,{\&quot;id\&quot;:1,\&quot;name\&quot;:\&quot;admin\&quot;,\&quot;password\&quot;:\&quot;AZICOnu9cyUFFvBp3xi1AA==\&quot;}]&quot; 7) &quot;lastAccessedTime&quot; 8) &quot;1549558319123&quot; redis:6379&gt; ttl sps:sessions:expires:293d00d7-359f-4b82-87be-bbbe1fc64c8d (integer) 158 redis:6379&gt; smembers sps:expirations:1549558500000 1) &quot;\&quot;expires:293d00d7-359f-4b82-87be-bbbe1fc64c8d\&quot;&quot; # check cookie &gt; cat cookie.txt # Netscape HTTP Cookie File # http://curl.haxx.se/docs/http-cookies.html # This file was generated by libcurl! Edit at your own risk. HttpOnly_localhost    FALSE    /micro-auth/    FALSE    0    SESSION    MzAxZWJkNjMtOWYzMy00NWJiLWFhZjMtMGM0ZjJlMzIyYWM1</code></pre></li><li><p>GET <code>/authentication</code></p><pre><code class="lang-bash"> &gt; curl -b cookie.txt -i -H &quot;Content-Type:application/json&quot; -X GET http://localhost:8080/micro-auth/session/authentication HTTP/1.1 200 Content-Type: application/json;charset=UTF-8 Transfer-Encoding: chunked Date: Thu, 07 Feb 2019 16:27:48 GMT {&quot;success&quot;:true,&quot;code&quot;:1,&quot;data&quot;:{&quot;id&quot;:1,&quot;name&quot;:&quot;admin&quot;}} # check redis: redis:6379&gt; keys * 1) &quot;sps:expirations:1549558560000&quot;        # new 2) &quot;sps:sessions:293d00d7-359f-4b82-87be-bbbe1fc64c8d&quot; 3) &quot;sps:sessions:expires:293d00d7-359f-4b82-87be-bbbe1fc64c8d&quot; redis:6379&gt; hgetall sps:sessions:293d00d7-359f-4b82-87be-bbbe1fc64c8d 1) &quot;creationTime&quot; 2) &quot;1549558319123&quot; 3) &quot;maxInactiveInterval&quot; 4) &quot;180&quot; 5) &quot;sessionAttr:293d00d7-359f-4b82-87be-bbbe1fc64c8d&quot; 6) &quot;[\&quot;com.cj.auth.entity.User\&quot;,{\&quot;id\&quot;:1,\&quot;name\&quot;:\&quot;admin\&quot;,\&quot;password\&quot;:\&quot;AZICOnu9cyUFFvBp3xi1AA==\&quot;}]&quot; 7) &quot;lastAccessedTime&quot; 8) &quot;1549558358599&quot;                        # changed redis:6379&gt; ttl sps:sessions:expires:293d00d7-359f-4b82-87be-bbbe1fc64c8d (integer) 166                            # changed</code></pre></li><li><p>GET <code>/logout</code></p><pre><code class="lang-bash"> &gt; curl -b cookie.txt -i -H &quot;Content-Type:application/json&quot; -X GET http://localhost:8080/micro-auth/session/logout HTTP/1.1 200 Content-Type: application/json;charset=UTF-8 Transfer-Encoding: chunked Date: Thu, 07 Feb 2019 16:28:38 GMT {&quot;success&quot;:true,&quot;code&quot;:1,&quot;data&quot;:null} # check redis: redis:6379&gt; keys * 1) &quot;sps:sessions:293d00d7-359f-4b82-87be-bbbe1fc64c8d&quot; 2) &quot;sps:expirations:1549558680000&quot;                                # new 3) &quot;sps:sessions:expires:293d00d7-359f-4b82-87be-bbbe1fc64c8d&quot; redis:6379&gt; hgetall sps:sessions:293d00d7-359f-4b82-87be-bbbe1fc64c8d 1) &quot;creationTime&quot; 2) &quot;1549558319123&quot; 3) &quot;maxInactiveInterval&quot; 4) &quot;180&quot; 5) &quot;sessionAttr:293d00d7-359f-4b82-87be-bbbe1fc64c8d&quot; 6) &quot;&quot;                                    # removed 7) &quot;lastAccessedTime&quot; 8) &quot;1549558447429&quot;                        # changed redis:6379&gt; ttl sps:sessions:expires:293d00d7-359f-4b82-87be-bbbe1fc64c8d (integer) 151</code></pre></li><li><p>GET <code>/authentication</code></p><pre><code class="lang-bash"> &gt; curl -b cookie.txt -i -H &quot;Content-Type:application/json&quot; -X GET http://localhost:8080/micro-auth/session/authentication HTTP/1.1 200 Content-Type: application/json;charset=UTF-8 Transfer-Encoding: chunked Date: Thu, 07 Feb 2019 16:29:58 GMT {&quot;success&quot;:true,&quot;code&quot;:1,&quot;data&quot;:null} # check redis: redis:6379&gt; keys * 1) &quot;sps:sessions:301ebd63-9f33-45bb-aaf3-0c4f2e322ac5&quot; redis:6379&gt; keys * (empty list or set)</code></pre></li></ol><h3 id="header-26">Client：简化测试版</h3><pre><code class="lang-java">@RunWith(SpringRunner.class)@SpringBootTest(webEnvironment=SpringBootTest.WebEnvironment.RANDOM_PORT)public class ServiceCallAuthNoSessionTest {    @Autowired    private TestRestTemplate restTemplate;    private String authURL=&quot;http://localhost:8080/micro-auth/session&quot;;    private String cookie=&quot;&quot;;    private String name=&quot;admin&quot;;    private String password=&quot;admin123&quot;;    @Test    public void callTest(){        //getAuthentication        callGetAuthentication();        // login        callLoginTest();        // callGetAuthentication        callGetAuthentication();        // logout        callLogoutTest();        // getAuthentication        callGetAuthentication();    }    @Test    public void callLoginTest(){        System.out.println(&quot;call login...&quot;);        HttpHeaders headers = new HttpHeaders();         headers.setContentType(MediaType.APPLICATION_JSON);        Map&lt;String,String&gt; userMap=new HashMap&lt;String,String&gt;();        userMap.put(&quot;name&quot;, name);        userMap.put(&quot;password&quot;, password);        HttpEntity&lt;Map&gt; entity=new HttpEntity&lt;Map&gt;(userMap,headers);        HttpEntity&lt;Map&gt; response=restTemplate.exchange(authURL+&quot;/login&quot;,HttpMethod.POST,entity,Map.class);        System.out.println(response);        cookie=(String)response.getHeaders().getFirst(HttpHeaders.SET_COOKIE);        System.out.println(&quot;cookie:&quot;+cookie);    }    @Test    public void callLogoutTest(){        System.out.println(&quot;call logout...&quot;);        HttpHeaders headers = new HttpHeaders();         headers.set(HttpHeaders.COOKIE,cookie);         headers.setContentType(MediaType.APPLICATION_JSON);        HttpEntity&lt;String&gt; entity=new HttpEntity&lt;String&gt;(null,headers);        HttpEntity&lt;String&gt; response=restTemplate.exchange(authURL+&quot;/logout&quot;,HttpMethod.GET,entity,String.class);        System.out.println(response);    }    @Test    public void callGetAuthentication(){        System.out.println(&quot;call getAuthentication...&quot;);        HttpHeaders headers = new HttpHeaders();         headers.set(HttpHeaders.COOKIE,cookie);         headers.setContentType(MediaType.APPLICATION_JSON);        HttpEntity&lt;String&gt; entity=new HttpEntity&lt;String&gt;(null,headers);        HttpEntity&lt;Map&gt; response=restTemplate.exchange(authURL+&quot;/authentication&quot;,HttpMethod.GET,entity,Map.class);        System.out.println(response);    }}</code></pre><p>Run Junit Test: callTest</p><pre><code>call getAuthentication...&lt;200,{success=true, code=1, data=null},{Set-Cookie=[SESSION=ZmM1NDQ1M2EtZTU4ZC00NTNmLWI5ODEtNWQ0YmUyODI3MmE0; Path=/micro-auth/; HttpOnly], Content-Type=[application/json;charset=UTF-8], Transfer-Encoding=[chunked], Date=[Sat, 09 Feb 2019 07:02:26 GMT]}&gt;call login...&lt;200,{success=true, code=1, data={id=1, name=admin}},{Set-Cookie=[SESSION=ZmYxNDJiNTctNGU5Ny00MzFjLWFkMWYtNzkwMTJlMmUzZjIy; Path=/micro-auth/; HttpOnly], Content-Type=[application/json;charset=UTF-8], Transfer-Encoding=[chunked], Date=[Sat, 09 Feb 2019 07:02:26 GMT]}&gt;cookie:SESSION=ZmYxNDJiNTctNGU5Ny00MzFjLWFkMWYtNzkwMTJlMmUzZjIy; Path=/micro-auth/; HttpOnlycall getAuthentication...&lt;200,{success=true, code=1, data={id=1, name=admin}},{Content-Type=[application/json;charset=UTF-8], Transfer-Encoding=[chunked], Date=[Sat, 09 Feb 2019 07:02:26 GMT]}&gt;call logout...&lt;200,{&quot;success&quot;:true,&quot;code&quot;:1,&quot;data&quot;:null},{Content-Type=[application/json;charset=UTF-8], Transfer-Encoding=[chunked], Date=[Sat, 09 Feb 2019 07:02:26 GMT]}&gt;call getAuthentication...&lt;200,{success=true, code=1, data=null},{Content-Type=[application/json;charset=UTF-8], Transfer-Encoding=[chunked], Date=[Sat, 09 Feb 2019 07:02:26 GMT]}&gt;</code></pre><h2 id="header-27">Reference</h2><p><a href="https://github.com/sixDegree/micro-demo" target="_blank" rel="noopener">My demo: auth-demo</a></p>]]></content>
    
    <summary type="html">
    
      &lt;ol&gt;
&lt;li&gt;SpringBoot Redis + Demo&lt;/li&gt;
&lt;li&gt;应用：Session持久化：NoSession方案,SpringSession方案&lt;/li&gt;
&lt;li&gt;方案：NoSession示例&lt;/li&gt;
&lt;li&gt;方案：SpringSession示例&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="MicroService" scheme="http://sixdegree.github.io/tags/MicroService/"/>
    
  </entry>
  
</feed>
