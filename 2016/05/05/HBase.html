<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>HBase</title>
  
  <!-- Meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
    <meta name="keywords" content="hbase,nosql,bigdata,hadoop">
  
  
    <meta name="description" content="HBase introduction">
  

  <!-- Feed -->
  
    <link rel="alternative" href="/atom.xml" title="SixDegree" type="application/atom+xml">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/bootstrap/dist/css/bootstrap.css">
  
    <link rel="stylesheet" href="/highlight/demo/styles/tomorrow-night-bright.css">
  
  <link rel="stylesheet" href="/css/fontello.css">
  <link rel="stylesheet" href="/css/style.css">

  <!-- Site Analyse -->
  
	<script>
	var userID='2bbb83cc0f781dd7502e9d5e19661866';
	var _hmt = _hmt || [];
	(function() {
	  var hm = document.createElement("script");
	  hm.src = "//hm.baidu.com/hm.js?"+userID;
	  var s = document.getElementsByTagName("script")[0]; 
	  s.parentNode.insertBefore(hm, s);
	})();
	</script>


</head>

<body data-spy="scroll" data-target="#nav-catalog">
  <div id="top-push"></div>
<a href="#top-push" id="go-top">
	<span class="glyphicon glyphicon-chevron-up"></span>
</a>
  <aside id="sidebar">
    <section class="sidebar-header">Catalog</section>
     <nav id="nav-catalog">
        <ol class="sidebar-nav nav"><li class="sidebar-nav nav-item sidebar-nav nav-level-2"><a class="sidebar-nav nav-link" href="#概述"><span class="sidebar-nav nav-text">概述</span></a><ol class="sidebar-nav nav-child"><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#数据模型"><span class="sidebar-nav nav-text">数据模型</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#架构体系"><span class="sidebar-nav nav-text">架构体系</span></a></li></ol></li><li class="sidebar-nav nav-item sidebar-nav nav-level-2"><a class="sidebar-nav nav-link" href="#安装"><span class="sidebar-nav nav-text">安装</span></a><ol class="sidebar-nav nav-child"><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#伪分布式"><span class="sidebar-nav nav-text">伪分布式</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#集群式"><span class="sidebar-nav nav-text">集群式</span></a></li></ol></li><li class="sidebar-nav nav-item sidebar-nav nav-level-2"><a class="sidebar-nav nav-link" href="#操作"><span class="sidebar-nav nav-text">操作</span></a><ol class="sidebar-nav nav-child"><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#Shell-Cmd"><span class="sidebar-nav nav-text">Shell Cmd</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#Java-API"><span class="sidebar-nav nav-text">Java API</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#结合MapReduce操作"><span class="sidebar-nav nav-text">结合MapReduce操作</span></a></li></ol></li></ol>
    </nav>
  </aside>
  <span id="sidebar-ctrl" class="glyphicon glyphicon-list-alt circle"></span>
  <div id="wrapper">
    <header>
  <nav class="navbar navbar-default">
    <div class="container-fluid">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#nav-menu" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="/">SixDegree</a>
      </div>
      <div class="collapse navbar-collapse" id="nav-menu">
        <ul class="nav navbar-nav navbar-right">
          
              <li  >
                <a href="/">Blogs</a>
              </li>
          
              <li  >
                <a href="/tags.html">Tags</a>
              </li>
          
              <li  >
                <a href="/about.html">About</a>
              </li>
          
          
              <li>
                <a href="/atom.xml" target="_blank">
                  <span class="icon-rss"></span>
                </a>
              </li>
          
              <li>
                <a href="http://github.com/sixdegree" target="_blank">
                  <span class="icon-github"></span>
                </a>
              </li>
          
        </ul>
      </div>
    </div>
  </nav>
</header>



    <div class="container">
      <article class="detail" role="main">
  <section class="post-header">
    <h1 class="post-title">HBase</h1>
    <ul class="post-meta">
      <li>
        <span class="glyphicon glyphicon-calendar"></span>
        <time datetime="2016-05-04T16:00:00.000Z">2016-05-05</time>
      </li>
      
        <li>
         <span class="glyphicon glyphicon-tags"></span>
          
            <a href="/tags.html#tag-NoSql">NoSql</a>
          
            <a href="/tags.html#tag-BigData">BigData</a>
          
        </li>
      
    </ul>
  </section>
  <section class="post-content">
    <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>HBase：Hadoop Database</p>
<ul>
<li>一种在Hadoop之上的NoSQL 的Key/vale数据库,适合实时查询</li>
<li>利用Hadoop HDFS作为其文件存储系统</li>
<li>利用Hadoop MapReduce来处理其海量数据</li>
<li>利用Zookeeper作为协调工具</li>
<li>是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统</li>
<li>适合海量数据(如20PB)的秒级简单查询的数据库<ul>
<li>适合key-value查询</li>
<li>适合按时间排序top n的场景</li>
<li>适合大量读写</li>
</ul>
</li>
</ul>
<h3 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h3><ul>
<li><p>数据模型<br>  <img src="2016-05-05-HBase/datamodal.png" alt="Data Modal"><br>  <img src="2016-05-05-HBase/keyvalue.png" alt="Key Value"></p>
<ul>
<li>Table:存储管理数据</li>
<li><code>RowKey</code>:行键（类似于关系型数据库中的主键）</li>
<li><code>ColumnFamily</code>：列族（定义表时指定），可包含任意多个列（插入记录时动态增加）</li>
<li><code>Cell</code>：单元格，由<code>{rowKey,columnFamily:columnName}</code>确定的存储单元<ul>
<li>可存储一份数据的多个版本，由<code>Timestamp（时间戳）</code>属性区分，即数据具有版本特性</li>
<li>由<code>{rowKey, columnFamily:columnName, version}</code>可确定某一版的Data</li>
<li>若不指定时间戳或者版本，默认取最新的数据</li>
</ul>
</li>
</ul>
</li>
<li><p>逻辑数据模型<br>  <img src="2016-05-05-HBase/logicalmodal.png" alt="Logical Modal"></p>
</li>
<li><p>物理数据模型<br>  <img src="2016-05-05-HBase/physicalmodal.png" alt="Physical Modal"></p>
</li>
<li><p>说明：</p>
<ul>
<li>存储划分：<ul>
<li><code>Table</code>按<code>RowKey</code>范围<code>[startKey,endKey)</code>划分成N个<code>Region</code><ul>
<li>各个<code>Region</code>分散存储在不同的<code>RegionServer</code>（单独的物理机器）中</li>
<li>这样对表的操作转化为对多台<code>RegionServer</code>的并行操作</li>
</ul>
</li>
<li><code>Region</code>按<code>ColumnFamily</code>一对一划分成<code>Store</code>,每个store包括:<ul>
<li><code>MemStore</code> 内存存储 （先，达到阀值后，写入StoreFile）<ul>
<li><code>StoreFile</code> 文件存储（对应一个<code>HFile</code>，存放在Hadoop HDFS）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>存储结构：<ul>
<li><code>RowKey</code>,<code>ColumnName</code>按字典顺序物理存储</li>
<li><code>Timestamp</code>是一个64位整数</li>
<li>所有数据以<code>byte[]</code>存储</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="2016-05-05-HBase/region.png" alt="Region"></p>
<h3 id="架构体系"><a href="#架构体系" class="headerlink" title="架构体系"></a>架构体系</h3><p>主从式结构</p>
<ul>
<li>Master（主）：可以启动多个，由Zookeeper的Master Election机制保证总有一个Master运行<ul>
<li>管理RegionServer</li>
<li>分配Region</li>
</ul>
</li>
<li>RegionServer（从）：一个物理节点一个<ul>
<li>存储Region</li>
<li>响应用户I/O请求，向HDFS文件系统中读写数据</li>
<li>合并切分Region（StoreFile）</li>
</ul>
</li>
<li>Zookeeper（协调）<ul>
<li>保证集群中只有一个Running Master<ul>
<li>监控RegionServer的状态，实时通知给Master</li>
<li>存储HBase的Schema（包括有哪些Table，每个Table有哪些ColumnFamily）</li>
<li>存储Region寻址入口（即<code>-ROOT-</code>表的location）</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="2016-05-05-HBase/frame1.png" alt="Frame"></p>
<p><img src="2016-05-05-HBase/frame2.png" alt="Frame"></p>
<p>HRegion 进程：</p>
<ul>
<li>HLog ：预写式日志（所有更新操作先记录进日志，再操作），用于做灾难恢复Failover</li>
<li>Store ：每个Store存放一个列族<ul>
<li>MemStore 内存存储 （先，达到阀值后，写入StoreFile）</li>
<li>StoreFile 文件存储（对应一个HFile，存放在Hadoop HDFS）<ul>
<li>每次写入就形成一份单独的<ul>
<li>数量增长到一定阀值：合并StoreFile（合并时会进行版本合并和删除工作）– HDFS适合存储大文件</li>
<li>大小超过一定阀值：分割当前Region（再由HMaster分配到其他RegionServer）– 实现负载均衡</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>注意：</p>
<ul>
<li>HBase中有两张特殊的Table（<code>-ROOT-</code>和<code>.META.</code>）<ul>
<li><code>.META.</code>：记录了<code>用户表</code>的Region信息，<code>.META.</code>表本身可划分成N个regoin</li>
<li><code>-ROOT-</code>：记录了<code>.META.</code>表划分成的N个Region的信息，<code>-ROOT-</code>本身只有一个，不划分<br><img src="2016-05-05-HBase/meta.png" alt="META"></li>
</ul>
</li>
<li>Client访问HBase上数据数据（并不需要master参与）:<code>zookeeper</code>=&gt;<code>-ROOT-</code>表=&gt;<code>.META.</code>表=&gt;Region位置=&gt;访问</li>
<li>Client包含访问HBase的接口,可通过维护着一些cache来加快对HBase的访问（比如缓存region的位置信息）</li>
</ul>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="伪分布式"><a href="#伪分布式" class="headerlink" title="伪分布式"></a>伪分布式</h3><ol>
<li>下载解压安装包</li>
<li>设置环境变量（<code>/etc/profile</code>文件）<ul>
<li><code>HBASE_HOME</code></li>
<li><code>PATH</code></li>
</ul>
</li>
<li>配置（<code>$HBASE_HOME/conf</code>目录下）<ul>
<li><code>hbase-env.sh</code><pre><code class="nullvim">  export JAVA_HOME=/usr/local/jdk
  # 使用HBase内置的Zookeeper
  export HBASE_MANAGES_ZK=true
</code></pre>
</li>
<li><code>hbase-site.xml</code><pre><code class="nullxml">  &lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;hdfs://hadoop0:9000/hbase&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
    &lt;value&gt;hadoop0&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
  &lt;/property&gt;
</code></pre>
</li>
</ul>
</li>
<li>启动<ul>
<li>运行Hadoop</li>
<li>运行HBase: <code>start-hbase.sh</code></li>
</ul>
</li>
<li>验证<ul>
<li><code>jps</code> 三个进程：HMaster、HRegionServer、HQuorumPeer</li>
<li><code>http://hadoop0:60010</code></li>
</ul>
</li>
</ol>
<h3 id="集群式"><a href="#集群式" class="headerlink" title="集群式"></a>集群式</h3><p>（在原来的hadoop0上的hbase伪分布基础上进行搭建）</p>
<p>例如：</p>
<ul>
<li>Master：hadoop0</li>
<li>RegionServer：hadoop1,hadoop2</li>
</ul>
<ol>
<li>配置（<code>$HBASE_HOME/conf</code>目录下）<ul>
<li><code>hbase-env.sh</code><pre><code class="nullvim">  export JAVA_HOME=/usr/local/jdk
  # 不使用HBase内置的Zookeeper
  export HBASE_MANAGES_ZK=false
</code></pre>
</li>
<li><code>hbase-site.xml</code><pre><code class="nullxml">  ...
  &lt;!-- 配置Zookeeper监控管理的节点（hostname）--&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
    &lt;value&gt;hadoop0,hadoop1,hadoop2&lt;/value&gt;
  &lt;/property&gt;
</code></pre>
</li>
<li><code>regionservers</code> （配置RegionServer的hostname）<pre><code class="nullvim">  hadoop1
  hadoop2
</code></pre>
</li>
</ul>
</li>
<li>复制配置好的hbase到其他节点<pre><code class="nullvim"> scp -rp ./hbase `hadoop`@hadoop1:~
 scp -rp ./hbase `hadoop`@hadoop2:~
</code></pre>
</li>
<li>启动<ul>
<li>运行Hadoop</li>
<li>运行zookeeper</li>
<li>运行HBase: <code>start-hbase.sh</code></li>
</ul>
</li>
</ol>
<h2 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h2><p>注意：HBase不能支持where条件、Order by 查询，只支持按照Row key来查询，但是可以通过HBase提供的API进行条件过滤</p>
<h3 id="Shell-Cmd"><a href="#Shell-Cmd" class="headerlink" title="Shell Cmd"></a>Shell Cmd</h3><ul>
<li>启用HBase Shell命令行<pre><code class="nullshell">  &gt; hbase shell
  ...
  &gt; quite
</code></pre>
</li>
<li>常用命令<br>  |名称|命令表达式|<br>  |——|————-|<br>  |创建表| create ‘表名称’, ‘列族名称1’,’列族名称2’,’列族名称N’|<br>  |添加记录| put ‘表名称’, ‘行键’, ‘列族名称:列名称’, ‘值’|<br>  |查看记录| get ‘表名称’, ‘行键’|<br>  |查看表中的记录总数| count ‘表名称’|<br>  |删除记录 | delete ‘表名’ ,’行键’ , ‘列名称’|<br>  |删除一张表| 先要屏蔽该表，才能对该表进行删除，第一步 disable ‘表名称’ 第二步 drop ‘表名称’|<br>  |查看所有记录| scan “表名称” |<br>  |查看某个表某个列中所有数据| scan “表名称” , {COLUMNS=&gt;’列族名称:列名称’} |<br>  |更新记录| 就是重写一遍进行覆盖 |</li>
<li>注意：<ul>
<li>HBase其实没有delete操作，只有insert，以Timestamp区分新旧记录</li>
<li>为充分利用分布式，可使用reverse key，hash，复合行键等技巧改造行键</li>
<li>行键打乱后，可更均匀随机的分配到各节点，而不是集中在一个节点</li>
</ul>
</li>
</ul>
<p>操作示例：</p>
<ul>
<li><p>表操作：</p>
<ul>
<li>创建表<pre><code class="nullshell">  &gt; create &#39;users&#39;,&#39;user_id&#39;,&#39;address&#39;,&#39;info&#39;
</code></pre>
</li>
<li>查看表<pre><code class="nullshell">  &gt; list
  &gt; describe &#39;users&#39;
</code></pre>
</li>
<li>验证表<pre><code class="nullshell">  &gt; exists &#39;users&#39;
  &gt; is_enabled &#39;users&#39;
  &gt; is_disabled &#39;users&#39;
</code></pre>
</li>
<li>删除表<pre><code class="nullshell">  &gt; disable &#39;users&#39;
  &gt; delete &#39;users&#39;
</code></pre>
</li>
</ul>
</li>
<li><p>记录操作：</p>
<ul>
<li>插入<pre><code class="nullshell">  &gt; put &#39;users&#39;,&#39;xiaoming&#39;,&#39;info:age&#39;,&#39;24&#39;;
  &gt; put &#39;users&#39;,&#39;xiaoming&#39;,&#39;info:birthday&#39;,&#39;1987-06-17&#39;;
  &gt; put &#39;users&#39;,&#39;xiaoming&#39;,&#39;info:company&#39;,&#39;alibaba&#39;;
  &gt; put &#39;users&#39;,&#39;xiaoming&#39;,&#39;address:contry&#39;,&#39;china&#39;;
  &gt; put &#39;users&#39;,&#39;xiaoming&#39;,&#39;address:province&#39;,&#39;zhejiang&#39;;
  &gt; put &#39;users&#39;,&#39;xiaoming&#39;,&#39;address:city&#39;,&#39;hangzhou&#39;;
  &gt; put &#39;users&#39;,&#39;zhangyifei&#39;,&#39;info:birthday&#39;,&#39;1987-4-17&#39;;
  &gt; put &#39;users&#39;,&#39;zhangyifei&#39;,&#39;info:favorite&#39;,&#39;movie&#39;;
  &gt; put &#39;users&#39;,&#39;zhangyifei&#39;,&#39;info:company&#39;,&#39;alibaba&#39;;
  &gt; put &#39;users&#39;,&#39;zhangyifei&#39;,&#39;address:contry&#39;,&#39;china&#39;;
  &gt; put &#39;users&#39;,&#39;zhangyifei&#39;,&#39;address:province&#39;,&#39;guangdong&#39;;
  &gt; put &#39;users&#39;,&#39;zhangyifei&#39;,&#39;address:city&#39;,&#39;jieyang&#39;;
  &gt; put &#39;users&#39;,&#39;zhangyifei&#39;,&#39;address:town&#39;,&#39;xianqiao&#39;;
</code></pre>
</li>
<li><p>查询</p>
<pre><code class="nullshell">  &gt; get &#39;users&#39;,&#39;xiaoming&#39;
  &gt; get &#39;users&#39;,&#39;xiaoming&#39;,&#39;info&#39;
  &gt; get &#39;users&#39;,&#39;xiaoming&#39;,&#39;info:age&#39;

  # 获取单元格数据的版本数据
  &gt; get &#39;users&#39;,&#39;xiaoming&#39;,{COLUMN=&gt;&#39;info:age&#39;,VERSIONS=&gt;1}
  &gt; get &#39;users&#39;,&#39;xiaoming&#39;,{COLUMN=&gt;&#39;info:age&#39;,VERSIONS=&gt;2}
  &gt; get &#39;users&#39;,&#39;xiaoming&#39;,{COLUMN=&gt;&#39;info:age&#39;,VERSIONS=&gt;3}

  # 获取单元格数据的某个版本数据
  &gt; get &#39;users&#39;,&#39;xiaoming&#39;,{COLUMN=&gt;&#39;info:age&#39;,TIMESTAMP=&gt;1364874937056}

  # 全表扫描
  &gt; scan &#39;users&#39;

  # 统计表的行数
  &gt; count &#39;users&#39;
</code></pre>
</li>
<li>更新<pre><code class="nullshell">  &gt; put &#39;users&#39;,&#39;xiaoming&#39;,&#39;info:age&#39; ,&#39;29&#39;
  &gt; get &#39;users&#39;,&#39;xiaoming&#39;,&#39;info:age&#39;
</code></pre>
</li>
<li><p>删除</p>
<pre><code class="nullshell">  # 删除某列值
  &gt; delete &#39;users&#39;,&#39;xiaoming&#39;,&#39;info:age&#39;

  # 删除整行
  &gt;deleteall &#39;users&#39;,&#39;xiaoming&#39;

  # 清空
  &gt; truncate &#39;users&#39;
</code></pre>
</li>
</ul>
</li>
</ul>
<h3 id="Java-API"><a href="#Java-API" class="headerlink" title="Java API"></a>Java API</h3><p>依赖包：</p>
<pre><code class="nullxml">&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;
    &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;
    &lt;version&gt;${hbase.version}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>使用示例：</p>
<pre><code class="nulljava">Configuration conf = HBaseConfiguration.create();
conf.set(&quot;hbase.rootdir&quot;, &quot;hdfs://hadoop0:9000/hbase&quot;);
//使用eclipse时必须添加这个，否则无法定位
conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop0&quot;);
HBaseAdmin admin = new HBaseAdmin(conf);


String tableName=&quot;users&quot;;
String columnFamily=&quot;info&quot;;

// 1. 创建一张表
if (admin.tableExists(tableName)) {
    System.out.println(&quot;table exists!&quot;);
}else{
    HTableDescriptor tableDesc = new HTableDescriptor(tableName);
    tableDesc.addFamily(new HColumnDescriptor(columnFamily));
    admin.createTable(tableDesc);
    System.out.println(&quot;create table success!&quot;);
}

// 2. 添加一条记录
String rowKey=&quot;xiaoming&quot;;
String column=&quot;age&quot;;
String data=&quot;24&quot;;

HTable table = new HTable(conf, tableName);
Put p1 = new Put(Bytes.toBytes(rowKey));
p1.add(Bytes.toBytes(columnFamily), Bytes.toBytes(column),     Bytes.toBytes(data));
table.put(p1);
System.out.println(&quot;put&#39;&quot;+rowKey+&quot;&#39;,&quot;+columnFamily+&quot;:&quot;+column+&quot;&#39;,&#39;&quot;+data+&quot;&#39;&quot;);

// 3. 读取一条记录
Get get = new Get(Bytes.toBytes(rowKey));
Result result = table.get(get);
System.out.println(&quot;Get: &quot;+result);

// 4. 显示所有数据
Scan scan = new Scan();
ResultScanner scanner = table.getScanner(scan);
/*scan.setStartRow(Bytes.toBytes(&quot;134/&quot;));
scan.setStopRow( Bytes.toBytes(&quot;134:&quot;));
scan.setMaxVersions(1);*/
for (Result res : scanner) {
    System.out.println(&quot;Scan: &quot;+res    );
}
table.close();

// 5. 删除表
if(admin.tableExists(tableName)){
    try {
      admin.disableTable(tableName);
      admin.deleteTable(tableName);
    } catch (IOException e) {
      e.printStackTrace();
      System.out.println(&quot;Delete &quot;+tableName+&quot; 失败&quot;);
    }
}
admin.close();
System.out.println(&quot;Delete &quot;+tableName+&quot; 成功&quot;);
</code></pre>
<h3 id="结合MapReduce操作"><a href="#结合MapReduce操作" class="headerlink" title="结合MapReduce操作"></a>结合MapReduce操作</h3><p>Hbase对Mapreduce API进行了扩展，方便Mapreduce任务读写HTable数据</p>
<table>
<thead>
<tr>
<th>HBase MapReduce</th>
<th>Hadoop MapReduce</th>
</tr>
</thead>
<tbody>
<tr>
<td>org.apache.hadoop.hbase.mapreduce.TableMapper</td>
<td>org.apache.hadoop.mapreduce.Mapper</td>
</tr>
<tr>
<td>org.apache.hadoop.hbase.mapreduce.TableReducer</td>
<td>org.apache.hadoop.mapreduce.Reducer</td>
</tr>
<tr>
<td>org.apache.hadoop.hbase.mapreduce.TableInputFormat</td>
<td>org.apache.hadoop.mapreduce.InputFormat</td>
</tr>
<tr>
<td>org.apache.hadoop.hbase.mapreduce.TableOutputFormat</td>
<td>org.apache.hadoop.mapreduce.OutputFormat</td>
</tr>
</tbody>
</table>
<p>依赖包：</p>
<pre><code class="nullxml">&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;
    &lt;artifactId&gt;hbase-server&lt;/artifactId&gt;
    &lt;version&gt;${hbase.version}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>使用示例1：统计</p>
<pre><code class="nulljava">public static void main(String[] args) throws Exception {   
    Configuration config = HBaseConfiguration.create(); 
    configuration.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop0&quot;);

    Job job = new Job(config,&quot;HBaseTotal&quot;);  
    job.setJarByClass(TotalOnHBase.class); 

    Scan scan = new Scan();  
    scan.setCaching(500);
    scan.setCacheBlocks(false);
    TableMapReduceUtil.initTableMapperJob(&quot;access-log&quot;,scan,MyTableMapper.class,Text.class,IntWritable.class,job);  
    TableMapReduceUtil.initTableReducerJob(&quot;total-access&quot;,MyTableReducer.class,job);  
    job.waitForCompletion(true);
}
</code></pre>
<pre><code class="nulljava">public class MyTableMapper extends TableMapper&lt;Text, IntWritable&gt;  {  
    private final IntWritable ONE = new IntWritable(1);  
    private Text text = new Text();  
    public void map(ImmutableBytesWritable row, Result value, Context context) throws IOException, InterruptedException {  
        String ip = Bytes.toString(row.get()).split(&quot;-&quot;)[0];  
        String url = new String(value.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;url&quot;)));  
        text.set(ip+&quot;&amp;&quot;+url);  
        context.write(text, ONE);  
    }  
}
</code></pre>
<pre><code class="nulljava">public static class MyTableReducer extends TableReducer&lt;Text, IntWritable, ImmutableBytesWritable&gt;  {  
    public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {  
        int sum = 0;  
        for (IntWritable val : values) {  
            sum += val.get();  
        }  

        Put put = new Put(key.getBytes());  
        put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;count&quot;), Bytes.toBytes(String.valueOf(sum)));  

        context.write(null, put);  
    }  
}
</code></pre>
<p>使用示例2：批量导入（<code>TableOutputFormat</code>,<code>TableReducer</code>）</p>
<pre><code class="nulljava">public static void main(String[] args) throws Exception {
    final Configuration configuration = new Configuration();
    //设置zookeeper
    configuration.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop0&quot;);
    //设置hbase表名称
    configuration.set(TableOutputFormat.OUTPUT_TABLE, &quot;wlan_log&quot;);
    //将该值改大，防止hbase超时退出
    configuration.set(&quot;dfs.socket.timeout&quot;, &quot;180000&quot;);

    final Job job = new Job(configuration, &quot;HBaseBatchImport&quot;);
    FileInputFormat.setInputPaths(job, &quot;hdfs://hadoop0:9000/input&quot;);
    job.setInputFormatClass(TextInputFormat.class);
    job.setMapperClass(BatchImportMapper.class);
    job.setMapOutputKeyClass(LongWritable.class);
    job.setMapOutputValueClass(Text.class);
    //BatchImportReducer extends TableReducer
    job.setReducerClass(BatchImportReducer.class);
    //不再设置输出路径，而是设置输出格式类型
    job.setOutputFormatClass(TableOutputFormat.class);
    job.waitForCompletion(true);
}
</code></pre>
<pre><code class="nulljava">// BatchImportMapper 数据清理，省略...
// BatchImportReducer 将数据写入HBase
public class BatchImportReducer extends TableReducer&lt;LongWritable, Text, NullWritable&gt;{
    protected void reduce(LongWritable key, Iterable&lt;Text&gt; values,Context context) throws IOException ,InterruptedException {
        for (Text text : values) {
            final String[] splited = text.toString().split(&quot;\t&quot;);
            final Put put = new Put(Bytes.toBytes(splited[0]));
            put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;age&quot;), Bytes.toBytes(splited[1]));
            //省略其他字段，调用put.add(....)即可
            context.write(NullWritable.get(), put);
        }
    };
}
</code></pre>
  </section>
</article>

      <hr/>
      <section class="post-comment">
	<!-- disqus默认将数据加载到id为'disqus_thread'的容器中，可配置disqus_container_id改变-->
<div id="disqus_thread"> 
	<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</div>

	<script type="text/javascript">
		var disqus_shortname = 'sixdegreespace'; 
		var disqus_identifier = '2016/05/05/HBase.html';	
		var disqus_title = 'HBase';
		var disqus_url = 'http://sixdegree.github.io/2016/05/05/HBase.html' ;
		//var disqus_category_id = '4262241'; 

		(function() {
		    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
		    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
		})();
	</script>


 
</section>
    </div>
  </div>
</body>

<script src="/jquery/dist/jquery.min.js"></script>
<script src="/bootstrap/dist/js/bootstrap.min.js"></script>
<script src="/highlight/highlight.pack.js"></script>
<script type="text/javascript">
  hljs.initHighlightingOnLoad();
  
  $(document).ready(function(){
    var sidebarCtrl=$("#sidebar-ctrl");
    var sidebar=$("#sidebar");
    var wrapper=$("#wrapper");
    sidebarCtrl.on("click",function(event){
        //alert("click");
        sidebar.toggleClass("sidebar-toggle");
        wrapper.toggleClass("sidebar-toggle");
        sidebarCtrl.toggleClass("sidebar-toggle");
        sidebarCtrl.toggleClass("active");
    })
  });
</script>


</html>
