<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Hive</title>
  
  <!-- Meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
    <meta name="keywords" content="Hive,hadoop,bigdata">
  
  
    <meta name="description" content="Hive introduction">
  

  <!-- Feed -->
  
    <link rel="alternative" href="/atom.xml" title="SixDegree" type="application/atom+xml">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/bootstrap/dist/css/bootstrap.css">
  
    <link rel="stylesheet" href="/highlight/demo/styles/tomorrow-night-bright.css">
  
  <link rel="stylesheet" href="/css/fontello.css">
  <link rel="stylesheet" href="/css/style.css">

  <!-- Site Analyse -->
  
	<script>
	var userID='2bbb83cc0f781dd7502e9d5e19661866';
	var _hmt = _hmt || [];
	(function() {
	  var hm = document.createElement("script");
	  hm.src = "//hm.baidu.com/hm.js?"+userID;
	  var s = document.getElementsByTagName("script")[0]; 
	  s.parentNode.insertBefore(hm, s);
	})();
	</script>


</head>

<body data-spy="scroll" data-target="#nav-catalog">
  <div id="top-push"></div>
<a href="#top-push" id="go-top">
	<span class="glyphicon glyphicon-chevron-up"></span>
</a>
  <aside id="sidebar">
    <section class="sidebar-header">Catalog</section>
     <nav id="nav-catalog">
        <ol class="sidebar-nav nav"><li class="sidebar-nav nav-item sidebar-nav nav-level-2"><a class="sidebar-nav nav-link" href="#概述"><span class="sidebar-nav nav-text">概述</span></a><ol class="sidebar-nav nav-child"><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#系统架构"><span class="sidebar-nav nav-text">系统架构</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#数据存储"><span class="sidebar-nav nav-text">数据存储</span></a></li></ol></li><li class="sidebar-nav nav-item sidebar-nav nav-level-2"><a class="sidebar-nav nav-link" href="#安装"><span class="sidebar-nav nav-text">安装</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-2"><a class="sidebar-nav nav-link" href="#HiveQL"><span class="sidebar-nav nav-text">HiveQL</span></a><ol class="sidebar-nav nav-child"><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#数据库"><span class="sidebar-nav nav-text">数据库</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#表"><span class="sidebar-nav nav-text">表</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#分区表"><span class="sidebar-nav nav-text">分区表</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#桶表"><span class="sidebar-nav nav-text">桶表</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#抽样"><span class="sidebar-nav nav-text">抽样</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#单表查询"><span class="sidebar-nav nav-text">单表查询</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#连接查询"><span class="sidebar-nav nav-text">连接查询</span></a></li></ol></li><li class="sidebar-nav nav-item sidebar-nav nav-level-2"><a class="sidebar-nav nav-link" href="#示例"><span class="sidebar-nav nav-text">示例</span></a><ol class="sidebar-nav nav-child"><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#示例1：导入Apache-log"><span class="sidebar-nav nav-text">示例1：导入Apache log</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#示例2：创建内部表"><span class="sidebar-nav nav-text">示例2：创建内部表</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#示例3：内部静态分区表"><span class="sidebar-nav nav-text">示例3：内部静态分区表</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#示例4：内部动态分区表"><span class="sidebar-nav nav-text">示例4：内部动态分区表</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#示例5：内部表分桶"><span class="sidebar-nav nav-text">示例5：内部表分桶</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#示例6：内部分区分桶表"><span class="sidebar-nav nav-text">示例6：内部分区分桶表</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#示例6：外部表"><span class="sidebar-nav nav-text">示例6：外部表</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#示例7：外部分区分桶表"><span class="sidebar-nav nav-text">示例7：外部分区分桶表</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#示例8：全排序"><span class="sidebar-nav nav-text">示例8：全排序</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-3"><a class="sidebar-nav nav-link" href="#示例9：Join"><span class="sidebar-nav nav-text">示例9：Join</span></a></li></ol></li><li class="sidebar-nav nav-item sidebar-nav nav-level-2"><a class="sidebar-nav nav-link" href="#优化策略"><span class="sidebar-nav nav-text">优化策略</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-2"><a class="sidebar-nav nav-link" href="#自定义函数"><span class="sidebar-nav nav-text">自定义函数</span></a></li><li class="sidebar-nav nav-item sidebar-nav nav-level-2"><a class="sidebar-nav nav-link" href="#Java-API"><span class="sidebar-nav nav-text">Java API</span></a></li></ol>
    </nav>
  </aside>
  <span id="sidebar-ctrl" class="glyphicon glyphicon-list-alt circle"></span>
  <div id="wrapper">
    <header>
  <nav class="navbar navbar-default">
    <div class="container-fluid">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#nav-menu" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="/">SixDegree</a>
      </div>
      <div class="collapse navbar-collapse" id="nav-menu">
        <ul class="nav navbar-nav navbar-right">
          
              <li  >
                <a href="/">Blogs</a>
              </li>
          
              <li  >
                <a href="/tags.html">Tags</a>
              </li>
          
              <li  >
                <a href="/about.html">About</a>
              </li>
          
          
              <li>
                <a href="/atom.xml" target="_blank">
                  <span class="icon-rss"></span>
                </a>
              </li>
          
              <li>
                <a href="http://github.com/sixdegree" target="_blank">
                  <span class="icon-github"></span>
                </a>
              </li>
          
        </ul>
      </div>
    </div>
  </nav>
</header>



    <div class="container">
      <article class="detail" role="main">
  <section class="post-header">
    <h1 class="post-title">Hive</h1>
    <ul class="post-meta">
      <li>
        <span class="glyphicon glyphicon-calendar"></span>
        <time datetime="2016-05-09T16:00:00.000Z">2016-05-10</time>
      </li>
      
        <li>
         <span class="glyphicon glyphicon-tags"></span>
          
            <a href="/tags.html#tag-BigData">BigData</a>
          
        </li>
      
    </ul>
  </section>
  <section class="post-content">
    <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul>
<li>建立在Hadoop基础上的数据仓库，能够管理查询Hadoop中的数据</li>
<li>本质上，Hive是一个SQL解析引擎，将SQL语句转译成M/R Job，在Hadoop执行</li>
<li>Hive的表其实就是HDFS的目录，字段即为文件中的列，可以直接在M/R Job里使用这些数据</li>
<li>在HDFS中的默认存放位置：/user/hive/warehouse（hive-conf.xml的hive.metastore.warehouse.dir属性）</li>
</ul>
<h3 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h3><p><img src="2016-05-10-Hive/frame.png" alt="Frame"></p>
<ul>
<li>用户接口<ul>
<li>CLI：Shell命令行</li>
<li>JDBC/ODBC：Java Connection （与使用传统数据库JDBC的方式类似）</li>
<li>WebGUI：通过浏览器访问 Hive</li>
</ul>
</li>
<li>MetaStore<ul>
<li>存储元数据（例如：表名，表属性，列属性，分区属性，数据所在路径等）</li>
<li>存储在数据库中，目前支持 mysql、derby(默认，内置)<ul>
<li>默认使用内嵌的derby数据库作为存储引擎</li>
<li>Derby引擎的一次只能打开一个会话</li>
<li>MySQL等外置存储引擎，可支持多用户同时访问</li>
</ul>
</li>
</ul>
</li>
<li>Driver<ul>
<li>包含解释器，编译器，优化器，执行器</li>
<li>完成HQL=&gt;Job，Trigger Exec</li>
</ul>
</li>
<li>Hadoop<ul>
<li>用 HDFS 进行存储</li>
<li>利用 MapReduce 进行计算</li>
</ul>
</li>
<li>注意：大部分的查询由 MapReduce 完成，但有些不是，例如<code>select * from table</code>（包含<code>*</code>的查询）</li>
</ul>
<p>现在Hive的整体框架，计算引擎不仅仅支持Map/Reduce，并且还支持Tez、Spark等。根<br>据不同的计算引擎又可以使用不同的资源调度和存储系统</p>
<p><img src="2016-05-10-Hive/frame-new.png" alt="Frame New"></p>
<h3 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h3><p>存储结构主要包括：数据库、文件、表、视图</p>
<ul>
<li>使用 HDFS 进行存储，无专门的数据存储格式，也没有为数据建立索引</li>
<li>用户只需在建表时，指定Hive数据的列分隔符与行分隔符，Hive即可解析数据文件为Table</li>
<li>默认可以直接加载文本文件（TextFile），也支持SequenceFile </li>
<li>每一个 Table 在 Hive 中都有一个相应的目录存储数据</li>
</ul>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>Hive可以安装在Hadoop集群中的任何一台机器上<br>metastore支持三种存储模式</p>
<ul>
<li>本地内嵌模式（默认）：元数据保持在Hive内嵌的derby中，只允许一个会话连接</li>
<li>本地独立模式：元数据保持在本地的一个DB中，允许多会话连接</li>
<li><p>远端独立模式：元数据保持在远程的一个DB中，允许多会话连接</p>
</li>
<li><p>安装Hive：</p>
<ol>
<li>下载解压</li>
<li>设置环境变量（<code>/etc/profile</code>文件）<ul>
<li><code>HIVE_HOME</code></li>
<li><code>HADOOP_HOME</code></li>
<li><code>PATH</code></li>
</ul>
</li>
<li>配置（<code>$HIVE_HOME/conf</code>目录下）<ul>
<li><code>hive-log4j.properties.template</code> =&gt; <code>hive-log4j.properties</code></li>
<li><code>hive-default.xml.template</code> =&gt; <code>hive-site.xml</code><pre><code class="nullxml">&lt;property&gt;
 &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;
 &lt;value&gt;/user/hive/warehouse&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;hive.exec.stagingdir&lt;/name&gt;
 &lt;value&gt;/tmp/hive/.hive-staging&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;hive.exec.scratchdir&lt;/name&gt;
 &lt;value&gt;/tmp/hive&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;
 &lt;value&gt;/tmp/hive&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;
 &lt;value&gt;/tmp/hive/${hive.session.id}_resources&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt;
 &lt;value&gt;/tmp/hive/operation_logs&lt;/value&gt;
&lt;/property&gt;
</code></pre>
</li>
</ul>
</li>
<li><p>HDFS上创建和授权目录</p>
<pre><code>hadoop fs -mkidr /tmp
hadoop fs -chmod g+w /tmp

hadoop fs -mkidr /user/hive/warehouse
hadoop fs -chmod g+w /user/hive/warehouse
</code></pre></li>
</ol>
</li>
<li><p>使用外部数据库（例如MySQL）作为Hive的metastore：</p>
<ol>
<li><p>安装MySQL</p>
<pre><code class="nullvim">&gt; rpm -qa |grep mysql             # 检查mysql
&gt; rpm -e --nodeps mysql           # 强力卸载mysql

&gt; rpm -i mysql-server-********    # 安装mysql服务端
&gt; rpm -i mysql-client-********    # 安装mysql客户端

&gt; mysqld_safe &amp;                   # 启动mysql 服务端  
&gt; mysql_secure_installation       # 设置root用户密码
</code></pre>
</li>
<li>mysql 连接权限修改<pre><code class="nullvim">&gt; mysql -u root -p
mysql&gt; use mysql;
mysql&gt; select host,user from user;
mysql&gt; grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;mypassword&#39; with grant option;
mysql&gt; select host,user from user;
mysql&gt; flush privileges;
</code></pre>
</li>
<li>添加mysql的jdbc驱动包（置于到<code>$HIVE_HOME/lib</code>目录下）</li>
<li>添加配置Hive （<code>$HIVE_HOME/conf/hive-site.xml</code>）<pre><code class="nullxml">&lt;property&gt;
 &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
 &lt;value&gt;jdbc:mysql://cj.storm:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
 &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
 &lt;value&gt;root&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
 &lt;value&gt;admin&lt;/value&gt;
&lt;/property&gt;
</code></pre>
</li>
</ol>
</li>
<li><p>运行Hive:</p>
<ul>
<li>保证Hadoop是启动状态</li>
<li>设置Hive运行模式<ul>
<li>分为本地与集群两种，可通过<code>mapred.job.tracker</code>参数设置</li>
<li>例如：<code>&gt; hive -e &quot;SET mapred.job.tracker=local&quot;</code></li>
</ul>
</li>
<li>启动Hive<ul>
<li>启动命令行 <code>&gt; hive --service cli</code>，同<code>&gt; hive</code></li>
<li>启动Web（port：9999） <code>&gt; hive --service hwi &amp;</code>，需要另外下载放入<code>hive-hwi</code>war包</li>
</ul>
</li>
<li>验证<ul>
<li>Hive命令<pre><code class="nullvim">&gt; hive -help
&gt; hive
hive&gt; show databases;
OK
default
hive&gt; exit;
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="HiveQL"><a href="#HiveQL" class="headerlink" title="HiveQL"></a>HiveQL</h2><h3 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h3><p>类似传统数据库的DataBase，系统默认使用数据库<code>default</code>，也可指定</p>
<pre><code class="nullvim">&gt; hive
hive&gt; create database &lt;数据库名&gt; ;
hive&gt; use &lt;数据库名&gt; ;
...
hive&gt; drop database  &lt;数据库名&gt;;
hive&gt; drop database  &lt;数据库名&gt; cascade;
</code></pre>
<h3 id="表"><a href="#表" class="headerlink" title="表"></a>表</h3><ul>
<li><p>查看表：</p>
<pre><code class="nullsql"># 查看所有的表
show tables; 
# 支持模糊查询
show tables &#39;*tmp*&#39;; 

# 查看表有哪些分区
show partitions tmp_tb; 

#查看表详情
describe tmp_tb; 
describe formatted tmp_tb;
</code></pre>
</li>
<li>修改表结构（alter）<pre><code class="nullsql">alter table tmp_tb add columns (cols,string);
alter table tmp_tb add if not exists partition(day=&#39;2016-04-01&#39;,city=&#39;wx&#39;);
alter table tmp_tb drop if exists partition (daytime=&#39;2016-05-01&#39;,city=&#39;sz&#39;);
alter table tmp_tb clustered by (ip) into 3 buckets;
</code></pre>
</li>
<li>删除表（drop）<pre><code class="nullsql">drop table
</code></pre>
</li>
<li>清空表 （truncate）<pre><code class="nullsql"># 无法清空外部表
truncate table table_name;  # 不指定分区，将清空表中的所有分区
truncate table table_name partition (dt=&#39;20080808&#39;);
</code></pre>
</li>
<li><p>视图（view）</p>
<pre><code class="nullsql">CREATE VIEW v1 AS select * from t1;
</code></pre>
</li>
<li><p>创建表</p>
<pre><code class="nullsql"># 创建表
Create [EXTERNAL] TABLE [IF NOT EXISTS] table_name 
  [(col_name data_type, ...)]
  [PARTITIONED BY (col_name data_type, ...)] 
  [
    CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] 
    INTO num_buckets BUCKETS
  ] 
  [ROW FORMAT DELIMITED row_format] 
  [STORED AS file_format] 
  [LOCATION hdfs_path]

# 复制表结构
CREATE [EXTERNAL] TABLE target_table LIKE source_table [LOCATION hdfs_path];
</code></pre>
<ul>
<li>EXTERNAL ：标识创建一个外部表</li>
<li>PARTITIONED By ：分区（每个分区列单独一个目录，分区列本身不会存储在数据文件中）</li>
<li>CLUSTERED By ：分桶，根据指定列的Hash值切分（一个桶一个数据文件，内容包含桶列）</li>
<li>ROW FORMAT DELIMITED ：指定数据分割符（默认只认单个字符）<ul>
<li>FIELDS TERMINATED BY</li>
<li>LINES TERMINATED BY</li>
<li>COLLECTION ITEMS TERMINATED BY</li>
<li>MAP KEYS TERMINATED BY</li>
<li>…</li>
</ul>
</li>
<li>STORED AS ：数据存储方式<ul>
<li>TEXTFILE 纯文本，不压缩</li>
<li>SEQUENCEFILE 序列化，压缩</li>
</ul>
</li>
<li>LOCATION ：创建表时就加载数据，指定数据文件所在位置（可选）</li>
</ul>
</li>
<li><p>加载数据(Load dataset)</p>
<ul>
<li>LOAD Cmd：将HDFS文件导入已创建的Hive表（加载时不做检查，查询时检查）<pre><code class="nullsql">LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] 
INTO TABLE tablename
[PARTITION (partcol1=val1, partcol2=val2 ...)]
</code></pre>
<pre><code class="nullsql">!hadoop fs -ls input/hive/stocks_db;
load data inpath &#39;input/hive/stocks_db&#39; into table stocks;
</code></pre>
</li>
<li>CTAS：将Hive查询结果存放入一个新创表，原子级（select失败，table不会创建），目标表不能是分区表和外部表<pre><code class="nullsql">CREATE TABLE [IF NOT EXISTS] table_name 
AS SELECT …
</code></pre>
<pre><code class="nullsql">create table stocks_ctas as select * from stocks;
</code></pre>
</li>
<li><p>INSERT…SELECT：将Hive查询结果存入一个已创表</p>
<pre><code class="nullsql">INSERT OVERWRITE|INTO TABLE tablename 
[PARTITION (partcol1=val1, partcol2=val2 ...)] 
select_statement FROM from_statement
</code></pre>
<pre><code class="nullsql">insert into table stocks_ctas select s.* from stocks s;
insert overwrite table stocks_ctas select s.* from stocks s;

# 可以在同一个查询中使用多个insert子句
from stocks_ctas
insert into t1 select id,name
insert into t2 select id,tel
where age&gt;25
</code></pre>
</li>
<li>LOCATION：指定Hive表数据文件位置（注意：不会再移动数据文件到Hive配置的数据仓库中）<pre><code class="nullsql">!hadoop fs -ls /user/hive/input/hive/stocks_db;
create table stocks_loc(...) 
row format delimited fields terminated by &#39;,&#39;
location &#39;/user/hive/input/hive/stocks_db&#39;;
</code></pre>
</li>
</ul>
</li>
</ul>
<p>说明：</p>
<ol>
<li><p>Hive中有两种性质的表（Table Type）：</p>
<ul>
<li>managed_table (Hive内部表) <ul>
<li>使用Load data命令插数据时，会将数据文件<code>移动</code>到数据仓库（由<code>hive-site.xml</code>配置的<code>hive.metastore.warehouse.dir</code>指定）</li>
<li>使用Drop table命令删除表时，元数据（metastore的db中）与对应的数据文件都会被删除</li>
</ul>
</li>
<li>external_table (Hive外部表)<ul>
<li>使用Load data命令插数据时，不会将数据文件<code>移动</code>到数据仓库</li>
<li>使用Drop table命令删除表时，只有元数据会被删除，实际数据文件不会有影响</li>
</ul>
</li>
</ul>
</li>
<li><p>Hive中表的数据类型：</p>
<ul>
<li>基本数据类型<ul>
<li>tinyint/smallint/int/bigint</li>
<li>float/double</li>
<li>boolean</li>
<li>string</li>
</ul>
</li>
<li>复杂数据类型<ul>
<li>Array/Map/Struct</li>
<li>没有date/datetime</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h3><pre><code class="nullsql"># 使用普通表，会scan整张表，效率低
select * from stocks where symbol=&#39;XYZ&#39; and ymd=&#39;2003-02-01&#39;;

# 使用分区表，会先找到对应分区列目录，效率高
select * from stocks_partition where symbol=&#39;XYZ&#39; and ymd=&#39;2003-02-01&#39;;
</code></pre>
<p>分区表（partition table）<br>粗粒度的划分，分区列成了目录（为虚拟列），条件查询时可定位到目录提高效率</p>
<ul>
<li>创建分区表<pre><code class="nullsql">create table if not exists stocks_partition(
  col1 string,
  col2 string,
  exch_name string,
  yr int
)
partitioned by (symbol string)
row format delimited fields terminated by &#39;,&#39; ;
</code></pre>
</li>
<li><p>加载数据</p>
<ul>
<li><p>Using <code>insert</code></p>
<pre><code class="nullsql"># add partition(symbol=B7J) data:
insert into table stocks_partition partition(symbol=&#39;B7J&#39;)
select col1,col2,exch_name,yr from stocks where symbol =&#39;B7J&#39;;

# add partition(symbol=BB3) data:
insert into table stocks_partition partition(symbol=&#39;BB3&#39;)
select col1,col2,exch_name,yr from stocks where symbol =&#39;BB3&#39;;

=&gt; 也合并成一个insert
from stocks
insert into table stocks_partition partition(symbol=&#39;B7J&#39;)
select col1,col2,exch_name,yr from stocks where symbol=&#39;B7J&#39;
insert into table stocks_partition partition(symbol=&#39;BB3&#39;)
select col1,col2,exch_name,yr from stocks where symbol =&#39;BB3&#39;;

# 注意：如下方式是错误的
insert overwrite table stocks_partition partition(symbol=&#39;APPL&#39;)
select col1,col2,exch_name,yr from stocks where symbol=&#39;ZUU&#39;;
</code></pre>
</li>
<li><p>Using <code>location</code></p>
<pre><code class="nullsql"># add partition(symbol=ZUU) data:
insert overwrite directory &#39;output/hive/stocks-zuu&#39;
select col1,col2,exch_name,yr from stocks where symbol=&#39;ZUU&#39;;

alter table stocks_partition add if not exists partition (symbol=&#39;ZUU&#39;) 
location &#39;/output/hive/stocks-zuu&#39;
</code></pre>
</li>
</ul>
</li>
<li>添加删除分区<pre><code class="nullsql">alter table stocks_partition add if not exists partition(symbol=&#39;ZUU&#39;);
alter table stocks_partition drop if exists partition(symbol=&#39;ZUU&#39;);
</code></pre>
</li>
<li>查看表分区 <pre><code class="nullsql">hive&gt; show partitions stocks_partition
OK
symbol=B7K
symbol=BB3
symbol=ZUU
</code></pre>
</li>
<li><p>数据查询</p>
<pre><code class="nullsql">selecet * from stocks_partitions where symbol=&#39;XYZ&#39; and ymd=&#39;2003-02-01&#39;;

# 若设置了strict方式，则select的where中一定要包含partition column条件查询
set hive.mapred.mode=strict;
select * from stocks_partitions where ymd=&#39;2003-02-01&#39;;
</code></pre>
</li>
<li><p>动态分区</p>
<ul>
<li>创建动态分区表<pre><code class="nullsql">create table if not exists stocks_dynamic_partition(
 col1 string,
 col2 string
)
partitioned by (exch_name string,yr int,sym string)
row format delimited fields terminated by &#39;,&#39; ;
</code></pre>
</li>
<li>启动动态分区<pre><code class="nullsql">set hive.exec.dynamic.partition=true;
</code></pre>
</li>
<li><p>加载数据（注意：默认动态分区要求至少有一个静态分区）</p>
<pre><code class="nullsql"># 如下方式，默认会报错
# SemanticException:dynamic partition strict mode requires at least one static partition column
insert overwrite table stocks_dynamic_partition partition(exch_name,yr,symbol)
select col1,col2,exch_name,year(ymd),symbol from stocks;

=&gt; 解决方案1：
set hive.exec.dynamic.partition.mode=nostrict;

=&gt; 解决方案2：
insert overwrite table stocks_dynamic_partition partition(exch_name=&#39;ABCSE&#39;,yr,symbol)
select col1,col2,exch_name,year(ymd),symbol from stocks;
</code></pre>
<ul>
<li>查看表<pre><code class="nullsql">show partitions stocks_dynamic_partition;
select * from stocks_dynamic_partition where exch_name=&#39;ABCSE&#39; and yr=2013 limit 10;
</code></pre>
</li>
<li>注意动态分区的分区数量是有限制的，可根据需要扩大设置（不推荐partition数量过多）：<pre><code class="nullsql">set hive.exec.max.dynamic.partitions=1000;
set hive.exec.max.dynamic.partitions.pernode=500;
</code></pre>
</li>
<li>数据表的目录结构<pre><code>stocks_dynamic_partition/exch_name=ABCSE/yr=2013/symbol=GEL/
stocks_dynamic_partition/exch_name=ABCSE/yr=2013/symbol=ZUU/
stocks_dynamic_partition/exch_name=ABCSE/yr=2014/symbol=GEL/
...
</code></pre></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="桶表"><a href="#桶表" class="headerlink" title="桶表"></a>桶表</h3><p>桶表(Bucket Table)</p>
<ul>
<li>细粒度的划分，桶列仍在数据文件中</li>
<li>主要应用：<ul>
<li>提高数据抽样效率</li>
<li>提升某些查询操作效率，例如mapside join</li>
</ul>
</li>
</ul>
<ul>
<li>创建表<pre><code class="nullsql"> # 必须设置这个数据，hive才会按照你设置的桶的个数去生成数据
 set hive.enforce.bucketing = true;
 create table t4(id int) clustered by(id) into 4 buckets;
</code></pre>
</li>
<li>插入数据<pre><code class="nullsql"> insert into table t4 select id from t3;      # 追加
 insert overrite table t4 select id from t3;  # 全部重写
</code></pre>
</li>
<li><p>抽样查询</p>
<pre><code class="nullsql"># 查询带桶的表(在一部分桶上检索，效率高)
select * from t4 tablesample(bucket 1 out of 4 on id);

# 不带桶的表(会在整个数据集上检索，效率低)
select * from t3 tablesample(bucket 1 out of 4 on id);
select * from t3 tablesample(bucket 1 out of 4 on rand());
</code></pre>
</li>
<li>数据表的目录结构<pre><code>t4/000000_0
t4/000000_1
t4/000000_2
t4/000000_3
</code></pre></li>
<li><p>分区+分桶：</p>
<pre><code class="nullsql"># 创建表
create table if not exists stocks_bucket(
  col1 string,
  col2 string,
  symbol string
)
partitioned by (exch_name string,yr string)
clustered by (symbol) into 3 buckets
row format delimited fields terminated by &#39;,&#39; ;
</code></pre>
<pre><code class="nullsql"># 设置动态分区和使用桶
set hive.exec.dynamic.partition=true;
set hive.enforce.bucketing = true; 

# 插入数据
insert into table stocks_bucket partition (exch_name=&#39;ABCE&#39;,yr)
select col1,col2,year(ymd) from stocks
</code></pre>
<pre><code class="nullsql"># 抽样查询对比
select * from stocks tablesample(bucket 3 out of 5 on symbol) s;        # 低效
select * from stocks_bucket tablesample(bucket 3 out of 5 on symbol) s; # 高效
</code></pre>
<pre><code class="nullsql"># 数据表的目录结构
stocks_bucket/exch_name=ABCE/yr=2013
stocks_bucket/exch_name=ABCE/yr=2013/000000_0
stocks_bucket/exch_name=ABCE/yr=2013/000000_1
stocks_bucket/exch_name=ABCE/yr=2013/000000_2
stocks_bucket/exch_name=ABCE/yr=2014
stocks_bucket/exch_name=ABCE/yr=2014/000000_0
stocks_bucket/exch_name=ABCE/yr=2014/000000_1
stocks_bucket/exch_name=ABCE/yr=2014/000000_2
...
</code></pre>
</li>
</ul>
<h3 id="抽样"><a href="#抽样" class="headerlink" title="抽样"></a>抽样</h3><p>tablesample 抽样</p>
<ul>
<li>tablesample(n precent/rows)<ul>
<li>n precent</li>
<li>n rows</li>
</ul>
</li>
<li>tablesample(nM)<ul>
<li>n兆</li>
</ul>
</li>
<li><p>tablesample(bucket x out of y [on columns])</p>
<ul>
<li>x: 从第几个桶开始抽样（从1开始）</li>
<li>y: 抽样的桶数（若是分桶表，则必须为总bucket数的倍数或者因子）</li>
<li>columns: 抽样的列</li>
<li>注意：<ul>
<li>基于已经分桶的表抽样，查询只会扫描相应桶中的数据</li>
<li>基于未分桶表的抽样，查询时候需要扫描整表数据</li>
</ul>
</li>
<li><p>示例：</p>
<pre><code class="nullsql"># 1. t1 为未分桶表
# 1.1 scan全表，根据col1分为10个桶，从第3个桶中取数据；
select * from t1 tablesample(bucket 3 out of 10 on col1);

# 1.2 scan全表，根据随机数分为10个桶，从第3个桶中取数据；
select * from t1 tablesample(bucket 3 out of 10 on rand());

# 2. t2 为分桶表，有10个桶
# 2.1 直接从第3个桶中取数据
select * from t2 tablesample(bucket 3 out of 10 on col1);

# 2.2 共抽取2(10/5)个桶的数据，从第3个和第8(3+5)个桶中抽取数据
select * from t2 tablesample(bucket 3 out of 5 on col1);

# 2.3 共抽取0.5(10/20)个桶的数据，从第3个桶中抽取一半数据
select * from t2 tablesample(bucket 3 out of 20 on col1);
</code></pre>
</li>
</ul>
</li>
<li>例如：<ul>
<li>tablesample(50 precent)</li>
<li>tablesample(50 rows)</li>
<li>tablesample(50M)</li>
<li>tablesample(bucket 3 out of 10)</li>
<li>tablesample(bucket 3 out of 10 on rand())</li>
</ul>
</li>
</ul>
<h3 id="单表查询"><a href="#单表查询" class="headerlink" title="单表查询"></a>单表查询</h3><pre><code class="nullsql">SELECT [ALL | DISTINCT] select_expr, select_expr, ...
  FROM table_reference 
  [WHERE condition] 
  [GROUP BY col_list] [Having condition]
  [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list] | [ORDER BY col_list] ]
  [LIMIT number]
</code></pre>
<ul>
<li>where：过滤（mapper端）</li>
<li>group by：局部分组（reducer端），select中可使用一些聚合函数，例如sum，avg，count等</li>
<li>cluster by：等价于distribute by + sort by ,只是无法指定排序规则（默认asc）</li>
<li>distribute by：分区（partitioner），按指定字段划分数据到各个reduce/file</li>
<li>sort by：局部排序（reducer端）</li>
<li>order by：全局排序，只有一个reducer（数据量很大时慎用）</li>
<li>limit：减少数据量，传输到reduce端（单机）的数据记录数就减少到<code>n*(map个数)</code></li>
</ul>
<p>示例：</p>
<ul>
<li>加查询结果写入HDFS中<pre><code class="nullsql">insert overwrite local directory &#39;/home/hive/output/hive/stocks&#39;
row format delimited fields terminated by &#39;,&#39;
select * from stocks distributed by symbol sort by symbol asc,price_close desc;
</code></pre>
</li>
<li>全局排序 order by<pre><code class="nullsql"># 无论设置了多少个reducer，这里只会使用一个reducer（数据量很大时效率低）
set mapreduce.job.reduces=3;
select * from stocks order by price_close desc;
</code></pre>
</li>
<li>局部排序 sort by<pre><code class="nullsql"># 每个reducer中排序
set mapreduce.job.reduces=3;
select * from stocks sort by price_close desc;
</code></pre>
</li>
<li>分区 distribute/cluster by<pre><code class="nullsql"># distribute by 控制某个特定行应该到哪个reducer
# sort by 为每个reducer产生一个排好序的文件
# distribute by + sort by = cluster by
set mapreduce.job.reduces=3;
select * from stocks distributed by symbol sort by symbol asc;
select * from stocks cluster by symbol;
</code></pre>
</li>
<li>聚合操作 group by<pre><code class="nullsql">select symbol,count(*) from stocks group by symbol;
</code></pre>
</li>
<li>Top N查询<pre><code class="nullsql">SET mapred.reduce.tasks = 1
SELECT * FROM sales SORT BY amount DESC LIMIT 5
</code></pre>
</li>
</ul>
<h3 id="连接查询"><a href="#连接查询" class="headerlink" title="连接查询"></a>连接查询</h3><ul>
<li><p>使用Join</p>
<ul>
<li><code>{inner} join</code>,<code>{left|right|full} [outer] join</code>,<code>cross join</code><pre><code class="nullsql">SELECT a.* FROM a JOIN b ON (a.id = b.id AND a.department = b.department);
SELECT a.* FROM a LEFT JOIN b ON (a.id = b.id AND a.department = b.department);
SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2);
</code></pre>
</li>
<li>特例：<code>LEFT SEMI JOIN</code><pre><code class="nullsql">SELECT a.key, a.value FROM a WHERE a.key in (SELECT b.key FROM B);
SELECT a.key, a.val FROM a LEFT SEMI JOIN b ON (a.key = b.key);
</code></pre>
</li>
</ul>
</li>
<li><p>Hive中的Join可分为：</p>
<ul>
<li>Common Join（Reduce阶段完成join）<ul>
<li>Map阶段：读取源表的数据，以Join on条件中的列为key，以join后所关心的(select或者where中需要用到的)列为value（value中包含Tag，用于标明此value对应哪个表）</li>
<li>Shuffle阶段：根据key的值进行hash,并将key/value按照hash值推送至不同的reduce中，这样确保两个表中相同的key位于同一个reduce中</li>
<li>Reduce阶段：根据key的值完成join操作，通过Tag识别不同表中的数据</li>
</ul>
</li>
<li>Map Join（Map阶段完成join）：通常用于一个很小的表和一个大表进行join的场景<ul>
<li>Local Task（Client端本地执行）：扫描小表，装载到DistributeCache中</li>
<li>Map Task：读取DistributeCache数据至内存，遍历大表记录，两者进行join，输出结果</li>
<li>无Reducer Task</li>
</ul>
</li>
</ul>
</li>
<li><p>Hive中的join操作</p>
<ul>
<li><p>join操作是在where操作之前执行，即where条件不能起到减少join数据的作用，应尽量在<code>on</code>中加入约束条件</p>
<pre><code class="nullsql">SELECT a.val, b.val FROM a 
JOIN b ON (a.key=b.key)
WHERE a.ds=&#39;2009-07-07&#39; AND b.ds=&#39;2009-07-07&#39;

=&gt; 优化为：
SELECT a.val, b.val FROM a 
JOIN b ON (a.key=b.key AND b.ds=&#39;2009-07-07&#39; AND a.ds=&#39;2009-07-07&#39;)
</code></pre>
</li>
<li><p>多表连接，会转换成多个MR Job，但关联条件相同的多表join会自动优化成一个mapreduce job</p>
<pre><code class="nullsql"># 在两个mapred程序中执行join
SELECT a.val, b.val, c.val FROM a 
JOIN b ON (a.key = b.key1) 
JOIN c ON (c.key = b.key2)

# 在一个mapre程序中执行join
SELECT a.val, b.val, c.val FROM a 
JOIN b ON (a.key = b.key1) 
JOIN c ON (c.key = b.key1)
</code></pre>
</li>
<li>多表连接，前一个join生成的数据会缓存到内存，通过stream取后一张表数据，应尽量将记录多的表放在后面join，也可使用<code>/*+ STREAMTABLE(table) */</code>指定将哪个大表stream化<pre><code class="nullsql">SELECT /*+ STREAMTABLE(a) */ a.val, b.val, c.val FROM a 
JOIN b ON (a.key = b.key1) 
JOIN c ON (c.key = b.key1)
</code></pre>
</li>
<li><p>Map Side Join</p>
<ul>
<li>可在查询中使用<code>/*+ mapjoin(table) */</code> 指定将哪个小表装载到DistributeCache中<pre><code class="nullsql"># 注意： 这里无法使用a FULL/RIGHT JOIN b
SELECT /*+ MAPJOIN(b) */ a.key, a.value FROM a join b on a.key = b.key;
</code></pre>
</li>
<li>Auto Map Side Join：系统自动判断使用mapjoin（由参数hive.auto.convert.join决定，默认为true）<pre><code class="nullsql"># Local Task 中找出符合mapjoin条件的表，装载到DistributeCache中，后续使用map join；若未找到符合条件的表，则使用common join
set hive.auto.convert.join=true;
# 根据参数hive.mapjoin.smalltable.filesize的设置判断mapjoin的表
SELECT a.key, a.value FROM a join b on a.key = b.key;
</code></pre>
</li>
<li>与map join相关的hive参数<pre><code># hive.join.emit.interval 
# hive.auto.convert.join 
# hive.mapjoin.smalltable.filesize
# hive.mapjoin.size.key  
# hive.mapjoin.cache.numrows
</code></pre></li>
<li><p>Sort Map Bucket Map Join：根据join key将各个关联表进行Bucket，提高join效率</p>
<pre><code class="nullsql"># 创建分桶表
create table a_smb(...) clustered by (key) sort by (key) into 10 buckets;
create table b_smb(...) clustered by (key) sort by (key) into 5 buckets;

# 为分桶表加载数据
set hive.enforce.bucketing=true;
insert into table a_smb select * from a;
insert into table b_smb select * from b;

# 打开SMB Map Join
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true; 
set hive.optimize.bucketmapjoin.sortedmerge = true; 

set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat; 

select * from a_smb a join b_smb on a.key=b.key;
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><h3 id="示例1：导入Apache-log"><a href="#示例1：导入Apache-log" class="headerlink" title="示例1：导入Apache log"></a>示例1：导入Apache log</h3><ul>
<li>log格式:<pre><code>127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] &quot;GET /apache_pb.gif HTTP/1.0&quot; 200 2326
</code></pre></li>
<li><p>创建表:</p>
<pre><code class="nullsql">CREATE TABLE apachelog (
host STRING,
identity STRING,
username STRING,
time STRING,
request STRING,
status STRING,
size STRING,
referer STRING,
agent STRING)
ROW FORMAT SERDE &#39;org.apache.hadoop.hive.contrib.serde2.RegexSerDe&#39;
WITH SERDEPROPERTIES (
  &quot;input.regex&quot; = &quot;([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) ([^ \&quot;]*|\&quot;[^\&quot;]*\&quot;) (-|[0-9]*) (-|[0-9]*)(?: ([^ \&quot;]*|\&quot;.*\&quot;) ([^ \&quot;]*|\&quot;.*\&quot;))?&quot;,
  &quot;output.format.string&quot; = &quot;%1$s %2$s %3$s %4$s %5$s %6$s %7$s %8$s %9$s&quot;
)
STORED AS TEXTFILE;
</code></pre>
</li>
<li><p>添加jar包到hive的执行环境:</p>
<pre><code>add jar $HIVE_HOME/lib/hive-contrib-1.2.1.jar
</code></pre></li>
<li>加载数据:<pre><code># inpath will be deleted!
load data inpath &#39;/input/access_2013_05_31.log&#39; into table apachelog;
</code></pre></li>
<li>查询:<pre><code>show tables;
describe formatted apachelog;
select * from apachelog limit 10;
select count(*) from apachelog;
select status,count(*) from apachelog group by status;
</code></pre></li>
<li>HDFS目录:<pre><code>/user/hive/warehouse/apachelog
/user/hive/warehouse/apachelog/access_2013_05_31.log
</code></pre></li>
</ul>
<h3 id="示例2：创建内部表"><a href="#示例2：创建内部表" class="headerlink" title="示例2：创建内部表"></a>示例2：创建内部表</h3><ul>
<li>创建内部表log:<pre><code class="nullsql">create table log(
  ip string,
  datetime string,
  method string,
  url string,
  status string,
  size string
)
row format delimited fields terminated by &#39;,&#39; ;
</code></pre>
</li>
<li><p>插入数据:</p>
<pre><code class="nullsql">insert into table log 
select host,time,substring(split(request,&#39; &#39;)[0],2) as method,split(request,&#39; &#39;)[1] as url,status,size from apachelog; 

# 重新过滤掉某些数据（hive中没有delete操作，只能overwrite）
insert overwrite table log 
select * from log where length(method)&lt;7 and method in (&#39;GET&#39;,&#39;POST&#39;,&#39;PUT&#39;,&#39;DELETE&#39;,&#39;OPTION&#39;,&#39;HEAD&#39;);
</code></pre>
</li>
<li><p>查询:</p>
<pre><code class="nullsql">describe formatted log;

set mapred.reduce.tasks=3;
select method from log where length(method)&lt;7 group by method;
select status,method,count(*) from log group by status,method;
</code></pre>
</li>
<li>HDFS目录:<pre><code>/user/hive/warehouse/log
/user/hive/warehouse/log/000000_0
</code></pre></li>
</ul>
<h3 id="示例3：内部静态分区表"><a href="#示例3：内部静态分区表" class="headerlink" title="示例3：内部静态分区表"></a>示例3：内部静态分区表</h3><ul>
<li>创建内部分区表log_partition:（注意分区字段不能包含在建表字段中）<pre><code class="nullsql">create table log_partition(
  ip string,
  datetime string,
  url string,
  size string
)
partitioned by (status string,method string)
row format delimited fields terminated by &#39;,&#39; ;
</code></pre>
</li>
<li><p>插入partition数据:（使用insert…select）</p>
<pre><code class="nullsql">insert into table log_partition partition(status=&#39;200&#39;,method=&#39;GET&#39;)
select ip,datetime,url,size from log where status=&#39;200&#39; and method=&#39;GET&#39;;

insert into table log_partition partition(status=&#39;200&#39;,method=&#39;POST&#39;)
select ip,datetime,url,size from log where status=&#39;200&#39; and method=&#39;POST&#39;

from log
insert overwrite table log_partition partition(status=&#39;301&#39;,method=&#39;GET&#39;)
select ip,datetime,url,size where status=&#39;301&#39; and method=&#39;GET&#39;
insert overwrite table log_partition partition(status=&#39;301&#39;,method=&#39;POST&#39;)
select ip,datetime,url,size where status=&#39;301&#39; and method=&#39;POST&#39;
insert overwrite table log_partition partition(status=&#39;400&#39;,method=&#39;GET&#39;)
select ip,datetime,url,size where status=&#39;400&#39;and method=&#39;GET&#39;;
</code></pre>
</li>
<li><p>插入partition数据:（使用alter…location）</p>
<pre><code class="nullsql">insert overwrite directory &#39;/user/hive/warehouse/log_partition/status=400/method=POST&#39;
row format delimited fields terminated by &#39;,&#39;
select ip,datetime,url,size from log where status=&#39;400&#39; and method=&#39;POST&#39;;

alter table log_partition add if not exists partition (status=&#39;400&#39;,method=&#39;POST&#39;) 
location &#39;/user/hive/warehouse/log_partition/status=400/method=POST&#39;;
</code></pre>
</li>
<li>删除partition数据:（注意会删除partition对应的目录和文件）<pre><code class="nullsql">alter table log_partition drop if exists partition (status=&#39;301&#39;,method=&#39;GET&#39;);
</code></pre>
</li>
<li>查看:<pre><code class="nullsql">describe formatted log_partition;
show partitions log_partition;
select * from log_partition where status=&#39;400&#39; and method=&#39;POST&#39;;
</code></pre>
</li>
<li>HDFS目录:<pre><code>/user/hive/warehouse/log_partition
/user/hive/warehouse/log_partition/status=200
/user/hive/warehouse/log_partition/status=200/method=GET
/user/hive/warehouse/log_partition/status=200/method=GET/000000_0
/user/hive/warehouse/log_partition/status=301
/user/hive/warehouse/log_partition/status=301/method=POST
/user/hive/warehouse/log_partition/status=301/method=POST/000000_0
/user/hive/warehouse/log_partition/status=400
/user/hive/warehouse/log_partition/status=400/method=GET
/user/hive/warehouse/log_partition/status=400/method=GET/000000_0
/user/hive/warehouse/log_partition/status=400/method=POST
/user/hive/warehouse/log_partition/status=400/method=POST/000000_0
</code></pre></li>
</ul>
<h3 id="示例4：内部动态分区表"><a href="#示例4：内部动态分区表" class="headerlink" title="示例4：内部动态分区表"></a>示例4：内部动态分区表</h3><ul>
<li>创建内部分区表log_dynamic_partition:<pre><code class="nullsql">create table log_dynamic_partition like log_partition;
</code></pre>
</li>
<li>打开动态分区:<pre><code class="nullsql">set hive.exec.dynamic.partition=true;
</code></pre>
</li>
<li>插入数据:（注意：1. 字段和顺序；2. 第一个为静态partition）<pre><code class="nullsql">insert into table log_dynamic_partition partition(status=&#39;200&#39;,method)
select ip,datetime,url,size,method from log where status=&#39;200&#39;;
</code></pre>
</li>
<li>插入数据:（注意：1. 字段和顺序；2. 所有都为动态partition）<pre><code class="nullsql"># 设置为nostrict模式
set hive.exec.dynamic.partition.mode=nostrict;
# 插入
insert overwrite table log_dynamic_partition partition(status,method)
select ip,datetime,url,size,status,method from log;
</code></pre>
</li>
<li>查看:<pre><code class="nullsql">describe formatted log_dynamic_partition;
show partitions log_dynamic_partition;
select * from log_dynamic_partition where status=&#39;400&#39; and method=&#39;POST&#39;;
</code></pre>
</li>
<li>HDFS目录:<pre><code>/user/hive/warehouse/log_dynamic_partition
/user/hive/warehouse/log_dynamic_partition/status=200
/user/hive/warehouse/log_dynamic_partition/status=200/method=GET
/user/hive/warehouse/log_dynamic_partition/status=200/method=GET/000000_0
/user/hive/warehouse/log_dynamic_partition/status=301
/user/hive/warehouse/log_dynamic_partition/status=301/method=GET
/user/hive/warehouse/log_dynamic_partition/status=301/method=GET/000000_0
/user/hive/warehouse/log_dynamic_partition/status=301/method=POST
/user/hive/warehouse/log_dynamic_partition/status=301/method=POST/000000_0
/user/hive/warehouse/log_dynamic_partition/status=400
/user/hive/warehouse/log_dynamic_partition/status=400/method=GET
/user/hive/warehouse/log_dynamic_partition/status=400/method=GET/000000_0
/user/hive/warehouse/log_dynamic_partition/status=400/method=POST
/user/hive/warehouse/log_dynamic_partition/status=400/method=POST/000000_0
...
</code></pre></li>
</ul>
<h3 id="示例5：内部表分桶"><a href="#示例5：内部表分桶" class="headerlink" title="示例5：内部表分桶"></a>示例5：内部表分桶</h3><ul>
<li>创建分桶表log_bucket: （注意：分桶字段为建表中的字段）<pre><code class="nullsql">create table log_bucket(
  ip string,
  datetime string,
  method string,
  url string,
  status string,
  size string
)
clustered by (status,method) into 5 buckets
row format delimited fields terminated by &#39;,&#39; ;
</code></pre>
</li>
<li>打开分桶:<pre><code class="nullsql">set hive.enforce.bucketing = true;
</code></pre>
</li>
<li>插入数据:<pre><code class="nullsql">insert into table log_bucket
select * from log;
</code></pre>
</li>
<li>查看:<pre><code class="nullsql">describe formatted log_bucket;
select * from log_bucket tablesample(bucket 1 out of 5 on status);
select * from log tablesample(bucket 1 out of 5 on status);
</code></pre>
</li>
<li>HDFS目录:<pre><code>/user/hive/warehouse/log_bucket
/user/hive/warehouse/log_bucket/000000_0
/user/hive/warehouse/log_bucket/000001_0
/user/hive/warehouse/log_bucket/000002_0
/user/hive/warehouse/log_bucket/000003_0
/user/hive/warehouse/log_bucket/000004_0
</code></pre></li>
</ul>
<h3 id="示例6：内部分区分桶表"><a href="#示例6：内部分区分桶表" class="headerlink" title="示例6：内部分区分桶表"></a>示例6：内部分区分桶表</h3><ul>
<li>创建分区分桶表log_partition_bucket:<pre><code class="nullsql">create table log_partition_bucket(
  ip string,
  datetime string,
  url string,
  size string
)
partitioned by (status string,method string) 
clustered by (ip) into 5 buckets
row format delimited fields terminated by &#39;,&#39; ;
</code></pre>
</li>
<li>打开动态分区和分桶:<pre><code class="nullsql">set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.enforce.bucketing = true;
</code></pre>
</li>
<li>插入数据:<pre><code class="nullsql">insert into table log_partition_bucket partition(status,method)
select ip,datetime,url,size,status,method from log;
</code></pre>
</li>
<li>查看:<pre><code class="nullsql">describe formatted log_partition_bucket;
select * from log_partition_bucket tablesample(bucket 1 out of 5 on status) limit 5;
</code></pre>
</li>
<li>HDFS目录:<pre><code>/user/hive/warehouse/log_partition_bucket
/user/hive/warehouse/log_partition_bucket/status=200
/user/hive/warehouse/log_partition_bucket/status=200/method=GET
/user/hive/warehouse/log_partition_bucket/status=200/method=GET/000000_0
/user/hive/warehouse/log_partition_bucket/status=200/method=GET/000001_0
/user/hive/warehouse/log_partition_bucket/status=200/method=GET/000002_0
/user/hive/warehouse/log_partition_bucket/status=200/method=GET/000003_0
/user/hive/warehouse/log_partition_bucket/status=200/method=GET/000004_0
/user/hive/warehouse/log_partition_bucket/status=301
/user/hive/warehouse/log_partition_bucket/status=301/method=GET
/user/hive/warehouse/log_partition_bucket/status=301/method=GET/000000_0
/user/hive/warehouse/log_partition_bucket/status=301/method=GET/000001_0
/user/hive/warehouse/log_partition_bucket/status=301/method=GET/000002_0
/user/hive/warehouse/log_partition_bucket/status=301/method=GET/000003_0
/user/hive/warehouse/log_partition_bucket/status=301/method=GET/000004_0
/user/hive/warehouse/log_partition_bucket/status=301/method=POST
/user/hive/warehouse/log_partition_bucket/status=301/method=POST/000000_0
/user/hive/warehouse/log_partition_bucket/status=301/method=POST/000001_0
/user/hive/warehouse/log_partition_bucket/status=301/method=POST/000002_0
/user/hive/warehouse/log_partition_bucket/status=301/method=POST/000003_0
/user/hive/warehouse/log_partition_bucket/status=301/method=POST/000004_0
...
</code></pre></li>
</ul>
<h3 id="示例6：外部表"><a href="#示例6：外部表" class="headerlink" title="示例6：外部表"></a>示例6：外部表</h3><ul>
<li>创建外部表log_external:<pre><code class="nullsql">create external table log_external(
  ip string,
  datetime string,
  method string,
  url string,
  status string,
  size string
)
row format delimited fields terminated by &#39;,&#39; 
location &#39;/input/external&#39;;
</code></pre>
</li>
<li><p>插入数据:</p>
<pre><code class="nullsql"># generate file: /input/external/000000_0
insert into table log_external
select * from log where status=&#39;200&#39;;

# generate file: /input/external/000000_0_copy_1
insert into table log_external
select * from log where status=&#39;400&#39;;
</code></pre>
</li>
<li><p>插入数据:</p>
<pre><code class="nullsql"># generate file: /input/log_301/000000_0
insert overwrite directory &#39;/input/log_301&#39;
row format delimited fields terminated by &#39;,&#39;
select * from log where status=&#39;301&#39;; 

# generate file: /input/external/000000_0_copy_2
# delete file: /input/log_301/000000_0
load data inpath &#39;/input/log_301&#39; into table log_external;
</code></pre>
</li>
<li>查看:<pre><code class="nullsql">describe formatted log_external;
select * from log_external limit 5;
select * from log_external where status=&#39;400&#39; limit 5;
select * from log_external where status=&#39;301&#39; limit 5;
</code></pre>
</li>
<li>HDFS目录:<pre><code>/input/external/000000_0
/input/external/000000_0_copy_1
/input/external/000000_0_copy_2
</code></pre></li>
<li>删除表:（注意：1. 无法使用truncate清空外部表；2. 数据文件不会被删除）<pre><code class="nullsql">drop table log_external;
</code></pre>
</li>
<li>重新创建外部表:（不用重新插入数据就有数据可查出了）<pre><code class="nullsql">create external table log_external(
  ip string,
  datetime string,
  method string,
  url string,
  status string,
  size string
)
row format delimited fields terminated by &#39;,&#39; 
location &#39;/input/external&#39;;
</code></pre>
</li>
<li>查看表:<pre><code class="nullsql">describe formatted log_external;
select * from log_external limit 5;
select * from log_external where status=&#39;400&#39; limit 5;
select * from log_external where status=&#39;301&#39; limit 5;
</code></pre>
</li>
</ul>
<h3 id="示例7：外部分区分桶表"><a href="#示例7：外部分区分桶表" class="headerlink" title="示例7：外部分区分桶表"></a>示例7：外部分区分桶表</h3><ul>
<li>创建外部分区分桶表log_external_partition:<pre><code class="nullsql">create external table log_external_partition(
  ip string,
  datetime string,
  url string,
  size string
)
partitioned by (status string,method string)
clustered by (ip) into 5 buckets 
row format delimited fields terminated by &#39;,&#39;
location &#39;/input/external_partition&#39;;
</code></pre>
</li>
<li><p>插入partition数据: （alter location）</p>
<pre><code class="nullsql">insert overwrite directory &#39;/input/log_301_GET&#39; 
row format delimited fields terminated by &#39;,&#39;
select ip,datetime,url,size from log where status=&#39;301&#39; and method=&#39;GET&#39;; 

# generate file: /input/log_301_GET/000000_0~000004_0
alter table log_external_partition add partition (status=&#39;301&#39;,method=&#39;GET&#39;) 
location &#39;/input/log_301_GET&#39;;
</code></pre>
</li>
<li><p>插入partition数据:（动态分区）</p>
<pre><code class="nullsql">set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.enforce.bucketing = true;

# generate file: 
# /input/log_301_GET/000000_0_copy_1
# /input/external_partition/status=xxx/method=yyy/000000_0~000004_0
insert into table log_external_partition partition(status,method)
select ip,datetime,url,size,status,method from log;
</code></pre>
</li>
<li><p>HDFS目录：</p>
<pre><code>/input/log_301_GET
/input/log_301_GET/000000_0
/input/log_301_GET/000000_0_copy_1
/input/log_301_GET/000001_0
/input/log_301_GET/000002_0
/input/log_301_GET/000003_0
/input/log_301_GET/000004_0

/input/external_partition
/input/external_partition/status=200
/input/external_partition/status=200/method=GET
/input/external_partition/status=200/method=GET/000000_0
/input/external_partition/status=200/method=GET/000001_0
/input/external_partition/status=200/method=GET/000002_0
/input/external_partition/status=200/method=GET/000003_0
/input/external_partition/status=200/method=GET/000004_0
/input/external_partition/status=301
/input/external_partition/status=301/method=POST
/input/external_partition/status=301/method=POST/000000_0
/input/external_partition/status=301/method=POST/000001_0
/input/external_partition/status=301/method=POST/000002_0
/input/external_partition/status=301/method=POST/000003_0
/input/external_partition/status=301/method=POST/000004_0
...
</code></pre></li>
</ul>
<h3 id="示例8：全排序"><a href="#示例8：全排序" class="headerlink" title="示例8：全排序"></a>示例8：全排序</h3><ul>
<li><p>使用order by （不管设置几个reducer，最终只使用一个reducer）</p>
<pre><code class="nullsql">set mapred.reduce.tasks=5;

# 全排序
insert overwrite directory &#39;/input/log_order&#39; 
select ip,size from log order by ip;

# 局部排序
insert overwrite directory &#39;/input/log_distribute&#39; 
select ip,size from log distribute by ip sort by size;

# 全排序（数据量很大时，效率会高些）
insert overwrite directory &#39;/input/log_order_opt&#39;
select * from (select ip,size from log distribute by ip sort by size) s order by ip;
</code></pre>
</li>
<li>使用TotalOrderSort<ul>
<li>生成抽样文件</li>
<li>使用TotalOrderSort</li>
</ul>
</li>
</ul>
<h3 id="示例9：Join"><a href="#示例9：Join" class="headerlink" title="示例9：Join"></a>示例9：Join</h3><ul>
<li><p>创建两张表并插入数据:（create table as select）</p>
<pre><code class="nullsql">create table a_log 
row format delimited fields terminated by &#39;,&#39; 
as select * from log where status=&#39;200&#39;;

create table b_log
row format delimited fields terminated by &#39;,&#39; 
as select * from log where status=&#39;301&#39;;
</code></pre>
</li>
<li>查询: （会自动判断使用mapjoin）<pre><code class="nullsql">select * from a_log a join b_log b on a.ip=b.ip where a.method=&#39;POST&#39; limit 5;
select * from a_log a join b_log b on a.ip=b.ip and a.method=&#39;POST&#39; limit 5;
</code></pre>
</li>
<li>HDFS目录:<pre><code>/user/hive/warehouse/a_log
/user/hive/warehouse/a_log/000000_0
/user/hive/warehouse/b_log
/user/hive/warehouse/b_log/000000_0
</code></pre></li>
<li><p>创建两张分区表并插入数据:</p>
<pre><code class="nullsql">create table a_log_smb like a_log;
create table b_log_smb like b_log;

alter table a_log_smb clustered by (ip) into 3 buckets;
alter table b_log_smb clustered by (ip) into 5 buckets;

insert into a_log_smb select * from a_log;
insert into b_log_smb select * from b_log;
</code></pre>
</li>
<li>查询:(SMP)<pre><code>select * from a_log_smb a join b_log_smb b on a.ip=b.ip limit 5;
</code></pre></li>
<li>HDFS目录：<pre><code>/user/hive/warehouse/a_log_smb
/user/hive/warehouse/a_log_smb/000000_0
/user/hive/warehouse/a_log_smb/000001_0
/user/hive/warehouse/a_log_smb/000002_0
/user/hive/warehouse/b_log_smb
/user/hive/warehouse/b_log_smb/000000_0
/user/hive/warehouse/b_log_smb/000001_0
/user/hive/warehouse/b_log_smb/000002_0
/user/hive/warehouse/b_log_smb/000003_0
/user/hive/warehouse/b_log_smb/000004_0
</code></pre></li>
</ul>
<h2 id="优化策略"><a href="#优化策略" class="headerlink" title="优化策略"></a>优化策略</h2><p>数据倾斜：由于数据的不均衡原因，导致数据分布不均匀，造成数据大量的集中到一点，造成数据热点</p>
<p>Hadoop 计算框架特性：</p>
<ul>
<li>不怕数据大，怕数据倾斜</li>
<li>job过多，耗时长（job初始化时间长）</li>
</ul>
<p>常用优化手段：</p>
<ul>
<li>减少Job数</li>
<li>并行Job，例如设置：<pre><code class="nullsql"># 对于同一个SQL产生的JOB,如果不存在依赖的情况下，将会并行启动JOB
set hive.exec.parallel=true;
set hive.exec.parallel.thread.number=16;
</code></pre>
</li>
<li><p>合理设置Mapper和Reducer数</p>
<ul>
<li>减少mapper数：合并小文件<pre><code class="nullsql"># 100~128M的按照100M分割，&lt;100M合并
set mapred.max.split.size=100000000;
set mapred.min.split.size.per.node=100000000;
set mapred.min.split.size.per.rack=100000000;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
</code></pre>
</li>
<li>增加mapper数：拆分文件（分区，分桶）</li>
<li><p>reducer数</p>
<pre><code class="nullsql"># 每个reduce任务处理的数据量，默认为1000^3=1G
hive.exec.reducers.bytes.per.reducer

# 每个任务最大的reduce数，默认为999
hive.exec.reducers.max

# 设置reducer数量
mapred.reduce.tasks
</code></pre>
</li>
</ul>
</li>
<li><p>合理压缩，减少网络传输和I/O压力，例如设置：</p>
<pre><code class="nullsql">set mapred.output.compress = true;  
set mapred.output.compression.codec = org.apache.hadoop.io.compress.GzipCodec;  
set mapred.output.compression.type = BLOCK;  
set mapred.compress.map.output = true;  
set mapred.map.output.compression.codec = org.apache.hadoop.io.compress.LzoCodec;  

set hive.exec.compress.output = true;  
set hive.exec.compress.intermediate = true;  
set hive.intermediate.compression.codec = org.apache.hadoop.io.compress.LzoCodec;
</code></pre>
</li>
<li>以SequenceFile保存，节约序列化和反序列化时间</li>
<li>少用count distinct，例如：<pre><code class="nullsql">select status,count(distinct ip) from log group by status;
=&gt;
select status,count(ip) from (select status,ip from log group by status,ip) a
group by status;
</code></pre>
</li>
<li>join优化<ul>
<li>尽量将condition放入join on中</li>
<li>尽量大表滞后或使用STREAMTABLE(table)标识大表</li>
<li>使用SMB Map Join (SMB: Sort Merge Bucket) </li>
</ul>
</li>
<li>合理分区，分桶</li>
<li>小数据量，尽量使用本地MapReduce，例如设置：<pre><code class="nullsql">set hive.exec.mode.local.auto=true;  
set hive.exec.mode.local.auto.inputbytes.max=50000000;
set hive.exec.mode.local.auto.tasks.max=10;
</code></pre>
</li>
</ul>
<h2 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h2><p>查看函数</p>
<pre><code class="nullsql">SHOW FUNCTIONS; 
DESCRIBE FUNCTION &lt;function_name&gt;
</code></pre>
<p>自定义函数包括三种UDF、UDAF、UDTF，可直接应用于Select语句</p>
<ul>
<li>UDF：User-Defined-Function<ul>
<li>用户自定义函数（只能实现一进一出的操作）</li>
<li><code>extends UDF</code></li>
</ul>
</li>
<li>UDAF：User-Defined Aggregation Funcation<ul>
<li>用户自定义聚合函数（可实现多进一出的操作）</li>
<li><code>extends UDAF</code>+ 内部Evaluator<code>implements UDAFEvaluator</code><ul>
<li>init 初始化</li>
<li>iterate 遍历</li>
<li>terminatePartial 类似Hadoop的Combiner</li>
<li>merge 合并</li>
<li>terminate 返回最终的聚集函数结果</li>
</ul>
</li>
</ul>
</li>
<li>UDTF：User-Defined Table-Generating Function<ul>
<li>用户自定义表函数（可实现一进多出的操作）</li>
<li><code>extends GenericUDTF</code></li>
</ul>
</li>
</ul>
<p>UDF示例：</p>
<ol>
<li>自定义函数<pre><code class="nulljava">import org.apache.Hadoop.hive.ql.exec.UDF   
public class Helloword extends UDF{   
public String evaluate(){   
    return &quot;hello world!&quot;;   
}   
public String evaluate(String str){   
    return &quot;hello world: &quot; + str;   
}   
}
</code></pre>
</li>
<li>上传jar包到目标机器</li>
<li>添加到Hive中<pre><code class="nullvim"># 进入hive客户端，添加jar包
hive&gt; add jar udf_helloword.jar
</code></pre>
</li>
<li>创建临时函数<pre><code class="nullvim">hive&gt; create temporary function helloword as &#39;com.cj.hive.udf.Helloword&#39;
</code></pre>
</li>
<li>测试<pre><code class="nullsql">select helloword(name) from users;
</code></pre>
</li>
<li>删除临时函数<pre><code class="nullvim">hive&gt; drop temporaty function helloword;
</code></pre>
</li>
</ol>
<p>注：helloworld为临时的函数，所以每次进入hive都需要add jar以及create temporary操作</p>
<h2 id="Java-API"><a href="#Java-API" class="headerlink" title="Java API"></a>Java API</h2><ul>
<li>启动Hive远程服务<pre><code class="nullvim">&gt; hive --service hiveserver2 &gt;/dev/null  2&gt;/dev/null &amp;
</code></pre>
</li>
<li>Java客户端加入依赖包<pre><code class="nullxml">&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
    &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;
    &lt;version&gt;${hive.version}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
</li>
<li><p>JAVA客户端连接操作代码</p>
<pre><code class="nulljava">@Test
public void testConnection() throws ClassNotFoundException, SQLException {
  Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;);
  Connection conn = DriverManager.getConnection(&quot;jdbc:hive2://cj.storm:10000/default&quot;, &quot;&quot;, &quot;&quot;);

  Statement stmt = conn.createStatement();
  String querySQL = &quot;select * from log_partition where status=&#39;200&#39; and method=&#39;GET&#39; limit 10&quot;;
  ResultSet res = stmt.executeQuery(querySQL);
  while (res.next()) {
    System.out.println(res.getString(1) + &quot;\t&quot; + res.getString(2));
  }
  res.close();
  conn.close();
}
</code></pre>
</li>
</ul>
  </section>
</article>

      <hr/>
      <section class="post-comment">
	<!-- disqus默认将数据加载到id为'disqus_thread'的容器中，可配置disqus_container_id改变-->
<div id="disqus_thread"> 
	<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</div>

	<script type="text/javascript">
		var disqus_shortname = 'sixdegreespace'; 
		var disqus_identifier = '2016/05/10/Hive.html';	
		var disqus_title = 'Hive';
		var disqus_url = 'http://sixdegree.github.io/2016/05/10/Hive.html' ;
		//var disqus_category_id = '4262241'; 

		(function() {
		    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
		    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
		})();
	</script>


 
</section>
    </div>
  </div>
</body>

<script src="/jquery/dist/jquery.min.js"></script>
<script src="/bootstrap/dist/js/bootstrap.min.js"></script>
<script src="/highlight/highlight.pack.js"></script>
<script type="text/javascript">
  hljs.initHighlightingOnLoad();
  
  $(document).ready(function(){
    var sidebarCtrl=$("#sidebar-ctrl");
    var sidebar=$("#sidebar");
    var wrapper=$("#wrapper");
    sidebarCtrl.on("click",function(event){
        //alert("click");
        sidebar.toggleClass("sidebar-toggle");
        wrapper.toggleClass("sidebar-toggle");
        sidebarCtrl.toggleClass("sidebar-toggle");
        sidebarCtrl.toggleClass("active");
    })
  });
</script>


</html>
